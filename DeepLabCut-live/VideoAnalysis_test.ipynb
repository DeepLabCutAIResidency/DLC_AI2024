{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlclive import DLCLive, Processor\n",
    "import dlclive\n",
    "from dlclive.display import Display\n",
    "import cv2\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #somewhat works, but doesn't sve keypoints on video\n",
    "# def analyze_video2(video_path: str, dlc_live):\n",
    "#     # Load video\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     poses = []\n",
    "#     frame_index = 0\n",
    "\n",
    "#     ###\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break  # End of video\n",
    "\n",
    "#         # Prepare the frame for the model\n",
    "#         print(\"a\")\n",
    "#         frame = np.array(frame, dtype=np.float32)\n",
    "#         print(\"b\")\n",
    "#         frame = np.transpose(frame,(2,0,1)) \n",
    "#         print(\"c\")\n",
    "#         frame = frame.reshape(1, frame.shape[0], frame.shape[1], frame.shape[2])\n",
    "#         print(\"e\")\n",
    "#         frame = (frame / 255.0)\n",
    "#         print(\"f\")\n",
    "#         #print(frame)\n",
    "        \n",
    "#         # Analyze the frame using the get_pose function\n",
    "#         pose = dlc_live.get_pose(frame)\n",
    "#         print(\"g\")\n",
    "        \n",
    "#         # Store the pose for this frame\n",
    "#         poses.append(pose)\n",
    "#         print(pose[\"bodypart\"][\"poses\"][0][0][0])\n",
    "    \n",
    "\n",
    "#         frame_index += 1\n",
    "#         print(frame_index)\n",
    "    \n",
    "#     # Release the video capture object\n",
    "#     cap.release()\n",
    "    \n",
    "#     return poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video2(video_path: str, dlc_live, pcutoff=0.5, display_radius=5, colors=None, resize=None, im_size=None):\n",
    "    # Load video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    poses = []\n",
    "    frame_index = 0\n",
    "\n",
    "    # Get the video writer setup to save the output video with keypoints\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    if resize:\n",
    "        frame_width, frame_height = im_size\n",
    "    vwriter = cv2.VideoWriter('output_with_keypoints.avi', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize default colors if not provided\n",
    "    if colors is None:\n",
    "        # Assuming there are 11 keypoints, adjust the length based on your model\n",
    "        colors = [(255, 0, 0)] * 11  # Default to red for all keypoints; adjust if needed\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        # Prepare the frame for the model\n",
    "        frame_for_model = np.array(frame, dtype=np.float32)\n",
    "        frame_for_model = np.transpose(frame_for_model, (2, 0, 1))\n",
    "        frame_for_model = frame_for_model.reshape(1, frame_for_model.shape[0], frame_for_model.shape[1], frame_for_model.shape[2])\n",
    "        frame_for_model = (frame_for_model / 255.0)\n",
    "        \n",
    "        # Analyze the frame using the get_pose function\n",
    "        pose = dlc_live.get_pose(frame_for_model)\n",
    "        \n",
    "        # Store the pose for this frame\n",
    "        poses.append(pose)\n",
    "        \n",
    "        # Visualize keypoints on the frame\n",
    "        this_pose = poses[-1][\"bodypart\"][\"poses\"][0]  # Adjust indexing as per your actual data structure\n",
    "        for j in range(this_pose.shape[0]):\n",
    "            confidence_tensor = this_pose[j, 2]  # Access the tensor with confidence values\n",
    "            confidence = confidence_tensor[0].item()  # Extract the first confidence score as a scalar\n",
    "\n",
    "            if confidence > pcutoff:\n",
    "                x = int(this_pose[j, 0, 0].item())  # Assuming the first element corresponds to the x-coordinate\n",
    "                y = int(this_pose[j, 1, 0].item())  # Assuming the first element corresponds to the y-coordinate\n",
    "\n",
    "                # Draw circle only if color is valid and provided for this keypoint\n",
    "                if j < len(colors):\n",
    "                    frame = cv2.circle(frame, (x, y), display_radius, colors[j], thickness=-1)\n",
    "                else:\n",
    "                    frame = cv2.circle(frame, (x, y), display_radius, (0, 255, 0), thickness=-1)  # Default color green\n",
    "\n",
    "        # Resize the frame if needed\n",
    "        if resize is not None:\n",
    "            frame = cv2.resize(frame, im_size)\n",
    "\n",
    "        # Write the frame with keypoints to the output video\n",
    "        vwriter.write(frame)\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "    # Release the video capture and writer objects\n",
    "    cap.release()\n",
    "    vwriter.release()\n",
    "\n",
    "    return poses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annastuckert/anaconda3/envs/deeplabcut3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dlclive import DLCLive, Processor\n",
    "import dlclive\n",
    "from dlclive.display import Display\n",
    "import cv2\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dlc_proc = Processor()\n",
    "dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt', model_type='pytorch') #change model type to 'pytorch' for raw .pt model\n",
    "\n",
    "#very short video\n",
    "video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_03s.avi'\n",
    "#short video\n",
    "#video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_1s.avi'\n",
    "\n",
    "poses = analyze_video2(video_path, dlc_live)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplabcut3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
