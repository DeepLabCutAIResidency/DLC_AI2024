{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlclive import DLCLive, Processor\n",
    "import dlclive\n",
    "from dlclive.display import Display\n",
    "import cv2\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#somewhat works, but doesn't sve keypoints on video\n",
    "def analyze_video2(video_path: str, dlc_live):\n",
    "    # Load video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    poses = []\n",
    "    frame_index = 0\n",
    "\n",
    "    ###\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        # Prepare the frame for the model\n",
    "        #print(\"a\")\n",
    "        frame = np.array(frame, dtype=np.float32)\n",
    "        #print(\"b\")\n",
    "        frame = np.transpose(frame,(2,0,1)) \n",
    "        #print(\"c\")\n",
    "        frame = frame.reshape(1, frame.shape[0], frame.shape[1], frame.shape[2])\n",
    "        #print(\"e\")\n",
    "        frame = (frame / 255.0)\n",
    "        #print(\"f\")\n",
    "        #print(frame)\n",
    "        \n",
    "        # Analyze the frame using the get_pose function\n",
    "        pose = dlc_live.get_pose(frame)\n",
    "        #print(\"g\")\n",
    "        \n",
    "        # Store the pose for this frame\n",
    "        poses.append(pose)\n",
    "        #print(pose[\"bodypart\"][\"poses\"][0][0][0])\n",
    "    \n",
    "\n",
    "        frame_index += 1\n",
    "        print(frame_index)\n",
    "    \n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    return poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_video2(video_path: str, dlc_live, pcutoff=0.5, display_radius=5, colors=None, resize=None, im_size=None):\n",
    "#     # Load video\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     poses = []\n",
    "#     frame_index = 0\n",
    "\n",
    "#     # Get the video writer setup to save the output video with keypoints\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "#     frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#     frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "#     if resize:\n",
    "#         frame_width, frame_height = im_size\n",
    "#     vwriter = cv2.VideoWriter('output_with_keypoints.avi', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "#     # Initialize default colors if not provided\n",
    "#     if colors is None:\n",
    "#         # Assuming there are 11 keypoints, adjust the length based on your model\n",
    "#         colors = [(255, 0, 0)] * 11  # Default to red for all keypoints; adjust if needed\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break  # End of video\n",
    "\n",
    "#         # Prepare the frame for the model\n",
    "#         frame_for_model = np.array(frame, dtype=np.float32)\n",
    "#         frame_for_model = np.transpose(frame_for_model, (2, 0, 1))\n",
    "#         frame_for_model = frame_for_model.reshape(1, frame_for_model.shape[0], frame_for_model.shape[1], frame_for_model.shape[2])\n",
    "#         frame_for_model = (frame_for_model / 255.0)\n",
    "        \n",
    "#         # Analyze the frame using the get_pose function\n",
    "#         pose = dlc_live.get_pose(frame_for_model)\n",
    "        \n",
    "#         # Store the pose for this frame\n",
    "#         poses.append(pose)\n",
    "        \n",
    "#         # Visualize keypoints on the frame\n",
    "#         this_pose = poses[-1][\"bodypart\"][\"poses\"][0]  # Adjust indexing as per your actual data structure\n",
    "#         for j in range(this_pose.shape[0]):\n",
    "#             confidence_tensor = this_pose[j, 2]  # Access the tensor with confidence values\n",
    "#             confidence = confidence_tensor[0].item()  # Extract the first confidence score as a scalar\n",
    "\n",
    "#             if confidence > pcutoff:\n",
    "#                 x = int(this_pose[j, 0, 0].item())  # Assuming the first element corresponds to the x-coordinate\n",
    "#                 y = int(this_pose[j, 1, 0].item())  # Assuming the first element corresponds to the y-coordinate\n",
    "\n",
    "#                 # Draw circle only if color is valid and provided for this keypoint\n",
    "#                 if j < len(colors):\n",
    "#                     frame = cv2.circle(frame, (x, y), display_radius, colors[j], thickness=-1)\n",
    "#                 else:\n",
    "#                     frame = cv2.circle(frame, (x, y), display_radius, (0, 255, 0), thickness=-1)  # Default color green\n",
    "\n",
    "#         # Resize the frame if needed\n",
    "#         if resize is not None:\n",
    "#             frame = cv2.resize(frame, im_size)\n",
    "\n",
    "#         # Write the frame with keypoints to the output video\n",
    "#         vwriter.write(frame)\n",
    "\n",
    "#         frame_index += 1\n",
    "\n",
    "#     # Release the video capture and writer objects\n",
    "#     cap.release()\n",
    "#     vwriter.release()\n",
    "\n",
    "#     return poses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annastuckert/anaconda3/envs/deeplabcut3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dlclive import DLCLive, Processor\n",
    "import dlclive\n",
    "from dlclive.display import Display\n",
    "import cv2\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to use homemade function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #profiling\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import gc\n",
    "\n",
    "# def analyze_video2(video_path: str, dlc_live):\n",
    "#     # Load video\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     poses = []\n",
    "#     frame_index = 0\n",
    "\n",
    "#     if not cap.isOpened():\n",
    "#         print(\"Error: Could not open video.\")\n",
    "#         return poses\n",
    "\n",
    "#     # Load the model once\n",
    "#     pose_model = dlc_live.load_model()\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break  # End of video\n",
    "\n",
    "#         try:\n",
    "#             # Prepare the frame for the model\n",
    "#             print(f\"Processing frame {frame_index}...\")\n",
    "\n",
    "#             frame = np.array(frame, dtype=np.float32)\n",
    "#             frame = np.transpose(frame, (2, 0, 1))\n",
    "#             frame = frame.reshape(1, frame.shape[0], frame.shape[1], frame.shape[2])\n",
    "#             frame = frame / 255.0\n",
    "\n",
    "#             # Analyze the frame using the get_pose function\n",
    "#             pose = dlc_live.get_pose(frame, pose_model=pose_model)\n",
    "#             print(f\"Pose extracted for frame {frame_index}.\")\n",
    "\n",
    "#             # Store the pose for this frame\n",
    "#             poses.append(pose)\n",
    "#             print(f\"Pose stored for frame {frame_index}.\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing frame {frame_index}: {e}\")\n",
    "#             break\n",
    "\n",
    "#         # Clean up to free memory\n",
    "#         del frame, pose  # Remove references to objects to free memory\n",
    "#         torch.cuda.empty_cache()  # Clear GPU cache if using GPU\n",
    "#         gc.collect()  # Run garbage collection to free memory\n",
    "\n",
    "#         frame_index += 1\n",
    "#         print(f\"Completed frame {frame_index}.\")\n",
    "\n",
    "#     # Release the video capture object\n",
    "#     cap.release()\n",
    "#     print(\"Video capture released.\")\n",
    "    \n",
    "#     return poses\n",
    "\n",
    "# # Assuming this is your Processor and DLCLive setup\n",
    "# dlc_proc = Processor()\n",
    "# dlc_live = DLCLive(\n",
    "#     pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\",\n",
    "#     processor=dlc_proc,\n",
    "#     snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt',\n",
    "#     model_type='pytorch'\n",
    "# )\n",
    "\n",
    "# # Choose your video path\n",
    "# video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_1s.avi'\n",
    "\n",
    "# # Call the analysis function\n",
    "# poses = analyze_video2(video_path, dlc_live)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def analyze_video2(video_path: str, dlc_live):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_index = 0\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return []\n",
    "\n",
    "    pose_model = dlc_live.load_model()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            print(f\"Processing frame {frame_index}...\")\n",
    "\n",
    "            # Reduce frame size to reduce memory usage\n",
    "            frame = cv2.resize(frame, (640, 480))  # Adjust size as needed\n",
    "\n",
    "            frame = np.array(frame, dtype=np.float32)\n",
    "            frame = np.transpose(frame, (2, 0, 1))\n",
    "            frame = frame.reshape(1, frame.shape[0], frame.shape[1], frame.shape[2])\n",
    "            frame = frame / 255.0\n",
    "\n",
    "            # Analyze the frame\n",
    "            pose = dlc_live.get_pose(frame, pose_model=pose_model)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {frame_index}: {e}\")\n",
    "            break\n",
    "\n",
    "        # Explicitly delete objects to free memory\n",
    "        del frame, pose\n",
    "        torch.cuda.empty_cache()  # Clear GPU cache if using GPU\n",
    "        gc.collect()  # Force garbage collection\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(\"Video capture released.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame 0...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 1...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 2...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 3...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 4...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 5...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 6...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 7...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 8...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 9...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 10...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 11...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 12...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 13...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 14...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 15...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 16...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 17...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 18...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 19...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 20...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 21...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 22...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 23...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 24...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 25...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 26...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 27...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 28...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 29...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 30...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 31...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 32...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 33...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 34...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 35...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 36...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 37...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 38...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 39...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 40...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 41...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 42...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 43...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 44...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 45...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 46...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 47...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 48...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 49...\n",
      "a\n",
      "b\n",
      "c\n",
      "Processing frame 50...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "dlc_proc = Processor()\n",
    "dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt', model_type='pytorch') #change model type to 'pytorch' for raw .pt model\n",
    "\n",
    "#very short video\n",
    "#video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_03s.avi'\n",
    "#short video\n",
    "video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_1s.avi'\n",
    "\n",
    "poses = analyze_video2(video_path, dlc_live)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to use benchmarking_2 script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dlc_proc = Processor()\n",
    "dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt', model_type='pytorch') #change model type to 'pytorch' for raw .pt model\n",
    "\n",
    "#very short video\n",
    "video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_03s.avi'\n",
    "#short video\n",
    "#video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_1s.avi'\n",
    "from dlclive.benchmarking_2 import benchmarking_videos\n",
    "#poses = analyze_video2(video_path, dlc_live)\n",
    "\n",
    "pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\"\n",
    "snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt'\n",
    "video_paths='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_1s.avi'\n",
    "\n",
    "poses = benchmarking_videos(pytorch_cfg, snapshot, video_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to use original benchmark.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlclive.benchmark import benchmark_videos\n",
    "\n",
    "pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\"\n",
    "#snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt'\n",
    "video_paths='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_03s.avi'\n",
    "\n",
    "benchmark_videos(pytorch_cfg, video_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplabcut3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
