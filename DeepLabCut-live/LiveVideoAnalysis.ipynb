{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate model if it does not yet exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In case you do not have a .onnx model exported, use this cell to export your DLC3.0 snapshot\n",
    "\n",
    "# from dlclive import DLCLive\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "# import time\n",
    "\n",
    "# from deeplabcut.pose_estimation_pytorch.config import read_config_as_dict\n",
    "# from deeplabcut.pose_estimation_pytorch.models import PoseModel\n",
    "# import torch\n",
    "# import onnxruntime as ort\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# #Anna\n",
    "# root = Path(\"/Users/annastuckert/Downloads/Hand-AnnaStuckert-2024-08-21/dlc-models-pytorch/iteration-0/HandAug21-trainset95shuffle101/train\")\n",
    "# model_cfg = read_config_as_dict(root / \"pytorch_config.yaml\")\n",
    "# weights_path = root / \"snapshot-200.pt\"\n",
    "\n",
    "# model = PoseModel.build(model_cfg[\"model\"])\n",
    "# weights = torch.load(weights_path, map_location=device)\n",
    "# model.load_state_dict(weights[\"model\"])\n",
    "\n",
    "# dummy_input = torch.zeros((1, 3, 224, 224))\n",
    "\n",
    "# torch.onnx.export(\n",
    "#     model,\n",
    "#     dummy_input,\n",
    "#     \"/Users/annastuckert/Downloads/Hand-AnnaStuckert-2024-08-21/dlc-models-pytorch/iteration-0/HandAug21-trainset95shuffle101/train/resnet.onnx\",\n",
    "#     verbose=False,\n",
    "#     input_names=[\"input\"],\n",
    "#     dynamic_axes={\"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run live video inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Adding stopping mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import colorcet as cc\n",
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import ImageColor\n",
    "\n",
    "\n",
    "def analyze_live_video(\n",
    "    dlc_live,\n",
    "    camera: float = 0,\n",
    "    experiment_name: str = \"Test\",\n",
    "    pcutoff=0.5,\n",
    "    display_radius=5,\n",
    "    resize=None,\n",
    "    save_poses=False,\n",
    "    save_dir=\"model_predictions\",\n",
    "    draw_keypoint_names=False,\n",
    "    cmap=\"bmy\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze a video to track keypoints using an imported DeepLabCut model, visualize keypoints on the video, and optionally save the keypoint data and the labelled video.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dlc_live : DLCLive\n",
    "        An instance of the DLCLive class.\n",
    "    camera : float, deafult=0 (webcam)\n",
    "        The camera to record the live video from\n",
    "    experiment_name : str, default = \"Test\"\n",
    "        Prefix to label generated pose and video files\n",
    "    pcutoff : float, optional, default=0.5\n",
    "        The probability cutoff value below which keypoints are not visualized.\n",
    "    display_radius : int, optional, default=5\n",
    "        The radius of the circles drawn to represent keypoints on the video frames.\n",
    "    resize : tuple of int (width, height) or None, optional, default=None\n",
    "        The size to which the frames should be resized. If None, the frames are not resized.\n",
    "    save_poses : bool, optional, default=False\n",
    "        Whether to save the detected poses to CSV and HDF5 files.\n",
    "    save_dir : str, optional, default=\"model_predictions\"\n",
    "        The directory where the output video and pose data will be saved.\n",
    "    draw_keypoint_names : bool, optional, default=False\n",
    "        Whether to draw the names of the keypoints on the video frames.\n",
    "    cmap : str, optional, default=\"bmy\"\n",
    "        The colormap from the colorcet library to use for keypoint visualization.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    poses : list of dict\n",
    "        A list of dictionaries where each dictionary contains the frame number and the corresponding pose data.\n",
    "    \"\"\"\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(name=save_dir, exist_ok=True)\n",
    "\n",
    "    # Load video\n",
    "    cap = cv2.VideoCapture(camera)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return\n",
    "    # Start empty dict to save poses to for each frame\n",
    "    poses = []\n",
    "    # Create variable indicate current frame. Later in the code +1 is added to frame_index\n",
    "    frame_index = 0\n",
    "\n",
    "    # Load the DLC model\n",
    "    try:\n",
    "        pose_model = dlc_live.load_model()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Could not load DLC model. Details: {e}\")\n",
    "        return\n",
    "\n",
    "    # Retrieve bodypart names and number of keypoints\n",
    "    bodyparts = dlc_live.cfg[\"metadata\"][\"bodyparts\"]\n",
    "    num_keypoints = len(bodyparts)\n",
    "\n",
    "    # Set colors and convert to RGB\n",
    "    cmap_colors = getattr(cc, cmap)\n",
    "    colors = [\n",
    "        ImageColor.getrgb(color)\n",
    "        for color in cmap_colors[:: int(len(cmap_colors) / num_keypoints)]\n",
    "    ]\n",
    "\n",
    "    # Define output video path\n",
    "    output_video_path = os.path.join(save_dir, f\"{experiment_name}_DLCLIVE_LABELLED.mp4\")\n",
    "\n",
    "    # Get video writer setup\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width, frame_height = (\n",
    "        int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "    )\n",
    "\n",
    "    if resize:\n",
    "        frame_width, frame_height = resize\n",
    "    vwriter = cv2.VideoWriter(\n",
    "        filename=output_video_path,\n",
    "        fourcc=fourcc,\n",
    "        fps=fps,\n",
    "        frameSize=(frame_width, frame_height),\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            pose = dlc_live.get_pose(frame, pose_model=pose_model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing frame {frame_index}: {e}\")\n",
    "            continue\n",
    "\n",
    "        poses.append({\"frame\": frame_index, \"pose\": pose})\n",
    "\n",
    "        # Visualize keypoints\n",
    "        this_pose = pose[\"poses\"][0][0]\n",
    "        for j in range(this_pose.shape[0]):\n",
    "            if this_pose[j, 2] > pcutoff:\n",
    "                x, y = map(int, this_pose[j, :2])\n",
    "                cv2.circle(\n",
    "                    frame,\n",
    "                    center=(x, y),\n",
    "                    radius=display_radius,\n",
    "                    color=colors[j],\n",
    "                    thickness=-1,\n",
    "                )\n",
    "\n",
    "                if draw_keypoint_names:\n",
    "                    cv2.putText(\n",
    "                        frame,\n",
    "                        text=bodyparts[j],\n",
    "                        org=(x + 10, y),\n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        fontScale=0.5,\n",
    "                        color=colors[j],\n",
    "                        thickness=1,\n",
    "                        lineType=cv2.LINE_AA,\n",
    "                    )\n",
    "\n",
    "        if resize:\n",
    "            frame = cv2.resize(src=frame, dsize=resize)\n",
    "\n",
    "        vwriter.write(image=frame)\n",
    "        frame_index += 1\n",
    "\n",
    "    cap.release()\n",
    "    vwriter.release()\n",
    "\n",
    "    if save_poses:\n",
    "        save_poses_to_files(experiment_name, save_dir, bodyparts, poses)\n",
    "\n",
    "    return poses\n",
    "\n",
    "\n",
    "def save_poses_to_files(experiment_name, save_dir, bodyparts, poses):\n",
    "    \"\"\"\n",
    "    Save the keypoint poses detected in the video to CSV and HDF5 files.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    experiment_name : str, default = \"Test\"\n",
    "        Prefix to label generated pose and video files\n",
    "    save_dir : str\n",
    "        The directory where the pose data files will be saved.\n",
    "    bodyparts : list of str\n",
    "        A list of body part names corresponding to the keypoints.\n",
    "    poses : list of dict\n",
    "        A list of dictionaries where each dictionary contains the frame number and the corresponding pose data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    base_filename = os.path.splitext(os.path.basename(experiment_name))[0]\n",
    "    csv_save_path = os.path.join(save_dir, f\"{base_filename}_poses.csv\")\n",
    "    h5_save_path = os.path.join(save_dir, f\"{base_filename}_poses.h5\")\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(csv_save_path, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        header = [\"frame\"] + [\n",
    "            f\"{bp}_{axis}\" for bp in bodyparts for axis in [\"x\", \"y\", \"confidence\"]\n",
    "        ]\n",
    "        writer.writerow(header)\n",
    "        for entry in poses:\n",
    "            frame_num = entry[\"frame\"]\n",
    "            pose = entry[\"pose\"][\"poses\"][0][0]\n",
    "            row = [frame_num] + [item for kp in pose for item in kp]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # Save to HDF5\n",
    "    with h5py.File(h5_save_path, \"w\") as hf:\n",
    "        hf.create_dataset(name=\"frames\", data=[entry[\"frame\"] for entry in poses])\n",
    "        for i, bp in enumerate(bodyparts):\n",
    "            hf.create_dataset(\n",
    "                name=f\"{bp}_x\",\n",
    "                data=[entry[\"pose\"][\"poses\"][0][0][i, 0].item() for entry in poses],\n",
    "            )\n",
    "            hf.create_dataset(\n",
    "                name=f\"{bp}_y\",\n",
    "                data=[entry[\"pose\"][\"poses\"][0][0][i, 1].item() for entry in poses],\n",
    "            )\n",
    "            hf.create_dataset(\n",
    "                name=f\"{bp}_confidence\",\n",
    "                data=[entry[\"pose\"][\"poses\"][0][0][i, 2].item() for entry in poses],\n",
    "            )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annastuckert/anaconda3/envs/deeplabcut3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-21 16:20:13.156 python[43079:2032191] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX inference took 3.740891933441162 sec\n",
      "ONNX inference took 3.3362958431243896 sec\n",
      "ONNX inference took 3.5694820880889893 sec\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from dlclive import DLCLive\n",
    "\n",
    "dlc_live = DLCLive(\n",
    "    path=\"/Users/annastuckert/Downloads/Hand-AnnaStuckert-2024-08-21/dlc-models-pytorch/iteration-0/HandAug21-trainset95shuffle101/train\",\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cpu\",\n",
    "    display=True,\n",
    ")\n",
    "#short video\n",
    "#video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_1s.avi'\n",
    "#working_directory =\n",
    "#providing output directory\n",
    "poses = analyze_live_video(dlc_live=dlc_live, camera=0, save_poses=True, save_dir='output_directory', draw_keypoint_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it until you press Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import h5py\n",
    "import colorcet as cc\n",
    "import numpy as np\n",
    "from PIL import ImageColor\n",
    "\n",
    "\n",
    "def analyze_live_video(\n",
    "    dlc_live,\n",
    "    camera: float = 0,\n",
    "    experiment_name: str = \"Test\",\n",
    "    pcutoff=0.5,\n",
    "    display_radius=5,\n",
    "    resize=None,\n",
    "    save_poses=False,\n",
    "    save_dir=\"model_predictions\",\n",
    "    draw_keypoint_names=False,\n",
    "    cmap=\"bmy\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze a video to track keypoints using an imported DeepLabCut model, visualize keypoints on the video, and optionally save the keypoint data and the labelled video.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dlc_live : DLCLive\n",
    "        An instance of the DLCLive class.\n",
    "    camera : float, default=0 (webcam)\n",
    "        The camera to record the live video from\n",
    "    experiment_name : str, default = \"Test\"\n",
    "        Prefix to label generated pose and video files\n",
    "    pcutoff : float, optional, default=0.5\n",
    "        The probability cutoff value below which keypoints are not visualized.\n",
    "    display_radius : int, optional, default=5\n",
    "        The radius of the circles drawn to represent keypoints on the video frames.\n",
    "    resize : tuple of int (width, height) or None, optional, default=None\n",
    "        The size to which the frames should be resized. If None, the frames are not resized.\n",
    "    save_poses : bool, optional, default=False\n",
    "        Whether to save the detected poses to CSV and HDF5 files.\n",
    "    save_dir : str, optional, default=\"model_predictions\"\n",
    "        The directory where the output video and pose data will be saved.\n",
    "    draw_keypoint_names : bool, optional, default=False\n",
    "        Whether to draw the names of the keypoints on the video frames.\n",
    "    cmap : str, optional, default=\"bmy\"\n",
    "        The colormap from the colorcet library to use for keypoint visualization.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    poses : list of dict\n",
    "        A list of dictionaries where each dictionary contains the frame number and the corresponding pose data.\n",
    "    \"\"\"\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(name=save_dir, exist_ok=True)\n",
    "\n",
    "    # Load video\n",
    "    cap = cv2.VideoCapture(camera)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video source {camera}\")\n",
    "        return\n",
    "\n",
    "    # Define output video path\n",
    "    output_video_path = os.path.join(save_dir, f\"{experiment_name}_DLCLIVE_LABELLED.mp4\")\n",
    "\n",
    "    # Get video writer setup\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width, frame_height = (\n",
    "        int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "    )\n",
    "\n",
    "    if resize:\n",
    "        frame_width, frame_height = resize\n",
    "\n",
    "    vwriter = cv2.VideoWriter(\n",
    "        filename=output_video_path,\n",
    "        fourcc=fourcc,\n",
    "        fps=fps,\n",
    "        frameSize=(frame_width, frame_height),\n",
    "    )\n",
    "\n",
    "    # Load the DLC model\n",
    "    try:\n",
    "        pose_model = dlc_live.load_model()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Could not load DLC model. Details: {e}\")\n",
    "        cap.release()\n",
    "        return\n",
    "\n",
    "    # Retrieve bodypart names and number of keypoints\n",
    "    bodyparts = dlc_live.cfg[\"metadata\"][\"bodyparts\"]\n",
    "    num_keypoints = len(bodyparts)\n",
    "\n",
    "    # Set colors and convert to RGB\n",
    "    cmap_colors = getattr(cc, cmap)\n",
    "    colors = [\n",
    "        ImageColor.getrgb(color)\n",
    "        for color in cmap_colors[:: int(len(cmap_colors) / num_keypoints)]\n",
    "    ]\n",
    "\n",
    "    poses = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            pose = dlc_live.get_pose(frame, pose_model=pose_model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing frame {len(poses)}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Log inference time\n",
    "        inference_time = time.time() - start_time\n",
    "        print(f\"Frame {len(poses)} processed in {inference_time:.2f} seconds.\")\n",
    "\n",
    "        poses.append({\"frame\": len(poses), \"pose\": pose})\n",
    "\n",
    "        # Visualize keypoints\n",
    "        this_pose = pose[\"poses\"][0][0]\n",
    "        for j in range(this_pose.shape[0]):\n",
    "            if this_pose[j, 2] > pcutoff:\n",
    "                x, y = map(int, this_pose[j, :2])\n",
    "                cv2.circle(\n",
    "                    frame,\n",
    "                    center=(x, y),\n",
    "                    radius=display_radius,\n",
    "                    color=colors[j],\n",
    "                    thickness=-1,\n",
    "                )\n",
    "\n",
    "                if draw_keypoint_names:\n",
    "                    cv2.putText(\n",
    "                        frame,\n",
    "                        text=bodyparts[j],\n",
    "                        org=(x + 10, y),\n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        fontScale=0.5,\n",
    "                        color=colors[j],\n",
    "                        thickness=1,\n",
    "                        lineType=cv2.LINE_AA,\n",
    "                    )\n",
    "\n",
    "        if resize:\n",
    "            frame = cv2.resize(src=frame, dsize=resize)\n",
    "\n",
    "        vwriter.write(image=frame)\n",
    "\n",
    "        # Display the frame (optional)\n",
    "        cv2.imshow('Live Inference', frame)\n",
    "\n",
    "        # Check if 'q' key was pressed to stop recording\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Video capture stopped by user.\")\n",
    "            break\n",
    "\n",
    "    # Release resources properly\n",
    "    cap.release()\n",
    "    vwriter.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if save_poses:\n",
    "        save_poses_to_files(experiment_name, save_dir, bodyparts, poses)\n",
    "\n",
    "    print(\"Analysis done.\")\n",
    "    return poses\n",
    "\n",
    "\n",
    "def save_poses_to_files(experiment_name, save_dir, bodyparts, poses):\n",
    "    \"\"\"\n",
    "    Save the keypoint poses detected in the video to CSV and HDF5 files.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    experiment_name : str, default = \"Test\"\n",
    "        Prefix to label generated pose and video files\n",
    "    save_dir : str\n",
    "        The directory where the pose data files will be saved.\n",
    "    bodyparts : list of str\n",
    "        A list of body part names corresponding to the keypoints.\n",
    "    poses : list of dict\n",
    "        A list of dictionaries where each dictionary contains the frame number and the corresponding pose data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    base_filename = os.path.splitext(os.path.basename(experiment_name))[0]\n",
    "    csv_save_path = os.path.join(save_dir, f\"{base_filename}_poses.csv\")\n",
    "    h5_save_path = os.path.join(save_dir, f\"{base_filename}_poses.h5\")\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(csv_save_path, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        header = [\"frame\"] + [\n",
    "            f\"{bp}_{axis}\" for bp in bodyparts for axis in [\"x\", \"y\", \"confidence\"]\n",
    "        ]\n",
    "        writer.writerow(header)\n",
    "        for entry in poses:\n",
    "            frame_num = entry[\"frame\"]\n",
    "            pose = entry[\"pose\"][\"poses\"][0][0]\n",
    "            row = [frame_num] + [item for kp in pose for item in kp]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # Save to HDF5\n",
    "    with h5py.File(h5_save_path, \"w\") as hf:\n",
    "        hf.create_dataset(name=\"frames\", data=[entry[\"frame\"] for entry in poses])\n",
    "        for i, bp in enumerate(bodyparts):\n",
    "            hf.create_dataset(\n",
    "                name=f\"{bp}_x\",\n",
    "                data=[entry[\"pose\"][\"poses\"][0][0][i, 0].item() for entry in poses],\n",
    "            )\n",
    "            hf.create_dataset(\n",
    "                name=f\"{bp}_y\",\n",
    "                data=[entry[\"pose\"][\"poses\"][0][0][i, 1].item() for entry in poses],\n",
    "            )\n",
    "            hf.create_dataset(\n",
    "                name=f\"{bp}_confidence\",\n",
    "                data=[entry[\"pose\"][\"poses\"][0][0][i, 2].item() for entry in poses],\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it for 1 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import h5py\n",
    "import colorcet as cc\n",
    "import numpy as np\n",
    "from PIL import ImageColor\n",
    "import cProfile\n",
    "import pstats\n",
    "from dlclive import DLCLive\n",
    "\n",
    "\n",
    "def analyze_live_video(\n",
    "    dlc_live,\n",
    "    camera: float = 0,\n",
    "    experiment_name: str = \"Test\",\n",
    "    pcutoff=0.5,\n",
    "    display_radius=5,\n",
    "    resize=None,\n",
    "    save_poses=False,\n",
    "    save_dir=\"model_predictions\",\n",
    "    draw_keypoint_names=False,\n",
    "    cmap=\"bmy\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze a video to track keypoints using an imported DeepLabCut model, visualize keypoints on the video, and optionally save the keypoint data and the labelled video.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dlc_live : DLCLive\n",
    "        An instance of the DLCLive class.\n",
    "    camera : float, default=0 (webcam)\n",
    "        The camera to record the live video from\n",
    "    experiment_name : str, default = \"Test\"\n",
    "        Prefix to label generated pose and video files\n",
    "    pcutoff : float, optional, default=0.5\n",
    "        The probability cutoff value below which keypoints are not visualized.\n",
    "    display_radius : int, optional, default=5\n",
    "        The radius of the circles drawn to represent keypoints on the video frames.\n",
    "    resize : tuple of int (width, height) or None, optional, default=None\n",
    "        The size to which the frames should be resized. If None, the frames are not resized.\n",
    "    save_poses : bool, optional, default=False\n",
    "        Whether to save the detected poses to CSV and HDF5 files.\n",
    "    save_dir : str, optional, default=\"model_predictions\"\n",
    "        The directory where the output video and pose data will be saved.\n",
    "    draw_keypoint_names : bool, optional, default=False\n",
    "        Whether to draw the names of the keypoints on the video frames.\n",
    "    cmap : str, optional, default=\"bmy\"\n",
    "        The colormap from the colorcet library to use for keypoint visualization.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    poses : list of dict\n",
    "        A list of dictionaries where each dictionary contains the frame number and the corresponding pose data.\n",
    "    \"\"\"\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(name=save_dir, exist_ok=True)\n",
    "\n",
    "    # Load video\n",
    "    cap = cv2.VideoCapture(camera)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video source {camera}\")\n",
    "        return\n",
    "\n",
    "    # Define output video path\n",
    "    output_video_path = os.path.join(save_dir, f\"{experiment_name}_DLCLIVE_LABELLED.mp4\")\n",
    "\n",
    "    # Get video writer setup\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width, frame_height = (\n",
    "        int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "    )\n",
    "\n",
    "    if resize:\n",
    "        frame_width, frame_height = resize\n",
    "\n",
    "    vwriter = cv2.VideoWriter(\n",
    "        filename=output_video_path,\n",
    "        fourcc=fourcc,\n",
    "        fps=fps,\n",
    "        frameSize=(frame_width, frame_height),\n",
    "    )\n",
    "\n",
    "    # Load the DLC model\n",
    "    try:\n",
    "        pose_model = dlc_live.load_model()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Could not load DLC model. Details: {e}\")\n",
    "        cap.release()\n",
    "        return\n",
    "\n",
    "    # Retrieve bodypart names and number of keypoints\n",
    "    bodyparts = dlc_live.cfg[\"metadata\"][\"bodyparts\"]\n",
    "    num_keypoints = len(bodyparts)\n",
    "\n",
    "    # Set colors and convert to RGB\n",
    "    cmap_colors = getattr(cc, cmap)\n",
    "    colors = [\n",
    "        ImageColor.getrgb(color)\n",
    "        for color in cmap_colors[:: int(len(cmap_colors) / num_keypoints)]\n",
    "    ]\n",
    "\n",
    "    poses = []\n",
    "    frames = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Capture the frame and store it\n",
    "        frames.append(frame.copy())\n",
    "\n",
    "        # Calculate elapsed time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time > 1.0:  # Stop capturing after 1 second\n",
    "            print(\"Recording stopped after 1 second.\")\n",
    "            break\n",
    "\n",
    "        # Display the frame (optional)\n",
    "        cv2.imshow('Recording...', frame)\n",
    "\n",
    "        # Check if 'q' key was pressed to stop recording early\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Video capture stopped by user.\")\n",
    "            break\n",
    "\n",
    "    # Release the video capture resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Start profiling the processing of frames\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "\n",
    "    # Process the captured frames\n",
    "    for i, frame in enumerate(frames):\n",
    "        try:\n",
    "            pose = dlc_live.get_pose(frame, pose_model=pose_model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing frame {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        poses.append({\"frame\": i, \"pose\": pose})\n",
    "\n",
    "        # Visualize keypoints\n",
    "        this_pose = pose[\"poses\"][0][0]\n",
    "        for j in range(this_pose.shape[0]):\n",
    "            if this_pose[j, 2] > pcutoff:\n",
    "                x, y = map(int, this_pose[j, :2])\n",
    "                cv2.circle(\n",
    "                    frame,\n",
    "                    center=(x, y),\n",
    "                    radius=display_radius,\n",
    "                    color=colors[j],\n",
    "                    thickness=-1,\n",
    "                )\n",
    "\n",
    "                if draw_keypoint_names:\n",
    "                    cv2.putText(\n",
    "                        frame,\n",
    "                        text=bodyparts[j],\n",
    "                        org=(x + 10, y),\n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        fontScale=0.5,\n",
    "                        color=colors[j],\n",
    "                        thickness=1,\n",
    "                        lineType=cv2.LINE_AA,\n",
    "                    )\n",
    "\n",
    "        if resize:\n",
    "            frame = cv2.resize(src=frame, dsize=resize)\n",
    "\n",
    "        vwriter.write(image=frame)\n",
    "\n",
    "    # Stop profiling\n",
    "    profiler.disable()\n",
    "\n",
    "    # Release the video writer resources\n",
    "    vwriter.release()\n",
    "\n",
    "    if save_poses:\n",
    "        save_poses_to_files(experiment_name, save_dir, bodyparts, poses)\n",
    "\n",
    "    print(\"Analysis done.\")\n",
    "\n",
    "    # Print profiling results\n",
    "    profiler.print_stats(sort='cumtime')\n",
    "\n",
    "    return poses\n",
    "\n",
    "\n",
    "def save_poses_to_files(experiment_name, save_dir, bodyparts, poses):\n",
    "    \"\"\"\n",
    "    Save the keypoint poses detected in the video to CSV and HDF5 files.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    experiment_name : str, default = \"Test\"\n",
    "        Prefix to label generated pose and video files\n",
    "    save_dir : str\n",
    "        The directory where the pose data files will be saved.\n",
    "    bodyparts : list of str\n",
    "        A list of body part names corresponding to the keypoints.\n",
    "    poses : list of dict\n",
    "        A list of dictionaries where each dictionary contains the frame number and the corresponding pose data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    base_filename = os.path.splitext(os.path.basename(experiment_name))[0]\n",
    "    csv_save_path = os.path.join(save_dir, f\"{base_filename}_poses.csv\")\n",
    "    h5_save_path = os.path.join(save_dir, f\"{base_filename}_poses.h5\")\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(csv_save_path, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        header = [\"frame\"] + [\n",
    "            f\"{bp}_{axis}\" for bp in bodyparts for axis in [\"x\", \"y\", \"confidence\"]\n",
    "        ]\n",
    "        writer.writerow(header)\n",
    "        for entry in poses:\n",
    "            frame_num = entry[\"frame\"]\n",
    "            pose = entry[\"pose\"][\"poses\"][0][0]\n",
    "            row = [frame_num] + [item for kp in pose for item in kp]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # Save to HDF5\n",
    "    with h5py.File(h5_save_path, \"w\") as hf:\n",
    "        hf.create_dataset(name=\"frames\", data=[entry[\"frame\"] for entry in poses])\n",
    "        for i, bp in enumerate(bodyparts):\n",
    "            hf.create_dataset(\n",
    "                name=f\"{bp}_x\",\n",
    "                data=[entry[\"pose\"][\"poses\"][0][0][i, 0].item() for entry in poses],\n",
    "            )\n",
    "            hf.create_dataset(\n",
    "                name=f\"{bp}_y\",\n",
    "                data=[entry[\"pose\"][\"poses\"][0][0][i, 1].item() for entry in poses],\n",
    "            )\n",
    "            hf.create_dataset(\n",
    "                name=f\"{bp}_confidence\",\n",
    "                data=[entry[\"pose\"][\"poses\"][0][0][i, 2].item() for entry in poses],\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording stopped after 1 second.\n",
      "ONNX inference took 2.6377220153808594 sec\n",
      "ONNX inference took 3.4302010536193848 sec\n",
      "ONNX inference took 3.005439043045044 sec\n",
      "ONNX inference took 2.7167038917541504 sec\n",
      "ONNX inference took 3.2396998405456543 sec\n",
      "ONNX inference took 3.641049861907959 sec\n",
      "ONNX inference took 2.3327910900115967 sec\n",
      "ONNX inference took 1.8497438430786133 sec\n",
      "ONNX inference took 1.8688640594482422 sec\n",
      "ONNX inference took 2.0275509357452393 sec\n",
      "ONNX inference took 2.119837999343872 sec\n",
      "ONNX inference took 1.881242275238037 sec\n",
      "ONNX inference took 3.470150947570801 sec\n",
      "ONNX inference took 3.8359780311584473 sec\n",
      "ONNX inference took 2.255268096923828 sec\n",
      "ONNX inference took 3.6002070903778076 sec\n",
      "ONNX inference took 3.729128837585449 sec\n",
      "ONNX inference took 3.662476062774658 sec\n",
      "ONNX inference took 3.599925994873047 sec\n",
      "ONNX inference took 3.7375919818878174 sec\n",
      "ONNX inference took 2.479177236557007 sec\n",
      "ONNX inference took 2.3914618492126465 sec\n",
      "ONNX inference took 2.0444281101226807 sec\n",
      "ONNX inference took 3.1075057983398438 sec\n",
      "Analysis done.\n"
     ]
    }
   ],
   "source": [
    "from dlclive import DLCLive\n",
    "\n",
    "dlc_live = DLCLive(\n",
    "    path=\"/Users/annastuckert/Downloads/Hand-AnnaStuckert-2024-08-21/dlc-models-pytorch/iteration-0/HandAug21-trainset95shuffle101/train\",\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cpu\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "\n",
    "poses = analyze_live_video(\n",
    "    dlc_live=dlc_live,\n",
    "    camera=0,\n",
    "    save_poses=True,\n",
    "    save_dir='output_directory',\n",
    "    draw_keypoint_names=True)\n",
    "\n",
    "#OBS actually takes longer to run on resized than original videos!\n",
    "# resized_width = 320  # Desired width of the video\n",
    "# resized_height = 240  # Desired height of the video\n",
    "\n",
    "# poses = analyze_live_video(\n",
    "#     dlc_live=dlc_live,\n",
    "#     camera=0,\n",
    "#     save_poses=True,\n",
    "#     save_dir='output_directory',\n",
    "#     draw_keypoint_names=True,\n",
    "#     resize=(resized_width, resized_height)  # Resize parameter\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplabcut3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
