{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLC Live PyTorch Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlclive import DLCLive\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from onnxruntime import quantization\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "import glob\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from onnxconverter_common import float16\n",
    "\n",
    "from deeplabcut.pose_estimation_pytorch.config import read_config_as_dict\n",
    "from deeplabcut.pose_estimation_pytorch.models import PoseModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root directory\n",
    "root = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfd543b54b54cb693bb92a0f539b743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Choose a Project!', options=('fly-kevin', 'hand-track', 'superanimal_quadruped_model', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projects = [d for d in next(os.walk(root))[1]]\n",
    "project_selection = widgets.Dropdown(\n",
    "    options=projects,\n",
    "    value=projects[0],\n",
    "    description=\"Choose a Project!\",\n",
    "    disabled=False,\n",
    ")\n",
    "display(project_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get working directory and read model pytorch config file\n",
    "working_dir = os.path.join(root, project_selection.value)\n",
    "model_cfg = read_config_as_dict(Path(working_dir) / \"pytorch_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e62ebf4e43941d7a00b7e1864d889b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Choose your snapshot!', options=('snapshot-100.pt', 'snapshot-detector-250.pt'), value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snapshots = glob.glob(os.path.normpath(working_dir + \"/*.pt*\"))\n",
    "snapshots = [s.split(\"/\")[-1] for s in snapshots]\n",
    "snapshot_selection = widgets.Dropdown(\n",
    "    options=snapshots,\n",
    "    value=snapshots[0],\n",
    "    description=\"Choose your snapshot!\",\n",
    "    disabled=False,\n",
    ")\n",
    "display(snapshot_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model exporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74394/1996578004.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(snapshot_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# Get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "snapshot_path = os.path.join(working_dir, snapshot_selection.value)\n",
    "# Create pretrained DLC3 PoseModel from snapshot  \n",
    "model = PoseModel.build(model_cfg[\"model\"])\n",
    "weights = torch.load(snapshot_path, map_location=device)\n",
    "model.load_state_dict(weights[\"model\"])\n",
    "\n",
    "# Random dummy input\n",
    "dummy_input = torch.zeros((1, 3, 224, 224))\n",
    "\n",
    "# Export PyTorch model to ONNX\n",
    "# Replace with your onnx model name\n",
    "onnx_model_name = \"superbird_ssdlite.onnx\" \n",
    "onnx_model_path = os.path.join(working_dir, onnx_model_name)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_model_path,\n",
    "    verbose=False,\n",
    "    input_names=[\"input\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}},\n",
    ")\n",
    "#* Note: batch_size, height, and width dimensions are dynamic, \n",
    "#* i.e. the onnx model can take different batch sizes, heights and width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX fp32 to ONNX fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32 to FP16 conversion\n",
    "onnx_fp16_model_name = onnx_model_name + \"_fp16.onnx\"\n",
    "onnx_fp16_model_path = os.path.join(working_dir, onnx_fp16_model_name)\n",
    "\n",
    "model_fp32 = onnx.load(onnx_model_path)\n",
    "model_fp16 = float16.convert_float_to_float16(model_fp32)\n",
    "onnx.save(model_fp16, onnx_fp16_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading.... superanimal_quadruped_hrnetw32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb64d8c96f24a8680e3673485f9903e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pose_model.pth:   0%|          | 0.00/160M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee319ada8506428cb99eed8da614c016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "detector.pt:   0%|          | 0.00/518M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dlclibrary import download_huggingface_model\n",
    "\n",
    "# Creates a folder and downloads the model to it\n",
    "model_dir = Path(\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superanimal_quadruped_model_pytorch\")\n",
    "model_dir.mkdir()\n",
    "download_huggingface_model(\"superanimal_quadruped_hrnetw32\", model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ea1959526f42508817929bb6a6c3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Choose a DLC ModelZoo model!', options=('full_human', 'full_cat', 'full_dog', 'primate_f…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "model_options = dlc.create_project.modelzoo.Modeloptions\n",
    "model_selection = widgets.Dropdown(\n",
    "    options=model_options,\n",
    "    value=model_options[0],\n",
    "    description=\"Choose a DLC ModelZoo model!\",\n",
    "    disabled=False,\n",
    ")\n",
    "display(model_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"superanimal_quadruped\"\n",
    "your_name = \"dlc24_residents\"\n",
    "model2use = model_selection.value\n",
    "video_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/1_20cms_0degUP_first.avi\"\n",
    "videotype = os.path.splitext(\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/1_20cms_0degUP_first.avi\"\n",
    ")[-1].lstrip(\n",
    "    \".\"\n",
    ")  # or MOV, or avi, whatever you uploaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path, train_config_path = dlc.create_pretrained_project(\n",
    "    project_name,\n",
    "    your_name,\n",
    "    [video_path],\n",
    "    working_directory=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superanimal_quadruped_model_pytorch\",\n",
    "    videotype=videotype,\n",
    "    model=model2use,\n",
    "    analyzevideo=False,\n",
    "    createlabeledvideo=False,\n",
    "    copy_videos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quant ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_fp32_model_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird_ssdlite/superbird_ssdlite.onnx\"\n",
    ")\n",
    "model_prep_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird_ssdlite/resnet_quant_prep.onnx\"\n",
    ")\n",
    "\n",
    "# prep for quantisation\n",
    "quantization.shape_inference.quant_pre_process(\n",
    "    onnx_fp32_model_path, model_prep_path, skip_symbolic_shape=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX model static quantization WITHOUT DATA CALIBRATION\n",
    "quant_resnet = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\"\n",
    "    + projects[3]\n",
    "    + \"/resnet_quant.onnx\"\n",
    ")\n",
    "model = onnx.load(\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3] + \"/resnet.onnx\"\n",
    ")\n",
    "\n",
    "# quant_format = ort.quantization.QuantFormat.QDQ  # Recommended format from 1.11\n",
    "# activation_type = ort.quantization.QuantType.QInt8\n",
    "# weight_type = ort.quantization.QuantType.QInt8\n",
    "\n",
    "\n",
    "class QuantizationDataReader(quantization.CalibrationDataReader):\n",
    "    def __init__(self, torch_ds, batch_size, input_name):\n",
    "\n",
    "        self.torch_dl = torch.utils.data.DataLoader(\n",
    "            torch_ds, batch_size=batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "        self.input_name = input_name\n",
    "        self.datasize = len(self.torch_dl)\n",
    "\n",
    "        self.enum_data = iter(self.torch_dl)\n",
    "\n",
    "    def to_numpy(self, pt_tensor):\n",
    "        return (\n",
    "            pt_tensor.detach().cpu().numpy()\n",
    "            if pt_tensor.requires_grad\n",
    "            else pt_tensor.cpu().numpy()\n",
    "        )\n",
    "\n",
    "    def get_next(self):\n",
    "        batch = next(self.enum_data, None)\n",
    "        if batch is not None:\n",
    "            return {self.input_name: self.to_numpy(batch[0])}\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = iter(self.torch_dl)\n",
    "\n",
    "\n",
    "qdr = QuntizationDataReader(\n",
    "    calib_ds, batch_size=2, input_name=ort_sess.get_inputs()[0].name\n",
    ")\n",
    "\n",
    "# quantizer = quantization.QuantizeStatic(model, quant_format, activation_type, weight_type)\n",
    "quant_model = quantization.quantize_static(\n",
    "    model_prep_path,\n",
    "    quant_resnet,\n",
    "    quant_format=quant_format,\n",
    "    activation_type=activation_type,\n",
    "    weight_type=weight_type,\n",
    ")\n",
    "onnx.save(quant_model, quant_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test frame\n",
    "img = cv2.imread(\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird/8aa7edce2e5f44f98f0d0ac73dedc006_553.jpg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with ONNX exported DLC 3.0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model took 0.254169225692749 sec\n",
      "PyTorch inference took 0.003065347671508789 sec\n",
      "PyTorch postprocessing took 0.07213997840881348 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'poses': tensor([[[[5.5056e+02, 3.4709e+02, 5.1385e-01],\n",
       "            [6.9339e+02, 3.7831e+02, 5.1809e-01],\n",
       "            [4.8250e+02, 1.6700e+02, 7.6433e-01],\n",
       "            [5.9321e+02, 3.7561e+02, 3.0986e-01],\n",
       "            [6.2806e+02, 3.4517e+02, 6.5673e-01],\n",
       "            [6.4639e+02, 3.5552e+02, 6.0317e-01],\n",
       "            [6.3539e+02, 3.5684e+02, 5.4960e-01],\n",
       "            [4.6005e+02, 1.6846e+02, 4.6797e-01],\n",
       "            [3.7940e+02, 3.8465e+02, 3.3203e-01],\n",
       "            [4.6718e+02, 3.2919e+02, 2.2717e-01],\n",
       "            [5.8489e+02, 3.5557e+02, 4.6419e-01],\n",
       "            [6.3402e+02, 3.5767e+02, 8.2448e-01],\n",
       "            [4.7162e+02, 1.7686e+02, 5.2562e-01],\n",
       "            [5.4127e+02, 6.1913e+02, 3.4762e-01],\n",
       "            [4.2060e+01, 1.2721e+01, 2.4560e-01],\n",
       "            [4.4452e+02, 5.5158e+01, 9.1034e-01],\n",
       "            [6.3097e+02, 3.7222e+02, 6.1695e-01],\n",
       "            [6.2662e+02, 3.7354e+02, 1.9132e-01],\n",
       "            [4.4740e+02, 5.9445e+01, 3.8444e-01],\n",
       "            [3.2789e+02, 3.9531e+02, 6.1854e-01],\n",
       "            [5.5549e+02, 3.4680e+02, 4.4980e-01],\n",
       "            [4.9916e+02, 3.9047e+02, 6.9780e-01],\n",
       "            [5.0565e+02, 1.6211e+02, 1.9148e-01],\n",
       "            [4.8183e+02, 1.7215e+02, 5.6379e-01],\n",
       "            [4.5955e+02, 1.6836e+02, 4.9937e-01],\n",
       "            [5.6776e+02, 3.5029e+02, 2.5395e-01],\n",
       "            [5.9694e+02, 3.7366e+02, 2.8638e-01],\n",
       "            [6.4648e+02, 3.6131e+02, 5.9041e-01],\n",
       "            [4.8591e+02, 1.6828e+02, 2.7837e-01],\n",
       "            [4.7486e+02, 1.6983e+02, 4.7442e-01],\n",
       "            [4.5529e+02, 1.5224e+02, 3.9059e-01],\n",
       "            [5.8642e+02, 3.5412e+02, 4.4458e-01],\n",
       "            [5.0722e+02, 4.0346e+02, 2.3651e-01],\n",
       "            [5.3065e+02, 5.1945e+02, 2.3412e-01],\n",
       "            [5.6775e+02, 5.0767e+02, 3.3619e-01],\n",
       "            [6.1218e+02, 3.4521e+02, 3.3985e-01],\n",
       "            [5.5182e+02, 3.4943e+02, 3.7414e-01],\n",
       "            [5.1409e+02, 5.3458e+02, 1.6791e-01],\n",
       "            [5.6909e+02, 3.5709e+02, 5.4134e-01],\n",
       "            [3.7337e+01, 4.5473e+00, 3.6217e-01],\n",
       "            [6.5091e+02, 3.6029e+02, 5.5448e-01],\n",
       "            [6.8430e+02, 3.7262e+02, 4.3378e-01]]]])},\n",
       " 0.003065347671508789)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dikra\n",
    "onnx_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird_ssdlite\",\n",
    "    model_type=\"pytorch\",\n",
    "    device=\"cuda\",\n",
    "    display=True,\n",
    "    snapshot=\"snapshot-100.pt\",\n",
    "    # resize=0.7,\n",
    "    # cropping=[0,512,0,512]\n",
    "    # precision=\"FP16\",\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# onnx_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# onnx_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "onnx_pose = onnx_dlc_live.init_inference(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot from 2024-08-20 14-29-53.png](./docs/assets/Screenshot%20from%202024-08-20%2014-36-00.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch inference took 0.01370382308959961 sec\n",
      "PyTorch postprocessing took 0.06238055229187012 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'poses': tensor([[[[5.5056e+02, 3.4709e+02, 5.1385e-01],\n",
       "            [6.9339e+02, 3.7831e+02, 5.1809e-01],\n",
       "            [4.8250e+02, 1.6700e+02, 7.6433e-01],\n",
       "            [5.9321e+02, 3.7561e+02, 3.0986e-01],\n",
       "            [6.2806e+02, 3.4517e+02, 6.5673e-01],\n",
       "            [6.4639e+02, 3.5552e+02, 6.0317e-01],\n",
       "            [6.3539e+02, 3.5684e+02, 5.4960e-01],\n",
       "            [4.6005e+02, 1.6846e+02, 4.6797e-01],\n",
       "            [3.7940e+02, 3.8465e+02, 3.3203e-01],\n",
       "            [4.6718e+02, 3.2919e+02, 2.2717e-01],\n",
       "            [5.8489e+02, 3.5557e+02, 4.6419e-01],\n",
       "            [6.3402e+02, 3.5767e+02, 8.2448e-01],\n",
       "            [4.7162e+02, 1.7686e+02, 5.2562e-01],\n",
       "            [5.4127e+02, 6.1913e+02, 3.4762e-01],\n",
       "            [4.2060e+01, 1.2721e+01, 2.4560e-01],\n",
       "            [4.4452e+02, 5.5158e+01, 9.1034e-01],\n",
       "            [6.3097e+02, 3.7222e+02, 6.1695e-01],\n",
       "            [6.2662e+02, 3.7354e+02, 1.9132e-01],\n",
       "            [4.4740e+02, 5.9445e+01, 3.8444e-01],\n",
       "            [3.2789e+02, 3.9531e+02, 6.1854e-01],\n",
       "            [5.5549e+02, 3.4680e+02, 4.4980e-01],\n",
       "            [4.9916e+02, 3.9047e+02, 6.9780e-01],\n",
       "            [5.0565e+02, 1.6211e+02, 1.9148e-01],\n",
       "            [4.8183e+02, 1.7215e+02, 5.6379e-01],\n",
       "            [4.5955e+02, 1.6836e+02, 4.9937e-01],\n",
       "            [5.6776e+02, 3.5029e+02, 2.5395e-01],\n",
       "            [5.9694e+02, 3.7366e+02, 2.8638e-01],\n",
       "            [6.4648e+02, 3.6131e+02, 5.9041e-01],\n",
       "            [4.8591e+02, 1.6828e+02, 2.7837e-01],\n",
       "            [4.7486e+02, 1.6983e+02, 4.7442e-01],\n",
       "            [4.5529e+02, 1.5224e+02, 3.9059e-01],\n",
       "            [5.8642e+02, 3.5412e+02, 4.4458e-01],\n",
       "            [5.0722e+02, 4.0346e+02, 2.3651e-01],\n",
       "            [5.3065e+02, 5.1945e+02, 2.3412e-01],\n",
       "            [5.6775e+02, 5.0767e+02, 3.3619e-01],\n",
       "            [6.1218e+02, 3.4521e+02, 3.3985e-01],\n",
       "            [5.5182e+02, 3.4943e+02, 3.7414e-01],\n",
       "            [5.1409e+02, 5.3458e+02, 1.6791e-01],\n",
       "            [5.6909e+02, 3.5709e+02, 5.4134e-01],\n",
       "            [3.7337e+01, 4.5473e+00, 3.6217e-01],\n",
       "            [6.5091e+02, 3.6029e+02, 5.5448e-01],\n",
       "            [6.8430e+02, 3.7262e+02, 4.3378e-01]]]])},\n",
       " 0.01370382308959961)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_pose = onnx_dlc_live.get_pose(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with snaptshot of DLC 3.0 model (.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dikra/MyHub/Code/DLC24_Hub/DLC_AI2024/DeepLabCut-live/dlclive/dlclive.py:257: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(model_path, map_location=torch.device(self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model took 0.4676992893218994 sec\n",
      "PyTorch inference took 0.034587860107421875 sec\n",
      "PyTorch postprocessing took 0.0018115043640136719 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'poses': tensor([[[[154.0203, 162.2280,   0.9152],\n",
       "            [146.8848, 158.9930,   0.9459],\n",
       "            [150.3487, 149.1102,   0.9093],\n",
       "            [196.7133, 137.2184,   0.8843],\n",
       "            [204.8920, 172.0188,   0.6791],\n",
       "            [342.8778,  81.4373,   0.6930],\n",
       "            [325.4101, 151.0759,   0.7803],\n",
       "            [240.2807, 110.5330,   0.6488],\n",
       "            [261.0057, 128.8403,   0.6076],\n",
       "            [254.8730, 154.8122,   0.8238],\n",
       "            [385.2763, 112.8773,   0.8098]]]])},\n",
       " 0.034587860107421875)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dikra\n",
    "pytorch_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait\",\n",
    "    snapshot=\"snapshot-263.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# pytorch_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# pytorch_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "pytorch_pose = pytorch_dlc_live.init_inference(frame=img)\n",
    "pytorch_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PyTorch model inference](./docs/assets/Screenshot%20from%202024-08-20%2014-29-53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is faster?\n",
    "\n",
    "Independent frame analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "root = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird\"\n",
    "test_images = glob.glob(os.path.normpath(root + \"/*.jpg\"))\n",
    "\n",
    "\n",
    "def mean_time_inference(dlc_live, images):\n",
    "    poses, times = [], []\n",
    "    for i, img_p in enumerate(images):\n",
    "        print(\"Frame #\", i)\n",
    "        img = cv2.imread(img_p)\n",
    "\n",
    "        if i == 0:\n",
    "            pose, t = dlc_live.init_inference(img)\n",
    "        else:\n",
    "            pose, t = dlc_live.get_pose(img)\n",
    "        poses.append(pose)\n",
    "        times.append(t)\n",
    "\n",
    "    return times, poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fly-kevin', 'hand-track', 'superbird', 'ventral-gait']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame #  0\n",
      "Loading the model took 0.0992898941040039 sec\n",
      "ONNX inference took 1.002748966217041 sec\n",
      "ONNX postprocessing took 0.0037970542907714844 sec\n",
      "Frame #  1\n",
      "ONNX inference took 1.185469627380371 sec\n",
      "ONNX postprocessing took 0.0041692256927490234 sec\n",
      "Frame #  2\n",
      "ONNX inference took 1.1702406406402588 sec\n",
      "ONNX postprocessing took 0.00862884521484375 sec\n",
      "Frame #  3\n",
      "ONNX inference took 1.6595604419708252 sec\n",
      "ONNX postprocessing took 0.004779815673828125 sec\n",
      "Frame #  4\n",
      "ONNX inference took 1.5690531730651855 sec\n",
      "ONNX postprocessing took 0.003983974456787109 sec\n",
      "Frame #  5\n",
      "ONNX inference took 0.3981649875640869 sec\n",
      "ONNX postprocessing took 0.0015933513641357422 sec\n",
      "Frame #  6\n",
      "ONNX inference took 0.5557830333709717 sec\n",
      "ONNX postprocessing took 0.002658367156982422 sec\n",
      "Frame #  7\n",
      "ONNX inference took 1.6932966709136963 sec\n",
      "ONNX postprocessing took 0.006421804428100586 sec\n",
      "Frame #  8\n",
      "ONNX inference took 1.2120893001556396 sec\n",
      "ONNX postprocessing took 0.0041429996490478516 sec\n"
     ]
    }
   ],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird_ssdlite\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"onnx\",\n",
    "    snapshot=\"snapshot-100.pt\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "times, poses = mean_time_inference(dlc_live, test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    snapshot=\"snapshot-200.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\",\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images)\n",
    "print(\n",
    "    f\"Inference of PyTorch model took on average {mean_time} seconds for {len(test_images)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model took 4.69077205657959 sec\n",
      "ONNX inference took 49.87957811355591 sec\n",
      "ONNX postprocessing took 0.0015039443969726562 sec\n",
      "[54.57309126853943]\n",
      "Inference of PyTorch model took on average 54.57309126853943 seconds for 1 images\n"
     ]
    }
   ],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3],\n",
    "    device=\"tensorrt\",\n",
    "    model_type=\"onnx\",\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images)\n",
    "print(\n",
    "    f\"Inference of PyTorch model took on average {mean_time} seconds for {len(test_images)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX inference took 0.02220296859741211 sec\n",
      "ONNX postprocessing took 0.0027968883514404297 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dlc_live.get_pose(img)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video analysis\n",
    "\n",
    "Currently the benchmark_pytorch.py script serves to provide a function for analyzing a preexisting video to test PyTorch for running video inference in DLC-Live. Code for running video inference on a live video feed is WIP.\n",
    "\n",
    "For true benchmarking purposes, we aim to add feature for recording the time it takes to analyze each frame / how many frames can be analyzed per second. Discuss what measure to use and consult the DLC Live paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dikra/miniconda3/envs/dlc-live/lib/python3.10/site-packages/_distutils_hack/__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import the analyze_video function from the file where it's defined\n",
    "from dlclive.benchmark_pytorch import analyze_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dikra/MyHub/Code/DLC24_Hub/DLC_AI2024/DeepLabCut-live/dlclive/dlclive.py:262: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(model_path, map_location=torch.device(self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model took 0.3833181858062744 sec\n",
      "PyTorch inference took 0.14445948600769043 sec\n",
      "PyTorch postprocessing took 0.02237534523010254 sec\n",
      "Frame 0 processing time: 1.0429 seconds\n",
      "PyTorch inference took 0.006478071212768555 sec\n",
      "PyTorch postprocessing took 0.0202486515045166 sec\n",
      "Frame 1 processing time: 0.0595 seconds\n",
      "PyTorch inference took 0.0038559436798095703 sec\n",
      "PyTorch postprocessing took 0.014013528823852539 sec\n",
      "Frame 2 processing time: 0.0553 seconds\n",
      "PyTorch inference took 0.0031075477600097656 sec\n",
      "PyTorch postprocessing took 0.009729623794555664 sec\n",
      "Frame 3 processing time: 0.0370 seconds\n",
      "PyTorch inference took 0.00953817367553711 sec\n",
      "PyTorch postprocessing took 0.012032508850097656 sec\n",
      "Frame 4 processing time: 0.0672 seconds\n",
      "PyTorch inference took 0.005718231201171875 sec\n",
      "PyTorch postprocessing took 0.008227348327636719 sec\n",
      "Frame 5 processing time: 0.0468 seconds\n",
      "PyTorch inference took 0.003618001937866211 sec\n",
      "PyTorch postprocessing took 0.010601282119750977 sec\n",
      "Frame 6 processing time: 0.0380 seconds\n",
      "PyTorch inference took 0.004567384719848633 sec\n",
      "PyTorch postprocessing took 0.011587142944335938 sec\n",
      "Frame 7 processing time: 0.0339 seconds\n",
      "PyTorch inference took 0.003925323486328125 sec\n",
      "PyTorch postprocessing took 0.009025096893310547 sec\n",
      "Frame 8 processing time: 0.0304 seconds\n",
      "PyTorch inference took 0.003171682357788086 sec\n",
      "PyTorch postprocessing took 0.00982213020324707 sec\n",
      "Frame 9 processing time: 0.0320 seconds\n",
      "PyTorch inference took 0.00424504280090332 sec\n",
      "PyTorch postprocessing took 0.008810997009277344 sec\n",
      "Frame 10 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.0033228397369384766 sec\n",
      "PyTorch postprocessing took 0.009706735610961914 sec\n",
      "Frame 11 processing time: 0.0368 seconds\n",
      "PyTorch inference took 0.003975391387939453 sec\n",
      "PyTorch postprocessing took 0.010195016860961914 sec\n",
      "Frame 12 processing time: 0.0292 seconds\n",
      "PyTorch inference took 0.006673336029052734 sec\n",
      "PyTorch postprocessing took 0.009145498275756836 sec\n",
      "Frame 13 processing time: 0.0411 seconds\n",
      "PyTorch inference took 0.003853321075439453 sec\n",
      "PyTorch postprocessing took 0.00960087776184082 sec\n",
      "Frame 14 processing time: 0.0275 seconds\n",
      "PyTorch inference took 0.004338741302490234 sec\n",
      "PyTorch postprocessing took 0.008657455444335938 sec\n",
      "Frame 15 processing time: 0.0344 seconds\n",
      "PyTorch inference took 0.00799107551574707 sec\n",
      "PyTorch postprocessing took 0.007384777069091797 sec\n",
      "Frame 16 processing time: 0.0529 seconds\n",
      "PyTorch inference took 0.00765228271484375 sec\n",
      "PyTorch postprocessing took 0.014682292938232422 sec\n",
      "Frame 17 processing time: 0.0503 seconds\n",
      "PyTorch inference took 0.004004240036010742 sec\n",
      "PyTorch postprocessing took 0.01004481315612793 sec\n",
      "Frame 18 processing time: 0.0328 seconds\n",
      "PyTorch inference took 0.0039255619049072266 sec\n",
      "PyTorch postprocessing took 0.009119749069213867 sec\n",
      "Frame 19 processing time: 0.0317 seconds\n",
      "PyTorch inference took 0.0038268566131591797 sec\n",
      "PyTorch postprocessing took 0.009530782699584961 sec\n",
      "Frame 20 processing time: 0.0315 seconds\n",
      "PyTorch inference took 0.006410360336303711 sec\n",
      "PyTorch postprocessing took 0.00751948356628418 sec\n",
      "Frame 21 processing time: 0.0342 seconds\n",
      "PyTorch inference took 0.0039386749267578125 sec\n",
      "PyTorch postprocessing took 0.009112358093261719 sec\n",
      "Frame 22 processing time: 0.0343 seconds\n",
      "PyTorch inference took 0.003158092498779297 sec\n",
      "PyTorch postprocessing took 0.009435176849365234 sec\n",
      "Frame 23 processing time: 0.0311 seconds\n",
      "PyTorch inference took 0.01165008544921875 sec\n",
      "PyTorch postprocessing took 0.02298426628112793 sec\n",
      "Frame 24 processing time: 0.0718 seconds\n",
      "PyTorch inference took 0.004618406295776367 sec\n",
      "PyTorch postprocessing took 0.010058403015136719 sec\n",
      "Frame 25 processing time: 0.0309 seconds\n",
      "PyTorch inference took 0.008069992065429688 sec\n",
      "PyTorch postprocessing took 0.010686635971069336 sec\n",
      "Frame 26 processing time: 0.0637 seconds\n",
      "PyTorch inference took 0.013264179229736328 sec\n",
      "PyTorch postprocessing took 0.013078927993774414 sec\n",
      "Frame 27 processing time: 0.0605 seconds\n",
      "PyTorch inference took 0.006331682205200195 sec\n",
      "PyTorch postprocessing took 0.010157346725463867 sec\n",
      "Frame 28 processing time: 0.0488 seconds\n",
      "PyTorch inference took 0.0039594173431396484 sec\n",
      "PyTorch postprocessing took 0.010072708129882812 sec\n",
      "Frame 29 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.003835439682006836 sec\n",
      "PyTorch postprocessing took 0.010040283203125 sec\n",
      "Frame 30 processing time: 0.0308 seconds\n",
      "PyTorch inference took 0.004022359848022461 sec\n",
      "PyTorch postprocessing took 0.009992122650146484 sec\n",
      "Frame 31 processing time: 0.0325 seconds\n",
      "PyTorch inference took 0.0038352012634277344 sec\n",
      "PyTorch postprocessing took 0.009194135665893555 sec\n",
      "Frame 32 processing time: 0.0267 seconds\n",
      "PyTorch inference took 0.003335714340209961 sec\n",
      "PyTorch postprocessing took 0.009877681732177734 sec\n",
      "Frame 33 processing time: 0.0334 seconds\n",
      "PyTorch inference took 0.004254579544067383 sec\n",
      "PyTorch postprocessing took 0.014145851135253906 sec\n",
      "Frame 34 processing time: 0.0394 seconds\n",
      "PyTorch inference took 0.0043032169342041016 sec\n",
      "PyTorch postprocessing took 0.015113353729248047 sec\n",
      "Frame 35 processing time: 0.0401 seconds\n",
      "PyTorch inference took 0.007308006286621094 sec\n",
      "PyTorch postprocessing took 0.013913393020629883 sec\n",
      "Frame 36 processing time: 0.0599 seconds\n",
      "PyTorch inference took 0.008470535278320312 sec\n",
      "PyTorch postprocessing took 0.006925344467163086 sec\n",
      "Frame 37 processing time: 0.0579 seconds\n",
      "PyTorch inference took 0.00442814826965332 sec\n",
      "PyTorch postprocessing took 0.009490251541137695 sec\n",
      "Frame 38 processing time: 0.0327 seconds\n",
      "PyTorch inference took 0.005624294281005859 sec\n",
      "PyTorch postprocessing took 0.013268232345581055 sec\n",
      "Frame 39 processing time: 0.0341 seconds\n",
      "PyTorch inference took 0.009521484375 sec\n",
      "PyTorch postprocessing took 0.009266853332519531 sec\n",
      "Frame 40 processing time: 0.0461 seconds\n",
      "PyTorch inference took 0.004079580307006836 sec\n",
      "PyTorch postprocessing took 0.009083986282348633 sec\n",
      "Frame 41 processing time: 0.0395 seconds\n",
      "PyTorch inference took 0.01546788215637207 sec\n",
      "PyTorch postprocessing took 0.011926651000976562 sec\n",
      "Frame 42 processing time: 0.0557 seconds\n",
      "PyTorch inference took 0.004664897918701172 sec\n",
      "PyTorch postprocessing took 0.009009122848510742 sec\n",
      "Frame 43 processing time: 0.0349 seconds\n",
      "PyTorch inference took 0.0032262802124023438 sec\n",
      "PyTorch postprocessing took 0.010508537292480469 sec\n",
      "Frame 44 processing time: 0.0320 seconds\n",
      "PyTorch inference took 0.0050694942474365234 sec\n",
      "PyTorch postprocessing took 0.00885772705078125 sec\n",
      "Frame 45 processing time: 0.0390 seconds\n",
      "PyTorch inference took 0.0042171478271484375 sec\n",
      "PyTorch postprocessing took 0.008325576782226562 sec\n",
      "Frame 46 processing time: 0.0322 seconds\n",
      "PyTorch inference took 0.003317117691040039 sec\n",
      "PyTorch postprocessing took 0.00958871841430664 sec\n",
      "Frame 47 processing time: 0.0350 seconds\n",
      "PyTorch inference took 0.00386810302734375 sec\n",
      "PyTorch postprocessing took 0.008756875991821289 sec\n",
      "Frame 48 processing time: 0.0280 seconds\n",
      "PyTorch inference took 0.003300189971923828 sec\n",
      "PyTorch postprocessing took 0.009760856628417969 sec\n",
      "Frame 49 processing time: 0.0265 seconds\n",
      "PyTorch inference took 0.0038466453552246094 sec\n",
      "PyTorch postprocessing took 0.009569406509399414 sec\n",
      "Frame 50 processing time: 0.0292 seconds\n",
      "PyTorch inference took 0.0036056041717529297 sec\n",
      "PyTorch postprocessing took 0.009394168853759766 sec\n",
      "Frame 51 processing time: 0.0337 seconds\n",
      "PyTorch inference took 0.011393547058105469 sec\n",
      "PyTorch postprocessing took 0.02131056785583496 sec\n",
      "Frame 52 processing time: 0.0599 seconds\n",
      "PyTorch inference took 0.005328178405761719 sec\n",
      "PyTorch postprocessing took 0.010815143585205078 sec\n",
      "Frame 53 processing time: 0.0418 seconds\n",
      "PyTorch inference took 0.0037834644317626953 sec\n",
      "PyTorch postprocessing took 0.009706974029541016 sec\n",
      "Frame 54 processing time: 0.0329 seconds\n",
      "PyTorch inference took 0.0031862258911132812 sec\n",
      "PyTorch postprocessing took 0.009637832641601562 sec\n",
      "Frame 55 processing time: 0.0251 seconds\n",
      "PyTorch inference took 0.0034503936767578125 sec\n",
      "PyTorch postprocessing took 0.010202169418334961 sec\n",
      "Frame 56 processing time: 0.0327 seconds\n",
      "PyTorch inference took 0.004294633865356445 sec\n",
      "PyTorch postprocessing took 0.009404182434082031 sec\n",
      "Frame 57 processing time: 0.0356 seconds\n",
      "PyTorch inference took 0.003979206085205078 sec\n",
      "PyTorch postprocessing took 0.015156745910644531 sec\n",
      "Frame 58 processing time: 0.0441 seconds\n",
      "PyTorch inference took 0.0029561519622802734 sec\n",
      "PyTorch postprocessing took 0.01000356674194336 sec\n",
      "Frame 59 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.004058122634887695 sec\n",
      "PyTorch postprocessing took 0.009947538375854492 sec\n",
      "Frame 60 processing time: 0.0332 seconds\n",
      "PyTorch inference took 0.0034933090209960938 sec\n",
      "PyTorch postprocessing took 0.011037588119506836 sec\n",
      "Frame 61 processing time: 0.0291 seconds\n",
      "PyTorch inference took 0.00406336784362793 sec\n",
      "PyTorch postprocessing took 0.009939908981323242 sec\n",
      "Frame 62 processing time: 0.0287 seconds\n",
      "PyTorch inference took 0.005021810531616211 sec\n",
      "PyTorch postprocessing took 0.00961160659790039 sec\n",
      "Frame 63 processing time: 0.0333 seconds\n",
      "PyTorch inference took 0.008782148361206055 sec\n",
      "PyTorch postprocessing took 0.02903890609741211 sec\n",
      "Frame 64 processing time: 0.0755 seconds\n",
      "PyTorch inference took 0.007349491119384766 sec\n",
      "PyTorch postprocessing took 0.020349740982055664 sec\n",
      "Frame 65 processing time: 0.0564 seconds\n",
      "PyTorch inference took 0.005197286605834961 sec\n",
      "PyTorch postprocessing took 0.009870529174804688 sec\n",
      "Frame 66 processing time: 0.0468 seconds\n",
      "PyTorch inference took 0.0032241344451904297 sec\n",
      "PyTorch postprocessing took 0.009858369827270508 sec\n",
      "Frame 67 processing time: 0.0338 seconds\n",
      "PyTorch inference took 0.008087158203125 sec\n",
      "PyTorch postprocessing took 0.012741327285766602 sec\n",
      "Frame 68 processing time: 0.0443 seconds\n",
      "PyTorch inference took 0.008232831954956055 sec\n",
      "PyTorch postprocessing took 0.009635210037231445 sec\n",
      "Frame 69 processing time: 0.0635 seconds\n",
      "PyTorch inference took 0.0031006336212158203 sec\n",
      "PyTorch postprocessing took 0.009915828704833984 sec\n",
      "Frame 70 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.003576040267944336 sec\n",
      "PyTorch postprocessing took 0.009254217147827148 sec\n",
      "Frame 71 processing time: 0.0348 seconds\n",
      "PyTorch inference took 0.005327463150024414 sec\n",
      "PyTorch postprocessing took 0.00974583625793457 sec\n",
      "Frame 72 processing time: 0.0402 seconds\n",
      "PyTorch inference took 0.003318309783935547 sec\n",
      "PyTorch postprocessing took 0.009565353393554688 sec\n",
      "Frame 73 processing time: 0.0256 seconds\n",
      "PyTorch inference took 0.003066539764404297 sec\n",
      "PyTorch postprocessing took 0.010790109634399414 sec\n",
      "Frame 74 processing time: 0.0326 seconds\n",
      "PyTorch inference took 0.0039196014404296875 sec\n",
      "PyTorch postprocessing took 0.009127616882324219 sec\n",
      "Frame 75 processing time: 0.0267 seconds\n",
      "PyTorch inference took 0.0029659271240234375 sec\n",
      "PyTorch postprocessing took 0.009956836700439453 sec\n",
      "Frame 76 processing time: 0.0330 seconds\n",
      "PyTorch inference took 0.003315448760986328 sec\n",
      "PyTorch postprocessing took 0.009772777557373047 sec\n",
      "Frame 77 processing time: 0.0307 seconds\n",
      "PyTorch inference took 0.007860898971557617 sec\n",
      "PyTorch postprocessing took 0.00948023796081543 sec\n",
      "Frame 78 processing time: 0.0459 seconds\n",
      "PyTorch inference took 0.006961345672607422 sec\n",
      "PyTorch postprocessing took 0.006639003753662109 sec\n",
      "Frame 79 processing time: 0.0449 seconds\n",
      "PyTorch inference took 0.008425712585449219 sec\n",
      "PyTorch postprocessing took 0.012265920639038086 sec\n",
      "Frame 80 processing time: 0.0683 seconds\n",
      "PyTorch inference took 0.0043871402740478516 sec\n",
      "PyTorch postprocessing took 0.016115427017211914 sec\n",
      "Frame 81 processing time: 0.0366 seconds\n",
      "PyTorch inference took 0.009267568588256836 sec\n",
      "PyTorch postprocessing took 0.009948968887329102 sec\n",
      "Frame 82 processing time: 0.0506 seconds\n",
      "PyTorch inference took 0.0040209293365478516 sec\n",
      "PyTorch postprocessing took 0.010230064392089844 sec\n",
      "Frame 83 processing time: 0.0364 seconds\n",
      "PyTorch inference took 0.0038557052612304688 sec\n",
      "PyTorch postprocessing took 0.015340566635131836 sec\n",
      "Frame 84 processing time: 0.0348 seconds\n",
      "PyTorch inference took 0.010696887969970703 sec\n",
      "PyTorch postprocessing took 0.01194143295288086 sec\n",
      "Frame 85 processing time: 0.0854 seconds\n",
      "PyTorch inference took 0.003806591033935547 sec\n",
      "PyTorch postprocessing took 0.015232086181640625 sec\n",
      "Frame 86 processing time: 0.0351 seconds\n",
      "PyTorch inference took 0.005121946334838867 sec\n",
      "PyTorch postprocessing took 0.009651660919189453 sec\n",
      "Frame 87 processing time: 0.0353 seconds\n",
      "PyTorch inference took 0.008324146270751953 sec\n",
      "PyTorch postprocessing took 0.011316061019897461 sec\n",
      "Frame 88 processing time: 0.0638 seconds\n",
      "PyTorch inference took 0.004680156707763672 sec\n",
      "PyTorch postprocessing took 0.01176595687866211 sec\n",
      "Frame 89 processing time: 0.0388 seconds\n",
      "PyTorch inference took 0.0038814544677734375 sec\n",
      "PyTorch postprocessing took 0.015339851379394531 sec\n",
      "Frame 90 processing time: 0.0368 seconds\n",
      "PyTorch inference took 0.006341457366943359 sec\n",
      "PyTorch postprocessing took 0.012839555740356445 sec\n",
      "Frame 91 processing time: 0.0386 seconds\n",
      "PyTorch inference took 0.003084897994995117 sec\n",
      "PyTorch postprocessing took 0.009725809097290039 sec\n",
      "Frame 92 processing time: 0.0364 seconds\n",
      "PyTorch inference took 0.006969451904296875 sec\n",
      "PyTorch postprocessing took 0.00935816764831543 sec\n",
      "Frame 93 processing time: 0.0299 seconds\n",
      "PyTorch inference took 0.012393474578857422 sec\n",
      "PyTorch postprocessing took 0.024317264556884766 sec\n",
      "Frame 94 processing time: 0.0536 seconds\n",
      "PyTorch inference took 0.011138200759887695 sec\n",
      "PyTorch postprocessing took 0.01284170150756836 sec\n",
      "Frame 95 processing time: 0.0639 seconds\n",
      "PyTorch inference took 0.0038661956787109375 sec\n",
      "PyTorch postprocessing took 0.011837482452392578 sec\n",
      "Frame 96 processing time: 0.0346 seconds\n",
      "PyTorch inference took 0.0065402984619140625 sec\n",
      "PyTorch postprocessing took 0.008091449737548828 sec\n",
      "Frame 97 processing time: 0.0484 seconds\n",
      "PyTorch inference took 0.00382232666015625 sec\n",
      "PyTorch postprocessing took 0.011196613311767578 sec\n",
      "Frame 98 processing time: 0.0331 seconds\n",
      "PyTorch inference took 0.003911733627319336 sec\n",
      "PyTorch postprocessing took 0.009119987487792969 sec\n",
      "Frame 99 processing time: 0.0278 seconds\n",
      "PyTorch inference took 0.002969980239868164 sec\n",
      "PyTorch postprocessing took 0.010503530502319336 sec\n",
      "Frame 100 processing time: 0.0321 seconds\n",
      "PyTorch inference took 0.0030171871185302734 sec\n",
      "PyTorch postprocessing took 0.010063409805297852 sec\n",
      "Frame 101 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.0029439926147460938 sec\n",
      "PyTorch postprocessing took 0.010166645050048828 sec\n",
      "Frame 102 processing time: 0.0334 seconds\n",
      "PyTorch inference took 0.0030126571655273438 sec\n",
      "PyTorch postprocessing took 0.010281562805175781 sec\n",
      "Frame 103 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.0030639171600341797 sec\n",
      "PyTorch postprocessing took 0.010046243667602539 sec\n",
      "Frame 104 processing time: 0.0297 seconds\n",
      "PyTorch inference took 0.0031747817993164062 sec\n",
      "PyTorch postprocessing took 0.00995635986328125 sec\n",
      "Frame 105 processing time: 0.0361 seconds\n",
      "PyTorch inference took 0.003184795379638672 sec\n",
      "PyTorch postprocessing took 0.009317159652709961 sec\n",
      "Frame 106 processing time: 0.0285 seconds\n",
      "PyTorch inference took 0.00458073616027832 sec\n",
      "PyTorch postprocessing took 0.010801315307617188 sec\n",
      "Frame 107 processing time: 0.0364 seconds\n",
      "PyTorch inference took 0.023385047912597656 sec\n",
      "PyTorch postprocessing took 0.03478074073791504 sec\n",
      "Frame 108 processing time: 0.0884 seconds\n",
      "PyTorch inference took 0.006179332733154297 sec\n",
      "PyTorch postprocessing took 0.006800174713134766 sec\n",
      "Frame 109 processing time: 0.0297 seconds\n",
      "PyTorch inference took 0.005622386932373047 sec\n",
      "PyTorch postprocessing took 0.008515119552612305 sec\n",
      "Frame 110 processing time: 0.0320 seconds\n",
      "PyTorch inference took 0.0031490325927734375 sec\n",
      "PyTorch postprocessing took 0.009822368621826172 sec\n",
      "Frame 111 processing time: 0.0377 seconds\n",
      "PyTorch inference took 0.006075143814086914 sec\n",
      "PyTorch postprocessing took 0.009680747985839844 sec\n",
      "Frame 112 processing time: 0.0494 seconds\n",
      "PyTorch inference took 0.0047185420989990234 sec\n",
      "PyTorch postprocessing took 0.010220050811767578 sec\n",
      "Frame 113 processing time: 0.0349 seconds\n",
      "PyTorch inference took 0.0064868927001953125 sec\n",
      "PyTorch postprocessing took 0.007872819900512695 sec\n",
      "Frame 114 processing time: 0.0482 seconds\n",
      "PyTorch inference took 0.0034406185150146484 sec\n",
      "PyTorch postprocessing took 0.009548664093017578 sec\n",
      "Frame 115 processing time: 0.0277 seconds\n",
      "PyTorch inference took 0.003011941909790039 sec\n",
      "PyTorch postprocessing took 0.010076522827148438 sec\n",
      "Frame 116 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.006928920745849609 sec\n",
      "PyTorch postprocessing took 0.012725830078125 sec\n",
      "Frame 117 processing time: 0.0471 seconds\n",
      "PyTorch inference took 0.005229473114013672 sec\n",
      "PyTorch postprocessing took 0.013265848159790039 sec\n",
      "Frame 118 processing time: 0.0389 seconds\n",
      "PyTorch inference took 0.003975391387939453 sec\n",
      "PyTorch postprocessing took 0.010057210922241211 sec\n",
      "Frame 119 processing time: 0.0314 seconds\n",
      "PyTorch inference took 0.002963542938232422 sec\n",
      "PyTorch postprocessing took 0.009526491165161133 sec\n",
      "Frame 120 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.0032227039337158203 sec\n",
      "PyTorch postprocessing took 0.010106801986694336 sec\n",
      "Frame 121 processing time: 0.0315 seconds\n",
      "PyTorch inference took 0.0032417774200439453 sec\n",
      "PyTorch postprocessing took 0.009983539581298828 sec\n",
      "Frame 122 processing time: 0.0316 seconds\n",
      "PyTorch inference took 0.007377147674560547 sec\n",
      "PyTorch postprocessing took 0.009064912796020508 sec\n",
      "Frame 123 processing time: 0.0434 seconds\n",
      "PyTorch inference took 0.0031175613403320312 sec\n",
      "PyTorch postprocessing took 0.011510133743286133 sec\n",
      "Frame 124 processing time: 0.0341 seconds\n",
      "PyTorch inference took 0.0029413700103759766 sec\n",
      "PyTorch postprocessing took 0.01004791259765625 sec\n",
      "Frame 125 processing time: 0.0347 seconds\n",
      "PyTorch inference took 0.003072023391723633 sec\n",
      "PyTorch postprocessing took 0.009943008422851562 sec\n",
      "Frame 126 processing time: 0.0306 seconds\n",
      "PyTorch inference took 0.00600743293762207 sec\n",
      "PyTorch postprocessing took 0.011661052703857422 sec\n",
      "Frame 127 processing time: 0.0477 seconds\n",
      "PyTorch inference took 0.004078865051269531 sec\n",
      "PyTorch postprocessing took 0.014751672744750977 sec\n",
      "Frame 128 processing time: 0.0416 seconds\n",
      "PyTorch inference took 0.005096912384033203 sec\n",
      "PyTorch postprocessing took 0.009507179260253906 sec\n",
      "Frame 129 processing time: 0.0498 seconds\n",
      "PyTorch inference took 0.0042688846588134766 sec\n",
      "PyTorch postprocessing took 0.009435415267944336 sec\n",
      "Frame 130 processing time: 0.0375 seconds\n",
      "PyTorch inference took 0.004527091979980469 sec\n",
      "PyTorch postprocessing took 0.009626626968383789 sec\n",
      "Frame 131 processing time: 0.0426 seconds\n",
      "PyTorch inference took 0.00462794303894043 sec\n",
      "PyTorch postprocessing took 0.009439706802368164 sec\n",
      "Frame 132 processing time: 0.0365 seconds\n",
      "PyTorch inference took 0.004441738128662109 sec\n",
      "PyTorch postprocessing took 0.012941122055053711 sec\n",
      "Frame 133 processing time: 0.0433 seconds\n",
      "PyTorch inference took 0.004030704498291016 sec\n",
      "PyTorch postprocessing took 0.01425790786743164 sec\n",
      "Frame 134 processing time: 0.0332 seconds\n",
      "PyTorch inference took 0.003779888153076172 sec\n",
      "PyTorch postprocessing took 0.00930333137512207 sec\n",
      "Frame 135 processing time: 0.0277 seconds\n",
      "PyTorch inference took 0.0030050277709960938 sec\n",
      "PyTorch postprocessing took 0.009486198425292969 sec\n",
      "Frame 136 processing time: 0.0248 seconds\n",
      "PyTorch inference took 0.004021406173706055 sec\n",
      "PyTorch postprocessing took 0.013209342956542969 sec\n",
      "Frame 137 processing time: 0.0358 seconds\n",
      "PyTorch inference took 0.0039064884185791016 sec\n",
      "PyTorch postprocessing took 0.010063886642456055 sec\n",
      "Frame 138 processing time: 0.0370 seconds\n",
      "PyTorch inference took 0.004906177520751953 sec\n",
      "PyTorch postprocessing took 0.011973142623901367 sec\n",
      "Frame 139 processing time: 0.0429 seconds\n",
      "PyTorch inference took 0.004180431365966797 sec\n",
      "PyTorch postprocessing took 0.009910106658935547 sec\n",
      "Frame 140 processing time: 0.0393 seconds\n",
      "PyTorch inference took 0.005235195159912109 sec\n",
      "PyTorch postprocessing took 0.01689314842224121 sec\n",
      "Frame 141 processing time: 0.0403 seconds\n",
      "PyTorch inference took 0.004716157913208008 sec\n",
      "PyTorch postprocessing took 0.008178234100341797 sec\n",
      "Frame 142 processing time: 0.0345 seconds\n",
      "PyTorch inference took 0.009202241897583008 sec\n",
      "PyTorch postprocessing took 0.009287118911743164 sec\n",
      "Frame 143 processing time: 0.0649 seconds\n",
      "PyTorch inference took 0.0075762271881103516 sec\n",
      "PyTorch postprocessing took 0.011176109313964844 sec\n",
      "Frame 144 processing time: 0.0611 seconds\n",
      "PyTorch inference took 0.0070874691009521484 sec\n",
      "PyTorch postprocessing took 0.009448528289794922 sec\n",
      "Frame 145 processing time: 0.0410 seconds\n",
      "PyTorch inference took 0.003867626190185547 sec\n",
      "PyTorch postprocessing took 0.009193658828735352 sec\n",
      "Frame 146 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.00727391242980957 sec\n",
      "PyTorch postprocessing took 0.009633302688598633 sec\n",
      "Frame 147 processing time: 0.0449 seconds\n",
      "PyTorch inference took 0.0029489994049072266 sec\n",
      "PyTorch postprocessing took 0.00997614860534668 sec\n",
      "Frame 148 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.006675004959106445 sec\n",
      "PyTorch postprocessing took 0.016425609588623047 sec\n",
      "Frame 149 processing time: 0.0405 seconds\n",
      "PyTorch inference took 0.0048830509185791016 sec\n",
      "PyTorch postprocessing took 0.009083271026611328 sec\n",
      "Frame 150 processing time: 0.0343 seconds\n",
      "PyTorch inference took 0.003701448440551758 sec\n",
      "PyTorch postprocessing took 0.01006937026977539 sec\n",
      "Frame 151 processing time: 0.0316 seconds\n",
      "PyTorch inference took 0.0037088394165039062 sec\n",
      "PyTorch postprocessing took 0.009730339050292969 sec\n",
      "Frame 152 processing time: 0.0261 seconds\n",
      "PyTorch inference took 0.005105018615722656 sec\n",
      "PyTorch postprocessing took 0.008501529693603516 sec\n",
      "Frame 153 processing time: 0.0345 seconds\n",
      "PyTorch inference took 0.005304813385009766 sec\n",
      "PyTorch postprocessing took 0.013868570327758789 sec\n",
      "Frame 154 processing time: 0.0365 seconds\n",
      "PyTorch inference took 0.004143476486206055 sec\n",
      "PyTorch postprocessing took 0.009358406066894531 sec\n",
      "Frame 155 processing time: 0.0276 seconds\n",
      "PyTorch inference took 0.00803518295288086 sec\n",
      "PyTorch postprocessing took 0.008536100387573242 sec\n",
      "Frame 156 processing time: 0.0520 seconds\n",
      "PyTorch inference took 0.010579586029052734 sec\n",
      "PyTorch postprocessing took 0.010780572891235352 sec\n",
      "Frame 157 processing time: 0.0639 seconds\n",
      "PyTorch inference took 0.0033817291259765625 sec\n",
      "PyTorch postprocessing took 0.010421514511108398 sec\n",
      "Frame 158 processing time: 0.0288 seconds\n",
      "PyTorch inference took 0.004966259002685547 sec\n",
      "PyTorch postprocessing took 0.009129524230957031 sec\n",
      "Frame 159 processing time: 0.0394 seconds\n",
      "PyTorch inference took 0.003937244415283203 sec\n",
      "PyTorch postprocessing took 0.014147281646728516 sec\n",
      "Frame 160 processing time: 0.0459 seconds\n",
      "PyTorch inference took 0.003975868225097656 sec\n",
      "PyTorch postprocessing took 0.014578580856323242 sec\n",
      "Frame 161 processing time: 0.0407 seconds\n",
      "PyTorch inference took 0.0068683624267578125 sec\n",
      "PyTorch postprocessing took 0.014945268630981445 sec\n",
      "Frame 162 processing time: 0.0418 seconds\n",
      "PyTorch inference took 0.003698587417602539 sec\n",
      "PyTorch postprocessing took 0.010189294815063477 sec\n",
      "Frame 163 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.003917694091796875 sec\n",
      "PyTorch postprocessing took 0.009987831115722656 sec\n",
      "Frame 164 processing time: 0.0335 seconds\n",
      "PyTorch inference took 0.007665157318115234 sec\n",
      "PyTorch postprocessing took 0.006935834884643555 sec\n",
      "Frame 165 processing time: 0.0465 seconds\n",
      "PyTorch inference took 0.0065996646881103516 sec\n",
      "PyTorch postprocessing took 0.0069043636322021484 sec\n",
      "Frame 166 processing time: 0.0317 seconds\n",
      "PyTorch inference took 0.004366397857666016 sec\n",
      "PyTorch postprocessing took 0.00984501838684082 sec\n",
      "Frame 167 processing time: 0.0346 seconds\n",
      "PyTorch inference took 0.00651860237121582 sec\n",
      "PyTorch postprocessing took 0.006615161895751953 sec\n",
      "Frame 168 processing time: 0.0424 seconds\n",
      "PyTorch inference took 0.005506992340087891 sec\n",
      "PyTorch postprocessing took 0.008080005645751953 sec\n",
      "Frame 169 processing time: 0.0358 seconds\n",
      "PyTorch inference took 0.004621744155883789 sec\n",
      "PyTorch postprocessing took 0.010247945785522461 sec\n",
      "Frame 170 processing time: 0.0373 seconds\n",
      "PyTorch inference took 0.0037841796875 sec\n",
      "PyTorch postprocessing took 0.010116338729858398 sec\n",
      "Frame 171 processing time: 0.0283 seconds\n",
      "PyTorch inference took 0.004665374755859375 sec\n",
      "PyTorch postprocessing took 0.009284734725952148 sec\n",
      "Frame 172 processing time: 0.0301 seconds\n",
      "PyTorch inference took 0.004099130630493164 sec\n",
      "PyTorch postprocessing took 0.015059947967529297 sec\n",
      "Frame 173 processing time: 0.0416 seconds\n",
      "PyTorch inference took 0.006331920623779297 sec\n",
      "PyTorch postprocessing took 0.021019935607910156 sec\n",
      "Frame 174 processing time: 0.0496 seconds\n",
      "PyTorch inference took 0.003494739532470703 sec\n",
      "PyTorch postprocessing took 0.009066343307495117 sec\n",
      "Frame 175 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.003014802932739258 sec\n",
      "PyTorch postprocessing took 0.010039567947387695 sec\n",
      "Frame 176 processing time: 0.0266 seconds\n",
      "PyTorch inference took 0.0032188892364501953 sec\n",
      "PyTorch postprocessing took 0.009652853012084961 sec\n",
      "Frame 177 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.0031769275665283203 sec\n",
      "PyTorch postprocessing took 0.009722471237182617 sec\n",
      "Frame 178 processing time: 0.0256 seconds\n",
      "PyTorch inference took 0.004454612731933594 sec\n",
      "PyTorch postprocessing took 0.009288549423217773 sec\n",
      "Frame 179 processing time: 0.0355 seconds\n",
      "PyTorch inference took 0.008690357208251953 sec\n",
      "PyTorch postprocessing took 0.009567975997924805 sec\n",
      "Frame 180 processing time: 0.0345 seconds\n",
      "PyTorch inference took 0.004068613052368164 sec\n",
      "PyTorch postprocessing took 0.00987553596496582 sec\n",
      "Frame 181 processing time: 0.0294 seconds\n",
      "PyTorch inference took 0.0077550411224365234 sec\n",
      "PyTorch postprocessing took 0.010033369064331055 sec\n",
      "Frame 182 processing time: 0.0429 seconds\n",
      "PyTorch inference took 0.003721475601196289 sec\n",
      "PyTorch postprocessing took 0.010042667388916016 sec\n",
      "Frame 183 processing time: 0.0280 seconds\n",
      "PyTorch inference took 0.00391840934753418 sec\n",
      "PyTorch postprocessing took 0.014677762985229492 sec\n",
      "Frame 184 processing time: 0.0370 seconds\n",
      "PyTorch inference took 0.004834413528442383 sec\n",
      "PyTorch postprocessing took 0.009714603424072266 sec\n",
      "Frame 185 processing time: 0.0373 seconds\n",
      "PyTorch inference took 0.005682229995727539 sec\n",
      "PyTorch postprocessing took 0.011127948760986328 sec\n",
      "Frame 186 processing time: 0.0323 seconds\n",
      "PyTorch inference took 0.0055086612701416016 sec\n",
      "PyTorch postprocessing took 0.009506702423095703 sec\n",
      "Frame 187 processing time: 0.0496 seconds\n",
      "PyTorch inference took 0.003841876983642578 sec\n",
      "PyTorch postprocessing took 0.014610052108764648 sec\n",
      "Frame 188 processing time: 0.0347 seconds\n",
      "PyTorch inference took 0.0045702457427978516 sec\n",
      "PyTorch postprocessing took 0.008731603622436523 sec\n",
      "Frame 189 processing time: 0.0288 seconds\n",
      "PyTorch inference took 0.007886171340942383 sec\n",
      "PyTorch postprocessing took 0.00549769401550293 sec\n",
      "Frame 190 processing time: 0.0520 seconds\n",
      "PyTorch inference took 0.0037970542907714844 sec\n",
      "PyTorch postprocessing took 0.012585639953613281 sec\n",
      "Frame 191 processing time: 0.0346 seconds\n",
      "PyTorch inference took 0.007322788238525391 sec\n",
      "PyTorch postprocessing took 0.017354965209960938 sec\n",
      "Frame 192 processing time: 0.0502 seconds\n",
      "PyTorch inference took 0.00404047966003418 sec\n",
      "PyTorch postprocessing took 0.010894298553466797 sec\n",
      "Frame 193 processing time: 0.0329 seconds\n",
      "PyTorch inference took 0.0034215450286865234 sec\n",
      "PyTorch postprocessing took 0.00948643684387207 sec\n",
      "Frame 194 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.003862142562866211 sec\n",
      "PyTorch postprocessing took 0.014405965805053711 sec\n",
      "Frame 195 processing time: 0.0363 seconds\n",
      "PyTorch inference took 0.0037124156951904297 sec\n",
      "PyTorch postprocessing took 0.01400136947631836 sec\n",
      "Frame 196 processing time: 0.0318 seconds\n",
      "PyTorch inference took 0.003099203109741211 sec\n",
      "PyTorch postprocessing took 0.00954747200012207 sec\n",
      "Frame 197 processing time: 0.0316 seconds\n",
      "PyTorch inference took 0.004868268966674805 sec\n",
      "PyTorch postprocessing took 0.009057044982910156 sec\n",
      "Frame 198 processing time: 0.0308 seconds\n",
      "PyTorch inference took 0.005334138870239258 sec\n",
      "PyTorch postprocessing took 0.008983373641967773 sec\n",
      "Frame 199 processing time: 0.0415 seconds\n",
      "PyTorch inference took 0.0043964385986328125 sec\n",
      "PyTorch postprocessing took 0.012526512145996094 sec\n",
      "Frame 200 processing time: 0.0486 seconds\n",
      "PyTorch inference took 0.005137205123901367 sec\n",
      "PyTorch postprocessing took 0.009534597396850586 sec\n",
      "Frame 201 processing time: 0.0396 seconds\n",
      "PyTorch inference took 0.004133939743041992 sec\n",
      "PyTorch postprocessing took 0.009095907211303711 sec\n",
      "Frame 202 processing time: 0.0369 seconds\n",
      "PyTorch inference took 0.007442951202392578 sec\n",
      "PyTorch postprocessing took 0.01227259635925293 sec\n",
      "Frame 203 processing time: 0.0445 seconds\n",
      "PyTorch inference took 0.00299072265625 sec\n",
      "PyTorch postprocessing took 0.010335922241210938 sec\n",
      "Frame 204 processing time: 0.0324 seconds\n",
      "PyTorch inference took 0.013431072235107422 sec\n",
      "PyTorch postprocessing took 0.01227426528930664 sec\n",
      "Frame 205 processing time: 0.0750 seconds\n",
      "PyTorch inference took 0.0063817501068115234 sec\n",
      "PyTorch postprocessing took 0.01170659065246582 sec\n",
      "Frame 206 processing time: 0.0482 seconds\n",
      "PyTorch inference took 0.0031538009643554688 sec\n",
      "PyTorch postprocessing took 0.010103464126586914 sec\n",
      "Frame 207 processing time: 0.0379 seconds\n",
      "PyTorch inference took 0.003481149673461914 sec\n",
      "PyTorch postprocessing took 0.010129690170288086 sec\n",
      "Frame 208 processing time: 0.0282 seconds\n",
      "PyTorch inference took 0.006445646286010742 sec\n",
      "PyTorch postprocessing took 0.011239051818847656 sec\n",
      "Frame 209 processing time: 0.0395 seconds\n",
      "PyTorch inference took 0.004186868667602539 sec\n",
      "PyTorch postprocessing took 0.009353399276733398 sec\n",
      "Frame 210 processing time: 0.0296 seconds\n",
      "PyTorch inference took 0.0029528141021728516 sec\n",
      "PyTorch postprocessing took 0.010074615478515625 sec\n",
      "Frame 211 processing time: 0.0326 seconds\n",
      "PyTorch inference took 0.0030236244201660156 sec\n",
      "PyTorch postprocessing took 0.010022878646850586 sec\n",
      "Frame 212 processing time: 0.0382 seconds\n",
      "PyTorch inference took 0.004151105880737305 sec\n",
      "PyTorch postprocessing took 0.010648488998413086 sec\n",
      "Frame 213 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.006319284439086914 sec\n",
      "PyTorch postprocessing took 0.02290487289428711 sec\n",
      "Frame 214 processing time: 0.0566 seconds\n",
      "PyTorch inference took 0.0055239200592041016 sec\n",
      "PyTorch postprocessing took 0.016377925872802734 sec\n",
      "Frame 215 processing time: 0.0518 seconds\n",
      "PyTorch inference took 0.0055408477783203125 sec\n",
      "PyTorch postprocessing took 0.016077518463134766 sec\n",
      "Frame 216 processing time: 0.0521 seconds\n",
      "PyTorch inference took 0.00421142578125 sec\n",
      "PyTorch postprocessing took 0.011094808578491211 sec\n",
      "Frame 217 processing time: 0.0389 seconds\n",
      "PyTorch inference took 0.0039539337158203125 sec\n",
      "PyTorch postprocessing took 0.012209177017211914 sec\n",
      "Frame 218 processing time: 0.0390 seconds\n",
      "PyTorch inference took 0.008171796798706055 sec\n",
      "PyTorch postprocessing took 0.011876583099365234 sec\n",
      "Frame 219 processing time: 0.0762 seconds\n",
      "PyTorch inference took 0.011832952499389648 sec\n",
      "PyTorch postprocessing took 0.011598587036132812 sec\n",
      "Frame 220 processing time: 0.0718 seconds\n",
      "PyTorch inference took 0.023949384689331055 sec\n",
      "PyTorch postprocessing took 0.009936332702636719 sec\n",
      "Frame 221 processing time: 0.0725 seconds\n",
      "PyTorch inference took 0.004081249237060547 sec\n",
      "PyTorch postprocessing took 0.009416818618774414 sec\n",
      "Frame 222 processing time: 0.0296 seconds\n",
      "PyTorch inference took 0.008749723434448242 sec\n",
      "PyTorch postprocessing took 0.008564233779907227 sec\n",
      "Frame 223 processing time: 0.0647 seconds\n",
      "PyTorch inference took 0.004327058792114258 sec\n",
      "PyTorch postprocessing took 0.01480412483215332 sec\n",
      "Frame 224 processing time: 0.0436 seconds\n",
      "PyTorch inference took 0.004720211029052734 sec\n",
      "PyTorch postprocessing took 0.01580190658569336 sec\n",
      "Frame 225 processing time: 0.0358 seconds\n",
      "PyTorch inference took 0.004358053207397461 sec\n",
      "PyTorch postprocessing took 0.016025543212890625 sec\n",
      "Frame 226 processing time: 0.0481 seconds\n",
      "PyTorch inference took 0.005419731140136719 sec\n",
      "PyTorch postprocessing took 0.019330739974975586 sec\n",
      "Frame 227 processing time: 0.0510 seconds\n",
      "PyTorch inference took 0.011062145233154297 sec\n",
      "PyTorch postprocessing took 0.021909475326538086 sec\n",
      "Frame 228 processing time: 0.0817 seconds\n",
      "PyTorch inference took 0.003598928451538086 sec\n",
      "PyTorch postprocessing took 0.009569168090820312 sec\n",
      "Frame 229 processing time: 0.0370 seconds\n",
      "PyTorch inference took 0.004094839096069336 sec\n",
      "PyTorch postprocessing took 0.010187625885009766 sec\n",
      "Frame 230 processing time: 0.0325 seconds\n",
      "PyTorch inference took 0.009613513946533203 sec\n",
      "PyTorch postprocessing took 0.01605224609375 sec\n",
      "Frame 231 processing time: 0.0585 seconds\n",
      "PyTorch inference took 0.017522573471069336 sec\n",
      "PyTorch postprocessing took 0.012330055236816406 sec\n",
      "Frame 232 processing time: 0.0599 seconds\n",
      "PyTorch inference took 0.004147052764892578 sec\n",
      "PyTorch postprocessing took 0.011803150177001953 sec\n",
      "Frame 233 processing time: 0.0312 seconds\n",
      "PyTorch inference took 0.00799107551574707 sec\n",
      "PyTorch postprocessing took 0.010358810424804688 sec\n",
      "Frame 234 processing time: 0.0620 seconds\n",
      "PyTorch inference took 0.0041620731353759766 sec\n",
      "PyTorch postprocessing took 0.008562326431274414 sec\n",
      "Frame 235 processing time: 0.0299 seconds\n",
      "PyTorch inference took 0.0038852691650390625 sec\n",
      "PyTorch postprocessing took 0.009442329406738281 sec\n",
      "Frame 236 processing time: 0.0340 seconds\n",
      "PyTorch inference took 0.0033147335052490234 sec\n",
      "PyTorch postprocessing took 0.01012420654296875 sec\n",
      "Frame 237 processing time: 0.0362 seconds\n",
      "PyTorch inference took 0.005083799362182617 sec\n",
      "PyTorch postprocessing took 0.008969545364379883 sec\n",
      "Frame 238 processing time: 0.0341 seconds\n",
      "PyTorch inference took 0.0040814876556396484 sec\n",
      "PyTorch postprocessing took 0.009308576583862305 sec\n",
      "Frame 239 processing time: 0.0407 seconds\n",
      "PyTorch inference took 0.009612560272216797 sec\n",
      "PyTorch postprocessing took 0.009436607360839844 sec\n",
      "Frame 240 processing time: 0.0397 seconds\n",
      "PyTorch inference took 0.006218671798706055 sec\n",
      "PyTorch postprocessing took 0.006913423538208008 sec\n",
      "Frame 241 processing time: 0.0347 seconds\n",
      "PyTorch inference took 0.0053861141204833984 sec\n",
      "PyTorch postprocessing took 0.008695840835571289 sec\n",
      "Frame 242 processing time: 0.0377 seconds\n",
      "PyTorch inference took 0.004308938980102539 sec\n",
      "PyTorch postprocessing took 0.009630441665649414 sec\n",
      "Frame 243 processing time: 0.0318 seconds\n",
      "PyTorch inference took 0.0071680545806884766 sec\n",
      "PyTorch postprocessing took 0.011144161224365234 sec\n",
      "Frame 244 processing time: 0.0436 seconds\n",
      "PyTorch inference took 0.0042362213134765625 sec\n",
      "PyTorch postprocessing took 0.008448362350463867 sec\n",
      "Frame 245 processing time: 0.0279 seconds\n",
      "PyTorch inference took 0.0029823780059814453 sec\n",
      "PyTorch postprocessing took 0.01012110710144043 sec\n",
      "Frame 246 processing time: 0.0275 seconds\n",
      "PyTorch inference took 0.0033698081970214844 sec\n",
      "PyTorch postprocessing took 0.010468482971191406 sec\n",
      "Frame 247 processing time: 0.0284 seconds\n",
      "PyTorch inference took 0.003076314926147461 sec\n",
      "PyTorch postprocessing took 0.009941577911376953 sec\n",
      "Frame 248 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.003082275390625 sec\n",
      "PyTorch postprocessing took 0.009775876998901367 sec\n",
      "Frame 249 processing time: 0.0292 seconds\n",
      "PyTorch inference took 0.0065343379974365234 sec\n",
      "PyTorch postprocessing took 0.0064356327056884766 sec\n",
      "Frame 250 processing time: 0.0388 seconds\n",
      "PyTorch inference took 0.0058133602142333984 sec\n",
      "PyTorch postprocessing took 0.012625694274902344 sec\n",
      "Frame 251 processing time: 0.0552 seconds\n",
      "PyTorch inference took 0.0038857460021972656 sec\n",
      "PyTorch postprocessing took 0.01186227798461914 sec\n",
      "Frame 252 processing time: 0.0410 seconds\n",
      "PyTorch inference took 0.003287076950073242 sec\n",
      "PyTorch postprocessing took 0.01000213623046875 sec\n",
      "Frame 253 processing time: 0.0304 seconds\n",
      "PyTorch inference took 0.002909421920776367 sec\n",
      "PyTorch postprocessing took 0.01019287109375 sec\n",
      "Frame 254 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.006353855133056641 sec\n",
      "PyTorch postprocessing took 0.009404420852661133 sec\n",
      "Frame 255 processing time: 0.0437 seconds\n",
      "PyTorch inference took 0.005451202392578125 sec\n",
      "PyTorch postprocessing took 0.009052038192749023 sec\n",
      "Frame 256 processing time: 0.0307 seconds\n",
      "PyTorch inference took 0.012561798095703125 sec\n",
      "PyTorch postprocessing took 0.025731325149536133 sec\n",
      "Frame 257 processing time: 0.0756 seconds\n",
      "PyTorch inference took 0.003734588623046875 sec\n",
      "PyTorch postprocessing took 0.014997720718383789 sec\n",
      "Frame 258 processing time: 0.0405 seconds\n",
      "PyTorch inference took 0.004042387008666992 sec\n",
      "PyTorch postprocessing took 0.013393878936767578 sec\n",
      "Frame 259 processing time: 0.0421 seconds\n",
      "PyTorch inference took 0.0031206607818603516 sec\n",
      "PyTorch postprocessing took 0.011158466339111328 sec\n",
      "Frame 260 processing time: 0.0349 seconds\n",
      "PyTorch inference took 0.003039836883544922 sec\n",
      "PyTorch postprocessing took 0.009776115417480469 sec\n",
      "Frame 261 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.006713151931762695 sec\n",
      "PyTorch postprocessing took 0.009787559509277344 sec\n",
      "Frame 262 processing time: 0.0357 seconds\n",
      "PyTorch inference took 0.003148317337036133 sec\n",
      "PyTorch postprocessing took 0.009706497192382812 sec\n",
      "Frame 263 processing time: 0.0311 seconds\n",
      "PyTorch inference took 0.005390167236328125 sec\n",
      "PyTorch postprocessing took 0.008915185928344727 sec\n",
      "Frame 264 processing time: 0.0411 seconds\n",
      "PyTorch inference took 0.011080026626586914 sec\n",
      "PyTorch postprocessing took 0.018182992935180664 sec\n",
      "Frame 265 processing time: 0.0776 seconds\n",
      "PyTorch inference took 0.002884387969970703 sec\n",
      "PyTorch postprocessing took 0.010174036026000977 sec\n",
      "Frame 266 processing time: 0.0335 seconds\n",
      "PyTorch inference took 0.0031151771545410156 sec\n",
      "PyTorch postprocessing took 0.010698795318603516 sec\n",
      "Frame 267 processing time: 0.0310 seconds\n",
      "PyTorch inference took 0.003070831298828125 sec\n",
      "PyTorch postprocessing took 0.009952783584594727 sec\n",
      "Frame 268 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.005233287811279297 sec\n",
      "PyTorch postprocessing took 0.01633906364440918 sec\n",
      "Frame 269 processing time: 0.0444 seconds\n",
      "PyTorch inference took 0.0043909549713134766 sec\n",
      "PyTorch postprocessing took 0.008538484573364258 sec\n",
      "Frame 270 processing time: 0.0316 seconds\n",
      "PyTorch inference took 0.0033736228942871094 sec\n",
      "PyTorch postprocessing took 0.009982824325561523 sec\n",
      "Frame 271 processing time: 0.0302 seconds\n",
      "PyTorch inference took 0.006623506546020508 sec\n",
      "PyTorch postprocessing took 0.01298379898071289 sec\n",
      "Frame 272 processing time: 0.0407 seconds\n",
      "PyTorch inference took 0.03403925895690918 sec\n",
      "PyTorch postprocessing took 0.014189481735229492 sec\n",
      "Frame 273 processing time: 0.0826 seconds\n",
      "PyTorch inference took 0.003880023956298828 sec\n",
      "PyTorch postprocessing took 0.010079383850097656 sec\n",
      "Frame 274 processing time: 0.0310 seconds\n",
      "PyTorch inference took 0.0043849945068359375 sec\n",
      "PyTorch postprocessing took 0.012977123260498047 sec\n",
      "Frame 275 processing time: 0.0369 seconds\n",
      "PyTorch inference took 0.00514674186706543 sec\n",
      "PyTorch postprocessing took 0.008365631103515625 sec\n",
      "Frame 276 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.00634312629699707 sec\n",
      "PyTorch postprocessing took 0.01177668571472168 sec\n",
      "Frame 277 processing time: 0.0702 seconds\n",
      "PyTorch inference took 0.002985715866088867 sec\n",
      "PyTorch postprocessing took 0.010077476501464844 sec\n",
      "Frame 278 processing time: 0.0261 seconds\n",
      "PyTorch inference took 0.002963542938232422 sec\n",
      "PyTorch postprocessing took 0.009966135025024414 sec\n",
      "Frame 279 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.003006458282470703 sec\n",
      "PyTorch postprocessing took 0.010091304779052734 sec\n",
      "Frame 280 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.0030472278594970703 sec\n",
      "PyTorch postprocessing took 0.010155200958251953 sec\n",
      "Frame 281 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.003002643585205078 sec\n",
      "PyTorch postprocessing took 0.010421037673950195 sec\n",
      "Frame 282 processing time: 0.0255 seconds\n",
      "PyTorch inference took 0.0030219554901123047 sec\n",
      "PyTorch postprocessing took 0.010426521301269531 sec\n",
      "Frame 283 processing time: 0.0266 seconds\n",
      "PyTorch inference took 0.003000974655151367 sec\n",
      "PyTorch postprocessing took 0.01034402847290039 sec\n",
      "Frame 284 processing time: 0.0293 seconds\n",
      "PyTorch inference took 0.0033440589904785156 sec\n",
      "PyTorch postprocessing took 0.00946950912475586 sec\n",
      "Frame 285 processing time: 0.0251 seconds\n",
      "PyTorch inference took 0.003766298294067383 sec\n",
      "PyTorch postprocessing took 0.0101776123046875 sec\n",
      "Frame 286 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.003870248794555664 sec\n",
      "PyTorch postprocessing took 0.009086847305297852 sec\n",
      "Frame 287 processing time: 0.0344 seconds\n",
      "PyTorch inference took 0.0063838958740234375 sec\n",
      "PyTorch postprocessing took 0.009031295776367188 sec\n",
      "Frame 288 processing time: 0.0443 seconds\n",
      "PyTorch inference took 0.009039878845214844 sec\n",
      "PyTorch postprocessing took 0.01129293441772461 sec\n",
      "Frame 289 processing time: 0.0690 seconds\n",
      "PyTorch inference took 0.003423452377319336 sec\n",
      "PyTorch postprocessing took 0.009860992431640625 sec\n",
      "Frame 290 processing time: 0.0297 seconds\n",
      "PyTorch inference took 0.004236698150634766 sec\n",
      "PyTorch postprocessing took 0.014731645584106445 sec\n",
      "Frame 291 processing time: 0.0431 seconds\n",
      "PyTorch inference took 0.00914454460144043 sec\n",
      "PyTorch postprocessing took 0.008884191513061523 sec\n",
      "Frame 292 processing time: 0.0616 seconds\n",
      "PyTorch inference took 0.011203527450561523 sec\n",
      "PyTorch postprocessing took 0.011844635009765625 sec\n",
      "Frame 293 processing time: 0.0977 seconds\n",
      "PyTorch inference took 0.0070383548736572266 sec\n",
      "PyTorch postprocessing took 0.006506204605102539 sec\n",
      "Frame 294 processing time: 0.0346 seconds\n",
      "PyTorch inference took 0.0037145614624023438 sec\n",
      "PyTorch postprocessing took 0.010275602340698242 sec\n",
      "Frame 295 processing time: 0.0340 seconds\n",
      "PyTorch inference took 0.0109710693359375 sec\n",
      "PyTorch postprocessing took 0.008387088775634766 sec\n",
      "Frame 296 processing time: 0.0554 seconds\n",
      "PyTorch inference took 0.006350040435791016 sec\n",
      "PyTorch postprocessing took 0.0092315673828125 sec\n",
      "Frame 297 processing time: 0.0420 seconds\n",
      "PyTorch inference took 0.007269144058227539 sec\n",
      "PyTorch postprocessing took 0.005545377731323242 sec\n",
      "Frame 298 processing time: 0.0455 seconds\n",
      "PyTorch inference took 0.003310680389404297 sec\n",
      "PyTorch postprocessing took 0.009680986404418945 sec\n",
      "Frame 299 processing time: 0.0281 seconds\n",
      "PyTorch inference took 0.00421905517578125 sec\n",
      "PyTorch postprocessing took 0.009629487991333008 sec\n",
      "Frame 300 processing time: 0.0320 seconds\n",
      "PyTorch inference took 0.006335258483886719 sec\n",
      "PyTorch postprocessing took 0.007093191146850586 sec\n",
      "Frame 301 processing time: 0.0398 seconds\n",
      "PyTorch inference took 0.00835728645324707 sec\n",
      "PyTorch postprocessing took 0.00891876220703125 sec\n",
      "Frame 302 processing time: 0.0375 seconds\n",
      "PyTorch inference took 0.006930112838745117 sec\n",
      "PyTorch postprocessing took 0.010900497436523438 sec\n",
      "Frame 303 processing time: 0.0762 seconds\n",
      "PyTorch inference took 0.0069217681884765625 sec\n",
      "PyTorch postprocessing took 0.007214069366455078 sec\n",
      "Frame 304 processing time: 0.0355 seconds\n",
      "PyTorch inference took 0.003719329833984375 sec\n",
      "PyTorch postprocessing took 0.010258674621582031 sec\n",
      "Frame 305 processing time: 0.0289 seconds\n",
      "PyTorch inference took 0.004193782806396484 sec\n",
      "PyTorch postprocessing took 0.008883476257324219 sec\n",
      "Frame 306 processing time: 0.0338 seconds\n",
      "PyTorch inference took 0.0030732154846191406 sec\n",
      "PyTorch postprocessing took 0.01038813591003418 sec\n",
      "Frame 307 processing time: 0.0290 seconds\n",
      "PyTorch inference took 0.0031392574310302734 sec\n",
      "PyTorch postprocessing took 0.009733200073242188 sec\n",
      "Frame 308 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.009006977081298828 sec\n",
      "PyTorch postprocessing took 0.006764888763427734 sec\n",
      "Frame 309 processing time: 0.0483 seconds\n",
      "PyTorch inference took 0.0030939579010009766 sec\n",
      "PyTorch postprocessing took 0.009557723999023438 sec\n",
      "Frame 310 processing time: 0.0282 seconds\n",
      "PyTorch inference took 0.003957033157348633 sec\n",
      "PyTorch postprocessing took 0.010042905807495117 sec\n",
      "Frame 311 processing time: 0.0286 seconds\n",
      "PyTorch inference took 0.003771543502807617 sec\n",
      "PyTorch postprocessing took 0.010092496871948242 sec\n",
      "Frame 312 processing time: 0.0335 seconds\n",
      "PyTorch inference took 0.0030689239501953125 sec\n",
      "PyTorch postprocessing took 0.009990930557250977 sec\n",
      "Frame 313 processing time: 0.0276 seconds\n",
      "PyTorch inference took 0.003026247024536133 sec\n",
      "PyTorch postprocessing took 0.010120868682861328 sec\n",
      "Frame 314 processing time: 0.0266 seconds\n",
      "PyTorch inference took 0.003156900405883789 sec\n",
      "PyTorch postprocessing took 0.009392499923706055 sec\n",
      "Frame 315 processing time: 0.0292 seconds\n",
      "PyTorch inference took 0.002943754196166992 sec\n",
      "PyTorch postprocessing took 0.009933710098266602 sec\n",
      "Frame 316 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.004338979721069336 sec\n",
      "PyTorch postprocessing took 0.00982356071472168 sec\n",
      "Frame 317 processing time: 0.0298 seconds\n",
      "PyTorch inference took 0.0031261444091796875 sec\n",
      "PyTorch postprocessing took 0.010022163391113281 sec\n",
      "Frame 318 processing time: 0.0351 seconds\n",
      "PyTorch inference took 0.00498199462890625 sec\n",
      "PyTorch postprocessing took 0.008542060852050781 sec\n",
      "Frame 319 processing time: 0.0337 seconds\n",
      "PyTorch inference took 0.004458427429199219 sec\n",
      "PyTorch postprocessing took 0.009437322616577148 sec\n",
      "Frame 320 processing time: 0.0330 seconds\n",
      "PyTorch inference took 0.01080179214477539 sec\n",
      "PyTorch postprocessing took 0.01116800308227539 sec\n",
      "Frame 321 processing time: 0.0498 seconds\n",
      "PyTorch inference took 0.004857301712036133 sec\n",
      "PyTorch postprocessing took 0.009461164474487305 sec\n",
      "Frame 322 processing time: 0.0314 seconds\n",
      "PyTorch inference took 0.0031871795654296875 sec\n",
      "PyTorch postprocessing took 0.009398221969604492 sec\n",
      "Frame 323 processing time: 0.0289 seconds\n",
      "PyTorch inference took 0.0038766860961914062 sec\n",
      "PyTorch postprocessing took 0.01505422592163086 sec\n",
      "Frame 324 processing time: 0.0408 seconds\n",
      "PyTorch inference took 0.0031278133392333984 sec\n",
      "PyTorch postprocessing took 0.009680986404418945 sec\n",
      "Frame 325 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.006612062454223633 sec\n",
      "PyTorch postprocessing took 0.00753021240234375 sec\n",
      "Frame 326 processing time: 0.0318 seconds\n",
      "PyTorch inference took 0.0038671493530273438 sec\n",
      "PyTorch postprocessing took 0.009555339813232422 sec\n",
      "Frame 327 processing time: 0.0308 seconds\n",
      "PyTorch inference took 0.0037217140197753906 sec\n",
      "PyTorch postprocessing took 0.010812044143676758 sec\n",
      "Frame 328 processing time: 0.0277 seconds\n",
      "PyTorch inference took 0.0030388832092285156 sec\n",
      "PyTorch postprocessing took 0.010071277618408203 sec\n",
      "Frame 329 processing time: 0.0301 seconds\n",
      "PyTorch inference took 0.004681825637817383 sec\n",
      "PyTorch postprocessing took 0.008433103561401367 sec\n",
      "Frame 330 processing time: 0.0288 seconds\n",
      "PyTorch inference took 0.0029435157775878906 sec\n",
      "PyTorch postprocessing took 0.00991511344909668 sec\n",
      "Frame 331 processing time: 0.0278 seconds\n",
      "PyTorch inference took 0.0030515193939208984 sec\n",
      "PyTorch postprocessing took 0.009562253952026367 sec\n",
      "Frame 332 processing time: 0.0272 seconds\n",
      "PyTorch inference took 0.0029718875885009766 sec\n",
      "PyTorch postprocessing took 0.00966024398803711 sec\n",
      "Frame 333 processing time: 0.0304 seconds\n",
      "PyTorch inference took 0.003452777862548828 sec\n",
      "PyTorch postprocessing took 0.009519100189208984 sec\n",
      "Frame 334 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.0029861927032470703 sec\n",
      "PyTorch postprocessing took 0.010123968124389648 sec\n",
      "Frame 335 processing time: 0.0279 seconds\n",
      "PyTorch inference took 0.0030295848846435547 sec\n",
      "PyTorch postprocessing took 0.010658740997314453 sec\n",
      "Frame 336 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.003815889358520508 sec\n",
      "PyTorch postprocessing took 0.009185075759887695 sec\n",
      "Frame 337 processing time: 0.0255 seconds\n",
      "PyTorch inference took 0.0029625892639160156 sec\n",
      "PyTorch postprocessing took 0.009864568710327148 sec\n",
      "Frame 338 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.0043795108795166016 sec\n",
      "PyTorch postprocessing took 0.008649110794067383 sec\n",
      "Frame 339 processing time: 0.0285 seconds\n",
      "PyTorch inference took 0.006829023361206055 sec\n",
      "PyTorch postprocessing took 0.011960506439208984 sec\n",
      "Frame 340 processing time: 0.0407 seconds\n",
      "PyTorch inference took 0.003842592239379883 sec\n",
      "PyTorch postprocessing took 0.009557962417602539 sec\n",
      "Frame 341 processing time: 0.0312 seconds\n",
      "PyTorch inference took 0.0034902095794677734 sec\n",
      "PyTorch postprocessing took 0.009450435638427734 sec\n",
      "Frame 342 processing time: 0.0309 seconds\n",
      "PyTorch inference took 0.0028731822967529297 sec\n",
      "PyTorch postprocessing took 0.009790897369384766 sec\n",
      "Frame 343 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.0030117034912109375 sec\n",
      "PyTorch postprocessing took 0.009546518325805664 sec\n",
      "Frame 344 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.0029180049896240234 sec\n",
      "PyTorch postprocessing took 0.009930133819580078 sec\n",
      "Frame 345 processing time: 0.0279 seconds\n",
      "PyTorch inference took 0.003905057907104492 sec\n",
      "PyTorch postprocessing took 0.00995326042175293 sec\n",
      "Frame 346 processing time: 0.0308 seconds\n",
      "PyTorch inference took 0.004049062728881836 sec\n",
      "PyTorch postprocessing took 0.008918523788452148 sec\n",
      "Frame 347 processing time: 0.0313 seconds\n",
      "PyTorch inference took 0.0033936500549316406 sec\n",
      "PyTorch postprocessing took 0.009315967559814453 sec\n",
      "Frame 348 processing time: 0.0318 seconds\n",
      "PyTorch inference took 0.0042667388916015625 sec\n",
      "PyTorch postprocessing took 0.008566617965698242 sec\n",
      "Frame 349 processing time: 0.0285 seconds\n",
      "PyTorch inference took 0.0028929710388183594 sec\n",
      "PyTorch postprocessing took 0.009948968887329102 sec\n",
      "Frame 350 processing time: 0.0287 seconds\n",
      "PyTorch inference took 0.0033740997314453125 sec\n",
      "PyTorch postprocessing took 0.00929713249206543 sec\n",
      "Frame 351 processing time: 0.0298 seconds\n",
      "PyTorch inference took 0.004523754119873047 sec\n",
      "PyTorch postprocessing took 0.009722232818603516 sec\n",
      "Frame 352 processing time: 0.0293 seconds\n",
      "PyTorch inference took 0.004453182220458984 sec\n",
      "PyTorch postprocessing took 0.009694814682006836 sec\n",
      "Frame 353 processing time: 0.0293 seconds\n",
      "PyTorch inference took 0.003748178482055664 sec\n",
      "PyTorch postprocessing took 0.009323596954345703 sec\n",
      "Frame 354 processing time: 0.0269 seconds\n",
      "PyTorch inference took 0.0029947757720947266 sec\n",
      "PyTorch postprocessing took 0.010230541229248047 sec\n",
      "Frame 355 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.003688812255859375 sec\n",
      "PyTorch postprocessing took 0.009519577026367188 sec\n",
      "Frame 356 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.0030074119567871094 sec\n",
      "PyTorch postprocessing took 0.010198831558227539 sec\n",
      "Frame 357 processing time: 0.0287 seconds\n",
      "PyTorch inference took 0.009722232818603516 sec\n",
      "PyTorch postprocessing took 0.007928609848022461 sec\n",
      "Frame 358 processing time: 0.0477 seconds\n",
      "PyTorch inference took 0.003794431686401367 sec\n",
      "PyTorch postprocessing took 0.018367767333984375 sec\n",
      "Frame 359 processing time: 0.0382 seconds\n",
      "PyTorch inference took 0.008746623992919922 sec\n",
      "PyTorch postprocessing took 0.01170659065246582 sec\n",
      "Frame 360 processing time: 0.0614 seconds\n",
      "PyTorch inference took 0.003275156021118164 sec\n",
      "PyTorch postprocessing took 0.009327173233032227 sec\n",
      "Frame 361 processing time: 0.0298 seconds\n",
      "PyTorch inference took 0.006227016448974609 sec\n",
      "PyTorch postprocessing took 0.00921010971069336 sec\n",
      "Frame 362 processing time: 0.0508 seconds\n",
      "PyTorch inference took 0.007544994354248047 sec\n",
      "PyTorch postprocessing took 0.009895563125610352 sec\n",
      "Frame 363 processing time: 0.0551 seconds\n",
      "PyTorch inference took 0.00435948371887207 sec\n",
      "PyTorch postprocessing took 0.009199142456054688 sec\n",
      "Frame 364 processing time: 0.0311 seconds\n",
      "PyTorch inference took 0.0044744014739990234 sec\n",
      "PyTorch postprocessing took 0.016648530960083008 sec\n",
      "Frame 365 processing time: 0.0425 seconds\n",
      "PyTorch inference took 0.0038025379180908203 sec\n",
      "PyTorch postprocessing took 0.009949445724487305 sec\n",
      "Frame 366 processing time: 0.0281 seconds\n",
      "PyTorch inference took 0.003815174102783203 sec\n",
      "PyTorch postprocessing took 0.011599302291870117 sec\n",
      "Frame 367 processing time: 0.0367 seconds\n",
      "PyTorch inference took 0.006981611251831055 sec\n",
      "PyTorch postprocessing took 0.023502826690673828 sec\n",
      "Frame 368 processing time: 0.0664 seconds\n",
      "PyTorch inference took 0.008066177368164062 sec\n",
      "PyTorch postprocessing took 0.011326789855957031 sec\n",
      "Frame 369 processing time: 0.0678 seconds\n",
      "PyTorch inference took 0.003669261932373047 sec\n",
      "PyTorch postprocessing took 0.009340524673461914 sec\n",
      "Frame 370 processing time: 0.0338 seconds\n",
      "PyTorch inference took 0.009093761444091797 sec\n",
      "PyTorch postprocessing took 0.014469623565673828 sec\n",
      "Frame 371 processing time: 0.0542 seconds\n",
      "PyTorch inference took 0.0033540725708007812 sec\n",
      "PyTorch postprocessing took 0.009850025177001953 sec\n",
      "Frame 372 processing time: 0.0306 seconds\n",
      "PyTorch inference took 0.006737709045410156 sec\n",
      "PyTorch postprocessing took 0.014267683029174805 sec\n",
      "Frame 373 processing time: 0.0435 seconds\n",
      "PyTorch inference took 0.002986907958984375 sec\n",
      "PyTorch postprocessing took 0.009820222854614258 sec\n",
      "Frame 374 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.00838923454284668 sec\n",
      "PyTorch postprocessing took 0.009645223617553711 sec\n",
      "Frame 375 processing time: 0.0383 seconds\n",
      "PyTorch inference took 0.0037696361541748047 sec\n",
      "PyTorch postprocessing took 0.014998197555541992 sec\n",
      "Frame 376 processing time: 0.0452 seconds\n",
      "PyTorch inference took 0.006173372268676758 sec\n",
      "PyTorch postprocessing took 0.012701749801635742 sec\n",
      "Frame 377 processing time: 0.0445 seconds\n",
      "PyTorch inference took 0.0039033889770507812 sec\n",
      "PyTorch postprocessing took 0.010014533996582031 sec\n",
      "Frame 378 processing time: 0.0286 seconds\n",
      "PyTorch inference took 0.003490924835205078 sec\n",
      "PyTorch postprocessing took 0.013867855072021484 sec\n",
      "Frame 379 processing time: 0.0329 seconds\n",
      "PyTorch inference took 0.0034101009368896484 sec\n",
      "PyTorch postprocessing took 0.009550333023071289 sec\n",
      "Frame 380 processing time: 0.0316 seconds\n",
      "PyTorch inference took 0.003717660903930664 sec\n",
      "PyTorch postprocessing took 0.009580373764038086 sec\n",
      "Frame 381 processing time: 0.0352 seconds\n",
      "PyTorch inference took 0.0038611888885498047 sec\n",
      "PyTorch postprocessing took 0.009584665298461914 sec\n",
      "Frame 382 processing time: 0.0310 seconds\n",
      "PyTorch inference took 0.007854700088500977 sec\n",
      "PyTorch postprocessing took 0.01195073127746582 sec\n",
      "Frame 383 processing time: 0.0552 seconds\n",
      "PyTorch inference took 0.0040552616119384766 sec\n",
      "PyTorch postprocessing took 0.012726306915283203 sec\n",
      "Frame 384 processing time: 0.0332 seconds\n",
      "PyTorch inference took 0.005213260650634766 sec\n",
      "PyTorch postprocessing took 0.009365320205688477 sec\n",
      "Frame 385 processing time: 0.0471 seconds\n",
      "PyTorch inference took 0.0030813217163085938 sec\n",
      "PyTorch postprocessing took 0.009699583053588867 sec\n",
      "Frame 386 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.003148794174194336 sec\n",
      "PyTorch postprocessing took 0.009620189666748047 sec\n",
      "Frame 387 processing time: 0.0256 seconds\n",
      "PyTorch inference took 0.0030105113983154297 sec\n",
      "PyTorch postprocessing took 0.010354042053222656 sec\n",
      "Frame 388 processing time: 0.0333 seconds\n",
      "PyTorch inference took 0.011700153350830078 sec\n",
      "PyTorch postprocessing took 0.018172264099121094 sec\n",
      "Frame 389 processing time: 0.0514 seconds\n",
      "PyTorch inference took 0.0032320022583007812 sec\n",
      "PyTorch postprocessing took 0.009715795516967773 sec\n",
      "Frame 390 processing time: 0.0314 seconds\n",
      "PyTorch inference took 0.003302335739135742 sec\n",
      "PyTorch postprocessing took 0.009231090545654297 sec\n",
      "Frame 391 processing time: 0.0328 seconds\n",
      "PyTorch inference took 0.004336118698120117 sec\n",
      "PyTorch postprocessing took 0.016314983367919922 sec\n",
      "Frame 392 processing time: 0.0383 seconds\n",
      "PyTorch inference took 0.0036139488220214844 sec\n",
      "PyTorch postprocessing took 0.008969545364379883 sec\n",
      "Frame 393 processing time: 0.0310 seconds\n",
      "PyTorch inference took 0.003098726272583008 sec\n",
      "PyTorch postprocessing took 0.009864330291748047 sec\n",
      "Frame 394 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.002900838851928711 sec\n",
      "PyTorch postprocessing took 0.010129928588867188 sec\n",
      "Frame 395 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.0029282569885253906 sec\n",
      "PyTorch postprocessing took 0.009700298309326172 sec\n",
      "Frame 396 processing time: 0.0294 seconds\n",
      "PyTorch inference took 0.007700443267822266 sec\n",
      "PyTorch postprocessing took 0.017476797103881836 sec\n",
      "Frame 397 processing time: 0.0757 seconds\n",
      "PyTorch inference took 0.003156423568725586 sec\n",
      "PyTorch postprocessing took 0.009505748748779297 sec\n",
      "Frame 398 processing time: 0.0333 seconds\n",
      "PyTorch inference took 0.0044667720794677734 sec\n",
      "PyTorch postprocessing took 0.00969243049621582 sec\n",
      "Frame 399 processing time: 0.0316 seconds\n",
      "PyTorch inference took 0.0030884742736816406 sec\n",
      "PyTorch postprocessing took 0.009862184524536133 sec\n",
      "Frame 400 processing time: 0.0361 seconds\n",
      "PyTorch inference took 0.002949237823486328 sec\n",
      "PyTorch postprocessing took 0.00996708869934082 sec\n",
      "Frame 401 processing time: 0.0261 seconds\n",
      "PyTorch inference took 0.002945423126220703 sec\n",
      "PyTorch postprocessing took 0.009828329086303711 sec\n",
      "Frame 402 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.003072977066040039 sec\n",
      "PyTorch postprocessing took 0.009842872619628906 sec\n",
      "Frame 403 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.003193378448486328 sec\n",
      "PyTorch postprocessing took 0.009964704513549805 sec\n",
      "Frame 404 processing time: 0.0266 seconds\n",
      "PyTorch inference took 0.0029942989349365234 sec\n",
      "PyTorch postprocessing took 0.010709762573242188 sec\n",
      "Frame 405 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.0030014514923095703 sec\n",
      "PyTorch postprocessing took 0.010023832321166992 sec\n",
      "Frame 406 processing time: 0.0343 seconds\n",
      "PyTorch inference took 0.003164529800415039 sec\n",
      "PyTorch postprocessing took 0.00930476188659668 sec\n",
      "Frame 407 processing time: 0.0255 seconds\n",
      "PyTorch inference took 0.0031766891479492188 sec\n",
      "PyTorch postprocessing took 0.009483814239501953 sec\n",
      "Frame 408 processing time: 0.0297 seconds\n",
      "PyTorch inference took 0.0040798187255859375 sec\n",
      "PyTorch postprocessing took 0.009801387786865234 sec\n",
      "Frame 409 processing time: 0.0353 seconds\n",
      "PyTorch inference took 0.003702878952026367 sec\n",
      "PyTorch postprocessing took 0.009818077087402344 sec\n",
      "Frame 410 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.010456085205078125 sec\n",
      "PyTorch postprocessing took 0.010562419891357422 sec\n",
      "Frame 411 processing time: 0.0583 seconds\n",
      "PyTorch inference took 0.01308131217956543 sec\n",
      "PyTorch postprocessing took 0.010373592376708984 sec\n",
      "Frame 412 processing time: 0.0466 seconds\n",
      "PyTorch inference took 0.0273287296295166 sec\n",
      "PyTorch postprocessing took 0.014757156372070312 sec\n",
      "Frame 413 processing time: 0.1051 seconds\n",
      "PyTorch inference took 0.004018068313598633 sec\n",
      "PyTorch postprocessing took 0.009909391403198242 sec\n",
      "Frame 414 processing time: 0.0287 seconds\n",
      "PyTorch inference took 0.003008604049682617 sec\n",
      "PyTorch postprocessing took 0.009779930114746094 sec\n",
      "Frame 415 processing time: 0.0245 seconds\n",
      "PyTorch inference took 0.0031919479370117188 sec\n",
      "PyTorch postprocessing took 0.00960230827331543 sec\n",
      "Frame 416 processing time: 0.0345 seconds\n",
      "PyTorch inference took 0.003103971481323242 sec\n",
      "PyTorch postprocessing took 0.009483814239501953 sec\n",
      "Frame 417 processing time: 0.0319 seconds\n",
      "PyTorch inference took 0.0029468536376953125 sec\n",
      "PyTorch postprocessing took 0.009976863861083984 sec\n",
      "Frame 418 processing time: 0.0281 seconds\n",
      "PyTorch inference took 0.0031387805938720703 sec\n",
      "PyTorch postprocessing took 0.009855031967163086 sec\n",
      "Frame 419 processing time: 0.0348 seconds\n",
      "PyTorch inference took 0.0037157535552978516 sec\n",
      "PyTorch postprocessing took 0.009341955184936523 sec\n",
      "Frame 420 processing time: 0.0266 seconds\n",
      "PyTorch inference took 0.014157533645629883 sec\n",
      "PyTorch postprocessing took 0.01251363754272461 sec\n",
      "Frame 421 processing time: 0.0489 seconds\n",
      "PyTorch inference took 0.003061056137084961 sec\n",
      "PyTorch postprocessing took 0.010147571563720703 sec\n",
      "Frame 422 processing time: 0.0364 seconds\n",
      "PyTorch inference took 0.004082679748535156 sec\n",
      "PyTorch postprocessing took 0.009285688400268555 sec\n",
      "Frame 423 processing time: 0.0283 seconds\n",
      "PyTorch inference took 0.0030410289764404297 sec\n",
      "PyTorch postprocessing took 0.009994983673095703 sec\n",
      "Frame 424 processing time: 0.0284 seconds\n",
      "PyTorch inference took 0.0032014846801757812 sec\n",
      "PyTorch postprocessing took 0.009882450103759766 sec\n",
      "Frame 425 processing time: 0.0314 seconds\n",
      "PyTorch inference took 0.0030438899993896484 sec\n",
      "PyTorch postprocessing took 0.009619474411010742 sec\n",
      "Frame 426 processing time: 0.0285 seconds\n",
      "PyTorch inference took 0.004334688186645508 sec\n",
      "PyTorch postprocessing took 0.009444475173950195 sec\n",
      "Frame 427 processing time: 0.0407 seconds\n",
      "PyTorch inference took 0.006559848785400391 sec\n",
      "PyTorch postprocessing took 0.0062258243560791016 sec\n",
      "Frame 428 processing time: 0.0410 seconds\n",
      "PyTorch inference took 0.0067522525787353516 sec\n",
      "PyTorch postprocessing took 0.01108098030090332 sec\n",
      "Frame 429 processing time: 0.0585 seconds\n",
      "PyTorch inference took 0.003989696502685547 sec\n",
      "PyTorch postprocessing took 0.010039091110229492 sec\n",
      "Frame 430 processing time: 0.0338 seconds\n",
      "PyTorch inference took 0.01198124885559082 sec\n",
      "PyTorch postprocessing took 0.012346744537353516 sec\n",
      "Frame 431 processing time: 0.0659 seconds\n",
      "PyTorch inference took 0.003945827484130859 sec\n",
      "PyTorch postprocessing took 0.009897232055664062 sec\n",
      "Frame 432 processing time: 0.0281 seconds\n",
      "PyTorch inference took 0.006757020950317383 sec\n",
      "PyTorch postprocessing took 0.008486270904541016 sec\n",
      "Frame 433 processing time: 0.0491 seconds\n",
      "PyTorch inference took 0.0037453174591064453 sec\n",
      "PyTorch postprocessing took 0.011472940444946289 sec\n",
      "Frame 434 processing time: 0.0387 seconds\n",
      "PyTorch inference took 0.003500699996948242 sec\n",
      "PyTorch postprocessing took 0.010395288467407227 sec\n",
      "Frame 435 processing time: 0.0334 seconds\n",
      "PyTorch inference took 0.003766775131225586 sec\n",
      "PyTorch postprocessing took 0.011883020401000977 sec\n",
      "Frame 436 processing time: 0.0331 seconds\n",
      "PyTorch inference took 0.003153562545776367 sec\n",
      "PyTorch postprocessing took 0.009856700897216797 sec\n",
      "Frame 437 processing time: 0.0327 seconds\n",
      "PyTorch inference took 0.007785797119140625 sec\n",
      "PyTorch postprocessing took 0.009948492050170898 sec\n",
      "Frame 438 processing time: 0.0384 seconds\n",
      "PyTorch inference took 0.0037572383880615234 sec\n",
      "PyTorch postprocessing took 0.009947061538696289 sec\n",
      "Frame 439 processing time: 0.0275 seconds\n",
      "PyTorch inference took 0.003768444061279297 sec\n",
      "PyTorch postprocessing took 0.010197639465332031 sec\n",
      "Frame 440 processing time: 0.0366 seconds\n",
      "PyTorch inference took 0.00888824462890625 sec\n",
      "PyTorch postprocessing took 0.016745567321777344 sec\n",
      "Frame 441 processing time: 0.0750 seconds\n",
      "PyTorch inference took 0.004502296447753906 sec\n",
      "PyTorch postprocessing took 0.016056537628173828 sec\n",
      "Frame 442 processing time: 0.0391 seconds\n",
      "PyTorch inference took 0.00951695442199707 sec\n",
      "PyTorch postprocessing took 0.01820087432861328 sec\n",
      "Frame 443 processing time: 0.0650 seconds\n",
      "PyTorch inference took 0.0040929317474365234 sec\n",
      "PyTorch postprocessing took 0.009900808334350586 sec\n",
      "Frame 444 processing time: 0.0323 seconds\n",
      "PyTorch inference took 0.007877349853515625 sec\n",
      "PyTorch postprocessing took 0.011628389358520508 sec\n",
      "Frame 445 processing time: 0.0530 seconds\n",
      "PyTorch inference took 0.006170511245727539 sec\n",
      "PyTorch postprocessing took 0.009859085083007812 sec\n",
      "Frame 446 processing time: 0.0423 seconds\n",
      "PyTorch inference took 0.0037708282470703125 sec\n",
      "PyTorch postprocessing took 0.009648799896240234 sec\n",
      "Frame 447 processing time: 0.0299 seconds\n",
      "PyTorch inference took 0.00747370719909668 sec\n",
      "PyTorch postprocessing took 0.009120702743530273 sec\n",
      "Frame 448 processing time: 0.0543 seconds\n",
      "PyTorch inference took 0.004220008850097656 sec\n",
      "PyTorch postprocessing took 0.011670827865600586 sec\n",
      "Frame 449 processing time: 0.0337 seconds\n",
      "PyTorch inference took 0.004591226577758789 sec\n",
      "PyTorch postprocessing took 0.009767532348632812 sec\n",
      "Frame 450 processing time: 0.0353 seconds\n",
      "PyTorch inference took 0.009195327758789062 sec\n",
      "PyTorch postprocessing took 0.0081024169921875 sec\n",
      "Frame 451 processing time: 0.0431 seconds\n",
      "PyTorch inference took 0.003769397735595703 sec\n",
      "PyTorch postprocessing took 0.010879278182983398 sec\n",
      "Frame 452 processing time: 0.0313 seconds\n",
      "PyTorch inference took 0.004881143569946289 sec\n",
      "PyTorch postprocessing took 0.008752822875976562 sec\n",
      "Frame 453 processing time: 0.0333 seconds\n",
      "PyTorch inference took 0.0032019615173339844 sec\n",
      "PyTorch postprocessing took 0.00939798355102539 sec\n",
      "Frame 454 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.0029892921447753906 sec\n",
      "PyTorch postprocessing took 0.009533166885375977 sec\n",
      "Frame 455 processing time: 0.0321 seconds\n",
      "PyTorch inference took 0.003454446792602539 sec\n",
      "PyTorch postprocessing took 0.01006627082824707 sec\n",
      "Frame 456 processing time: 0.0287 seconds\n",
      "PyTorch inference took 0.003334522247314453 sec\n",
      "PyTorch postprocessing took 0.00971078872680664 sec\n",
      "Frame 457 processing time: 0.0294 seconds\n",
      "PyTorch inference took 0.00302886962890625 sec\n",
      "PyTorch postprocessing took 0.009638547897338867 sec\n",
      "Frame 458 processing time: 0.0283 seconds\n",
      "PyTorch inference took 0.0030362606048583984 sec\n",
      "PyTorch postprocessing took 0.009732723236083984 sec\n",
      "Frame 459 processing time: 0.0292 seconds\n",
      "PyTorch inference took 0.0034503936767578125 sec\n",
      "PyTorch postprocessing took 0.009596109390258789 sec\n",
      "Frame 460 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.003105640411376953 sec\n",
      "PyTorch postprocessing took 0.010179758071899414 sec\n",
      "Frame 461 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.0035924911499023438 sec\n",
      "PyTorch postprocessing took 0.009612798690795898 sec\n",
      "Frame 462 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.003093242645263672 sec\n",
      "PyTorch postprocessing took 0.009951114654541016 sec\n",
      "Frame 463 processing time: 0.0354 seconds\n",
      "PyTorch inference took 0.003041982650756836 sec\n",
      "PyTorch postprocessing took 0.009444713592529297 sec\n",
      "Frame 464 processing time: 0.0284 seconds\n",
      "PyTorch inference took 0.002956867218017578 sec\n",
      "PyTorch postprocessing took 0.014176130294799805 sec\n",
      "Frame 465 processing time: 0.0339 seconds\n",
      "PyTorch inference took 0.003788471221923828 sec\n",
      "PyTorch postprocessing took 0.010107040405273438 sec\n",
      "Frame 466 processing time: 0.0303 seconds\n",
      "PyTorch inference took 0.0060269832611083984 sec\n",
      "PyTorch postprocessing took 0.016039609909057617 sec\n",
      "Frame 467 processing time: 0.0429 seconds\n",
      "PyTorch inference took 0.0045206546783447266 sec\n",
      "PyTorch postprocessing took 0.013066768646240234 sec\n",
      "Frame 468 processing time: 0.0343 seconds\n",
      "PyTorch inference took 0.003381490707397461 sec\n",
      "PyTorch postprocessing took 0.009880781173706055 sec\n",
      "Frame 469 processing time: 0.0316 seconds\n",
      "PyTorch inference took 0.008978843688964844 sec\n",
      "PyTorch postprocessing took 0.017153501510620117 sec\n",
      "Frame 470 processing time: 0.0677 seconds\n",
      "PyTorch inference took 0.003914356231689453 sec\n",
      "PyTorch postprocessing took 0.010083436965942383 sec\n",
      "Frame 471 processing time: 0.0335 seconds\n",
      "PyTorch inference took 0.00805974006652832 sec\n",
      "PyTorch postprocessing took 0.010797500610351562 sec\n",
      "Frame 472 processing time: 0.0521 seconds\n",
      "PyTorch inference took 0.00913548469543457 sec\n",
      "PyTorch postprocessing took 0.009041070938110352 sec\n",
      "Frame 473 processing time: 0.0413 seconds\n",
      "PyTorch inference took 0.0029120445251464844 sec\n",
      "PyTorch postprocessing took 0.009737014770507812 sec\n",
      "Frame 474 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.0067119598388671875 sec\n",
      "PyTorch postprocessing took 0.016185522079467773 sec\n",
      "Frame 475 processing time: 0.0455 seconds\n",
      "PyTorch inference took 0.004913330078125 sec\n",
      "PyTorch postprocessing took 0.00890040397644043 sec\n",
      "Frame 476 processing time: 0.0316 seconds\n",
      "PyTorch inference took 0.005754947662353516 sec\n",
      "PyTorch postprocessing took 0.00945591926574707 sec\n",
      "Frame 477 processing time: 0.0304 seconds\n",
      "PyTorch inference took 0.007660627365112305 sec\n",
      "PyTorch postprocessing took 0.005865335464477539 sec\n",
      "Frame 478 processing time: 0.0349 seconds\n",
      "PyTorch inference took 0.0029878616333007812 sec\n",
      "PyTorch postprocessing took 0.009547233581542969 sec\n",
      "Frame 479 processing time: 0.0284 seconds\n",
      "PyTorch inference took 0.003995418548583984 sec\n",
      "PyTorch postprocessing took 0.015151500701904297 sec\n",
      "Frame 480 processing time: 0.0424 seconds\n",
      "PyTorch inference took 0.004004240036010742 sec\n",
      "PyTorch postprocessing took 0.00853109359741211 sec\n",
      "Frame 481 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.005858898162841797 sec\n",
      "PyTorch postprocessing took 0.008385181427001953 sec\n",
      "Frame 482 processing time: 0.0405 seconds\n",
      "PyTorch inference took 0.006377696990966797 sec\n",
      "PyTorch postprocessing took 0.009682893753051758 sec\n",
      "Frame 483 processing time: 0.0469 seconds\n",
      "PyTorch inference took 0.007070302963256836 sec\n",
      "PyTorch postprocessing took 0.009567499160766602 sec\n",
      "Frame 484 processing time: 0.0626 seconds\n",
      "PyTorch inference took 0.004464387893676758 sec\n",
      "PyTorch postprocessing took 0.00996851921081543 sec\n",
      "Frame 485 processing time: 0.0351 seconds\n",
      "PyTorch inference took 0.00802159309387207 sec\n",
      "PyTorch postprocessing took 0.013843774795532227 sec\n",
      "Frame 486 processing time: 0.0588 seconds\n",
      "PyTorch inference took 0.004113912582397461 sec\n",
      "PyTorch postprocessing took 0.01104426383972168 sec\n",
      "Frame 487 processing time: 0.0343 seconds\n",
      "PyTorch inference took 0.005114316940307617 sec\n",
      "PyTorch postprocessing took 0.009193658828735352 sec\n",
      "Frame 488 processing time: 0.0380 seconds\n",
      "PyTorch inference took 0.004000425338745117 sec\n",
      "PyTorch postprocessing took 0.009917020797729492 sec\n",
      "Frame 489 processing time: 0.0295 seconds\n",
      "PyTorch inference took 0.007058143615722656 sec\n",
      "PyTorch postprocessing took 0.008059501647949219 sec\n",
      "Frame 490 processing time: 0.0393 seconds\n",
      "PyTorch inference took 0.005688667297363281 sec\n",
      "PyTorch postprocessing took 0.009184598922729492 sec\n",
      "Frame 491 processing time: 0.0409 seconds\n",
      "PyTorch inference took 0.002960681915283203 sec\n",
      "PyTorch postprocessing took 0.009507179260253906 sec\n",
      "Frame 492 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.0072748661041259766 sec\n",
      "PyTorch postprocessing took 0.007147550582885742 sec\n",
      "Frame 493 processing time: 0.0477 seconds\n",
      "PyTorch inference took 0.008458852767944336 sec\n",
      "PyTorch postprocessing took 0.011016130447387695 sec\n",
      "Frame 494 processing time: 0.0733 seconds\n",
      "PyTorch inference took 0.005175590515136719 sec\n",
      "PyTorch postprocessing took 0.017479419708251953 sec\n",
      "Frame 495 processing time: 0.0446 seconds\n",
      "PyTorch inference took 0.004949331283569336 sec\n",
      "PyTorch postprocessing took 0.01479649543762207 sec\n",
      "Frame 496 processing time: 0.0360 seconds\n",
      "PyTorch inference took 0.003961324691772461 sec\n",
      "PyTorch postprocessing took 0.011193037033081055 sec\n",
      "Frame 497 processing time: 0.0337 seconds\n",
      "PyTorch inference took 0.010723114013671875 sec\n",
      "PyTorch postprocessing took 0.016756057739257812 sec\n",
      "Frame 498 processing time: 0.0440 seconds\n",
      "PyTorch inference took 0.009169816970825195 sec\n",
      "PyTorch postprocessing took 0.0059812068939208984 sec\n",
      "Frame 499 processing time: 0.0348 seconds\n",
      "PyTorch inference took 0.008860349655151367 sec\n",
      "PyTorch postprocessing took 0.009996175765991211 sec\n",
      "Frame 500 processing time: 0.0662 seconds\n",
      "PyTorch inference took 0.003773212432861328 sec\n",
      "PyTorch postprocessing took 0.009653091430664062 sec\n",
      "Frame 501 processing time: 0.0278 seconds\n",
      "PyTorch inference took 0.005238056182861328 sec\n",
      "PyTorch postprocessing took 0.015110492706298828 sec\n",
      "Frame 502 processing time: 0.0393 seconds\n",
      "PyTorch inference took 0.007873296737670898 sec\n",
      "PyTorch postprocessing took 0.019296646118164062 sec\n",
      "Frame 503 processing time: 0.0537 seconds\n",
      "PyTorch inference took 0.0038106441497802734 sec\n",
      "PyTorch postprocessing took 0.012308359146118164 sec\n",
      "Frame 504 processing time: 0.0325 seconds\n",
      "PyTorch inference took 0.007338047027587891 sec\n",
      "PyTorch postprocessing took 0.006985664367675781 sec\n",
      "Frame 505 processing time: 0.0451 seconds\n",
      "PyTorch inference took 0.0048656463623046875 sec\n",
      "PyTorch postprocessing took 0.014182329177856445 sec\n",
      "Frame 506 processing time: 0.0379 seconds\n",
      "PyTorch inference took 0.004148006439208984 sec\n",
      "PyTorch postprocessing took 0.01501917839050293 sec\n",
      "Frame 507 processing time: 0.0454 seconds\n",
      "PyTorch inference took 0.0035593509674072266 sec\n",
      "PyTorch postprocessing took 0.009192228317260742 sec\n",
      "Frame 508 processing time: 0.0325 seconds\n",
      "PyTorch inference took 0.004110574722290039 sec\n",
      "PyTorch postprocessing took 0.013977289199829102 sec\n",
      "Frame 509 processing time: 0.0372 seconds\n",
      "PyTorch inference took 0.0076045989990234375 sec\n",
      "PyTorch postprocessing took 0.011752843856811523 sec\n",
      "Frame 510 processing time: 0.0435 seconds\n",
      "PyTorch inference took 0.0038144588470458984 sec\n",
      "PyTorch postprocessing took 0.014786481857299805 sec\n",
      "Frame 511 processing time: 0.0408 seconds\n",
      "PyTorch inference took 0.010900020599365234 sec\n",
      "PyTorch postprocessing took 0.009444475173950195 sec\n",
      "Frame 512 processing time: 0.0484 seconds\n",
      "PyTorch inference took 0.0037012100219726562 sec\n",
      "PyTorch postprocessing took 0.00954747200012207 sec\n",
      "Frame 513 processing time: 0.0289 seconds\n",
      "PyTorch inference took 0.0039958953857421875 sec\n",
      "PyTorch postprocessing took 0.010025501251220703 sec\n",
      "Frame 514 processing time: 0.0352 seconds\n",
      "PyTorch inference took 0.0030896663665771484 sec\n",
      "PyTorch postprocessing took 0.010004997253417969 sec\n",
      "Frame 515 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.003519296646118164 sec\n",
      "PyTorch postprocessing took 0.00970458984375 sec\n",
      "Frame 516 processing time: 0.0286 seconds\n",
      "PyTorch inference took 0.0033197402954101562 sec\n",
      "PyTorch postprocessing took 0.00963902473449707 sec\n",
      "Frame 517 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.0029888153076171875 sec\n",
      "PyTorch postprocessing took 0.009553909301757812 sec\n",
      "Frame 518 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.005017995834350586 sec\n",
      "PyTorch postprocessing took 0.0085601806640625 sec\n",
      "Frame 519 processing time: 0.0288 seconds\n",
      "PyTorch inference took 0.0030388832092285156 sec\n",
      "PyTorch postprocessing took 0.00945901870727539 sec\n",
      "Frame 520 processing time: 0.0299 seconds\n",
      "PyTorch inference took 0.003798246383666992 sec\n",
      "PyTorch postprocessing took 0.0086669921875 sec\n",
      "Frame 521 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.003011465072631836 sec\n",
      "PyTorch postprocessing took 0.009577035903930664 sec\n",
      "Frame 522 processing time: 0.0288 seconds\n",
      "PyTorch inference took 0.002923727035522461 sec\n",
      "PyTorch postprocessing took 0.009574651718139648 sec\n",
      "Frame 523 processing time: 0.0276 seconds\n",
      "PyTorch inference took 0.003094196319580078 sec\n",
      "PyTorch postprocessing took 0.00993490219116211 sec\n",
      "Frame 524 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.0110931396484375 sec\n",
      "PyTorch postprocessing took 0.012767553329467773 sec\n",
      "Frame 525 processing time: 0.0655 seconds\n",
      "PyTorch inference took 0.007321834564208984 sec\n",
      "PyTorch postprocessing took 0.010331153869628906 sec\n",
      "Frame 526 processing time: 0.0452 seconds\n",
      "PyTorch inference took 0.004809141159057617 sec\n",
      "PyTorch postprocessing took 0.014424562454223633 sec\n",
      "Frame 527 processing time: 0.0476 seconds\n",
      "PyTorch inference took 0.0036721229553222656 sec\n",
      "PyTorch postprocessing took 0.009390830993652344 sec\n",
      "Frame 528 processing time: 0.0284 seconds\n",
      "PyTorch inference took 0.020193815231323242 sec\n",
      "PyTorch postprocessing took 0.008924484252929688 sec\n",
      "Frame 529 processing time: 0.0554 seconds\n",
      "PyTorch inference took 0.0073392391204833984 sec\n",
      "PyTorch postprocessing took 0.009738445281982422 sec\n",
      "Frame 530 processing time: 0.0406 seconds\n",
      "PyTorch inference took 0.006943464279174805 sec\n",
      "PyTorch postprocessing took 0.008385419845581055 sec\n",
      "Frame 531 processing time: 0.0330 seconds\n",
      "PyTorch inference took 0.014156341552734375 sec\n",
      "PyTorch postprocessing took 0.019971847534179688 sec\n",
      "Frame 532 processing time: 0.1015 seconds\n",
      "PyTorch inference took 0.011832237243652344 sec\n",
      "PyTorch postprocessing took 0.01504063606262207 sec\n",
      "Frame 533 processing time: 0.0815 seconds\n",
      "PyTorch inference took 0.012644767761230469 sec\n",
      "PyTorch postprocessing took 0.021040916442871094 sec\n",
      "Frame 534 processing time: 0.0669 seconds\n",
      "PyTorch inference took 0.004199981689453125 sec\n",
      "PyTorch postprocessing took 0.009635448455810547 sec\n",
      "Frame 535 processing time: 0.0294 seconds\n",
      "PyTorch inference took 0.004023313522338867 sec\n",
      "PyTorch postprocessing took 0.008852243423461914 sec\n",
      "Frame 536 processing time: 0.0267 seconds\n",
      "PyTorch inference took 0.0045773983001708984 sec\n",
      "PyTorch postprocessing took 0.014267683029174805 sec\n",
      "Frame 537 processing time: 0.0374 seconds\n",
      "PyTorch inference took 0.004215240478515625 sec\n",
      "PyTorch postprocessing took 0.012420415878295898 sec\n",
      "Frame 538 processing time: 0.0413 seconds\n",
      "PyTorch inference took 0.00981593132019043 sec\n",
      "PyTorch postprocessing took 0.009679317474365234 sec\n",
      "Frame 539 processing time: 0.0506 seconds\n",
      "PyTorch inference took 0.0040740966796875 sec\n",
      "PyTorch postprocessing took 0.015479087829589844 sec\n",
      "Frame 540 processing time: 0.0418 seconds\n",
      "PyTorch inference took 0.004159688949584961 sec\n",
      "PyTorch postprocessing took 0.009866476058959961 sec\n",
      "Frame 541 processing time: 0.0336 seconds\n",
      "PyTorch inference took 0.0044803619384765625 sec\n",
      "PyTorch postprocessing took 0.009285449981689453 sec\n",
      "Frame 542 processing time: 0.0431 seconds\n",
      "PyTorch inference took 0.0038530826568603516 sec\n",
      "PyTorch postprocessing took 0.00918889045715332 sec\n",
      "Frame 543 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.003557920455932617 sec\n",
      "PyTorch postprocessing took 0.008910655975341797 sec\n",
      "Frame 544 processing time: 0.0277 seconds\n",
      "PyTorch inference took 0.003154277801513672 sec\n",
      "PyTorch postprocessing took 0.009859561920166016 sec\n",
      "Frame 545 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.0032682418823242188 sec\n",
      "PyTorch postprocessing took 0.009830474853515625 sec\n",
      "Frame 546 processing time: 0.0333 seconds\n",
      "PyTorch inference took 0.0030510425567626953 sec\n",
      "PyTorch postprocessing took 0.009932518005371094 sec\n",
      "Frame 547 processing time: 0.0368 seconds\n",
      "PyTorch inference took 0.0032548904418945312 sec\n",
      "PyTorch postprocessing took 0.009656190872192383 sec\n",
      "Frame 548 processing time: 0.0295 seconds\n",
      "PyTorch inference took 0.0034024715423583984 sec\n",
      "PyTorch postprocessing took 0.009823083877563477 sec\n",
      "Frame 549 processing time: 0.0361 seconds\n",
      "PyTorch inference took 0.004033088684082031 sec\n",
      "PyTorch postprocessing took 0.00853109359741211 sec\n",
      "Frame 550 processing time: 0.0272 seconds\n",
      "PyTorch inference took 0.004576444625854492 sec\n",
      "PyTorch postprocessing took 0.00935506820678711 sec\n",
      "Frame 551 processing time: 0.0408 seconds\n",
      "PyTorch inference took 0.0074024200439453125 sec\n",
      "PyTorch postprocessing took 0.0056416988372802734 sec\n",
      "Frame 552 processing time: 0.0485 seconds\n",
      "PyTorch inference took 0.003416299819946289 sec\n",
      "PyTorch postprocessing took 0.010603189468383789 sec\n",
      "Frame 553 processing time: 0.0312 seconds\n",
      "PyTorch inference took 0.004121303558349609 sec\n",
      "PyTorch postprocessing took 0.009304046630859375 sec\n",
      "Frame 554 processing time: 0.0275 seconds\n",
      "PyTorch inference took 0.005722522735595703 sec\n",
      "PyTorch postprocessing took 0.007875919342041016 sec\n",
      "Frame 555 processing time: 0.0367 seconds\n",
      "PyTorch inference took 0.0031843185424804688 sec\n",
      "PyTorch postprocessing took 0.009289979934692383 sec\n",
      "Frame 556 processing time: 0.0287 seconds\n",
      "PyTorch inference took 0.0041501522064208984 sec\n",
      "PyTorch postprocessing took 0.009163856506347656 sec\n",
      "Frame 557 processing time: 0.0277 seconds\n",
      "PyTorch inference took 0.0030825138092041016 sec\n",
      "PyTorch postprocessing took 0.009883880615234375 sec\n",
      "Frame 558 processing time: 0.0350 seconds\n",
      "PyTorch inference took 0.003417491912841797 sec\n",
      "PyTorch postprocessing took 0.009679317474365234 sec\n",
      "Frame 559 processing time: 0.0282 seconds\n",
      "PyTorch inference took 0.003206968307495117 sec\n",
      "PyTorch postprocessing took 0.009256362915039062 sec\n",
      "Frame 560 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.0031151771545410156 sec\n",
      "PyTorch postprocessing took 0.010558605194091797 sec\n",
      "Frame 561 processing time: 0.0307 seconds\n",
      "PyTorch inference took 0.003032684326171875 sec\n",
      "PyTorch postprocessing took 0.010127544403076172 sec\n",
      "Frame 562 processing time: 0.0325 seconds\n",
      "PyTorch inference took 0.0033257007598876953 sec\n",
      "PyTorch postprocessing took 0.009497880935668945 sec\n",
      "Frame 563 processing time: 0.0301 seconds\n",
      "PyTorch inference took 0.008487462997436523 sec\n",
      "PyTorch postprocessing took 0.01838970184326172 sec\n",
      "Frame 564 processing time: 0.0628 seconds\n",
      "PyTorch inference took 0.00961756706237793 sec\n",
      "PyTorch postprocessing took 0.01405787467956543 sec\n",
      "Frame 565 processing time: 0.0503 seconds\n",
      "PyTorch inference took 0.003919839859008789 sec\n",
      "PyTorch postprocessing took 0.008644819259643555 sec\n",
      "Frame 566 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.004285097122192383 sec\n",
      "PyTorch postprocessing took 0.008844614028930664 sec\n",
      "Frame 567 processing time: 0.0277 seconds\n",
      "PyTorch inference took 0.003238677978515625 sec\n",
      "PyTorch postprocessing took 0.009542226791381836 sec\n",
      "Frame 568 processing time: 0.0359 seconds\n",
      "PyTorch inference took 0.0041010379791259766 sec\n",
      "PyTorch postprocessing took 0.009947538375854492 sec\n",
      "Frame 569 processing time: 0.0364 seconds\n",
      "PyTorch inference took 0.004172325134277344 sec\n",
      "PyTorch postprocessing took 0.00977945327758789 sec\n",
      "Frame 570 processing time: 0.0322 seconds\n",
      "PyTorch inference took 0.0039098262786865234 sec\n",
      "PyTorch postprocessing took 0.010106801986694336 sec\n",
      "Frame 571 processing time: 0.0364 seconds\n",
      "PyTorch inference took 0.004252910614013672 sec\n",
      "PyTorch postprocessing took 0.011981964111328125 sec\n",
      "Frame 572 processing time: 0.0398 seconds\n",
      "PyTorch inference took 0.024332761764526367 sec\n",
      "PyTorch postprocessing took 0.026093244552612305 sec\n",
      "Frame 573 processing time: 0.0962 seconds\n",
      "PyTorch inference took 0.003089427947998047 sec\n",
      "PyTorch postprocessing took 0.009992122650146484 sec\n",
      "Frame 574 processing time: 0.0357 seconds\n",
      "PyTorch inference took 0.0029430389404296875 sec\n",
      "PyTorch postprocessing took 0.00957632064819336 sec\n",
      "Frame 575 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.004171609878540039 sec\n",
      "PyTorch postprocessing took 0.009490251541137695 sec\n",
      "Frame 576 processing time: 0.0342 seconds\n",
      "PyTorch inference took 0.003173351287841797 sec\n",
      "PyTorch postprocessing took 0.009998559951782227 sec\n",
      "Frame 577 processing time: 0.0284 seconds\n",
      "PyTorch inference took 0.0035867691040039062 sec\n",
      "PyTorch postprocessing took 0.009601116180419922 sec\n",
      "Frame 578 processing time: 0.0334 seconds\n",
      "PyTorch inference took 0.012095451354980469 sec\n",
      "PyTorch postprocessing took 0.02362203598022461 sec\n",
      "Frame 579 processing time: 0.0582 seconds\n",
      "PyTorch inference took 0.0039942264556884766 sec\n",
      "PyTorch postprocessing took 0.009671688079833984 sec\n",
      "Frame 580 processing time: 0.0337 seconds\n",
      "PyTorch inference took 0.004617214202880859 sec\n",
      "PyTorch postprocessing took 0.007916688919067383 sec\n",
      "Frame 581 processing time: 0.0290 seconds\n",
      "PyTorch inference took 0.003047466278076172 sec\n",
      "PyTorch postprocessing took 0.009967803955078125 sec\n",
      "Frame 582 processing time: 0.0348 seconds\n",
      "PyTorch inference took 0.003165006637573242 sec\n",
      "PyTorch postprocessing took 0.009944915771484375 sec\n",
      "Frame 583 processing time: 0.0345 seconds\n",
      "PyTorch inference took 0.006382942199707031 sec\n",
      "PyTorch postprocessing took 0.006682157516479492 sec\n",
      "Frame 584 processing time: 0.0303 seconds\n",
      "PyTorch inference took 0.004159450531005859 sec\n",
      "PyTorch postprocessing took 0.009674787521362305 sec\n",
      "Frame 585 processing time: 0.0349 seconds\n",
      "PyTorch inference took 0.0040514469146728516 sec\n",
      "PyTorch postprocessing took 0.008864641189575195 sec\n",
      "Frame 586 processing time: 0.0275 seconds\n",
      "PyTorch inference took 0.00426793098449707 sec\n",
      "PyTorch postprocessing took 0.009857892990112305 sec\n",
      "Frame 587 processing time: 0.0339 seconds\n",
      "PyTorch inference took 0.003370523452758789 sec\n",
      "PyTorch postprocessing took 0.009756326675415039 sec\n",
      "Frame 588 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.003912687301635742 sec\n",
      "PyTorch postprocessing took 0.014113664627075195 sec\n",
      "Frame 589 processing time: 0.0348 seconds\n",
      "PyTorch inference took 0.00774073600769043 sec\n",
      "PyTorch postprocessing took 0.00892782211303711 sec\n",
      "Frame 590 processing time: 0.0414 seconds\n",
      "PyTorch inference took 0.0038444995880126953 sec\n",
      "PyTorch postprocessing took 0.01552271842956543 sec\n",
      "Frame 591 processing time: 0.0392 seconds\n",
      "PyTorch inference took 0.007116556167602539 sec\n",
      "PyTorch postprocessing took 0.012764930725097656 sec\n",
      "Frame 592 processing time: 0.0628 seconds\n",
      "PyTorch inference took 0.003969430923461914 sec\n",
      "PyTorch postprocessing took 0.014859914779663086 sec\n",
      "Frame 593 processing time: 0.0386 seconds\n",
      "PyTorch inference took 0.006766557693481445 sec\n",
      "PyTorch postprocessing took 0.011253118515014648 sec\n",
      "Frame 594 processing time: 0.0408 seconds\n",
      "PyTorch inference took 0.00442051887512207 sec\n",
      "PyTorch postprocessing took 0.016485929489135742 sec\n",
      "Frame 595 processing time: 0.0387 seconds\n",
      "PyTorch inference took 0.0030438899993896484 sec\n",
      "PyTorch postprocessing took 0.009952306747436523 sec\n",
      "Frame 596 processing time: 0.0306 seconds\n",
      "PyTorch inference took 0.005116701126098633 sec\n",
      "PyTorch postprocessing took 0.008960723876953125 sec\n",
      "Frame 597 processing time: 0.0304 seconds\n",
      "PyTorch inference took 0.003012418746948242 sec\n",
      "PyTorch postprocessing took 0.009964227676391602 sec\n",
      "Frame 598 processing time: 0.0323 seconds\n",
      "PyTorch inference took 0.0031762123107910156 sec\n",
      "PyTorch postprocessing took 0.009779930114746094 sec\n",
      "Frame 599 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.0040318965911865234 sec\n",
      "PyTorch postprocessing took 0.01010894775390625 sec\n",
      "Frame 600 processing time: 0.0357 seconds\n",
      "PyTorch inference took 0.0065343379974365234 sec\n",
      "PyTorch postprocessing took 0.009601831436157227 sec\n",
      "Frame 601 processing time: 0.0540 seconds\n",
      "PyTorch inference took 0.004167795181274414 sec\n",
      "PyTorch postprocessing took 0.010178327560424805 sec\n",
      "Frame 602 processing time: 0.0315 seconds\n",
      "PyTorch inference took 0.008813858032226562 sec\n",
      "PyTorch postprocessing took 0.008620500564575195 sec\n",
      "Frame 603 processing time: 0.0496 seconds\n",
      "PyTorch inference took 0.004274129867553711 sec\n",
      "PyTorch postprocessing took 0.014818191528320312 sec\n",
      "Frame 604 processing time: 0.0360 seconds\n",
      "PyTorch inference took 0.003419160842895508 sec\n",
      "PyTorch postprocessing took 0.009589433670043945 sec\n",
      "Frame 605 processing time: 0.0340 seconds\n",
      "PyTorch inference took 0.0038776397705078125 sec\n",
      "PyTorch postprocessing took 0.013489246368408203 sec\n",
      "Frame 606 processing time: 0.0383 seconds\n",
      "PyTorch inference took 0.0044934749603271484 sec\n",
      "PyTorch postprocessing took 0.009725093841552734 sec\n",
      "Frame 607 processing time: 0.0356 seconds\n",
      "PyTorch inference took 0.006042003631591797 sec\n",
      "PyTorch postprocessing took 0.008430719375610352 sec\n",
      "Frame 608 processing time: 0.0369 seconds\n",
      "PyTorch inference took 0.0029311180114746094 sec\n",
      "PyTorch postprocessing took 0.010076284408569336 sec\n",
      "Frame 609 processing time: 0.0315 seconds\n",
      "PyTorch inference took 0.004061460494995117 sec\n",
      "PyTorch postprocessing took 0.014735221862792969 sec\n",
      "Frame 610 processing time: 0.0388 seconds\n",
      "PyTorch inference took 0.023272275924682617 sec\n",
      "PyTorch postprocessing took 0.017124652862548828 sec\n",
      "Frame 611 processing time: 0.0763 seconds\n",
      "PyTorch inference took 0.008807659149169922 sec\n",
      "PyTorch postprocessing took 0.013509750366210938 sec\n",
      "Frame 612 processing time: 0.0582 seconds\n",
      "PyTorch inference took 0.0037949085235595703 sec\n",
      "PyTorch postprocessing took 0.013490915298461914 sec\n",
      "Frame 613 processing time: 0.0349 seconds\n",
      "PyTorch inference took 0.005884408950805664 sec\n",
      "PyTorch postprocessing took 0.007048368453979492 sec\n",
      "Frame 614 processing time: 0.0353 seconds\n",
      "PyTorch inference took 0.008609771728515625 sec\n",
      "PyTorch postprocessing took 0.014926433563232422 sec\n",
      "Frame 615 processing time: 0.0573 seconds\n",
      "PyTorch inference took 0.0030312538146972656 sec\n",
      "PyTorch postprocessing took 0.009779930114746094 sec\n",
      "Frame 616 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.003104686737060547 sec\n",
      "PyTorch postprocessing took 0.009964942932128906 sec\n",
      "Frame 617 processing time: 0.0265 seconds\n",
      "PyTorch inference took 0.002849578857421875 sec\n",
      "PyTorch postprocessing took 0.010674476623535156 sec\n",
      "Frame 618 processing time: 0.0310 seconds\n",
      "PyTorch inference took 0.004051685333251953 sec\n",
      "PyTorch postprocessing took 0.011086225509643555 sec\n",
      "Frame 619 processing time: 0.0432 seconds\n",
      "PyTorch inference took 0.006402015686035156 sec\n",
      "PyTorch postprocessing took 0.006577968597412109 sec\n",
      "Frame 620 processing time: 0.0410 seconds\n",
      "PyTorch inference took 0.004101991653442383 sec\n",
      "PyTorch postprocessing took 0.008397102355957031 sec\n",
      "Frame 621 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.005927085876464844 sec\n",
      "PyTorch postprocessing took 0.007564067840576172 sec\n",
      "Frame 622 processing time: 0.0428 seconds\n",
      "PyTorch inference took 0.010910272598266602 sec\n",
      "PyTorch postprocessing took 0.008743524551391602 sec\n",
      "Frame 623 processing time: 0.0363 seconds\n",
      "PyTorch inference took 0.009050369262695312 sec\n",
      "PyTorch postprocessing took 0.00927424430847168 sec\n",
      "Frame 624 processing time: 0.0373 seconds\n",
      "PyTorch inference took 0.003916263580322266 sec\n",
      "PyTorch postprocessing took 0.01260685920715332 sec\n",
      "Frame 625 processing time: 0.0418 seconds\n",
      "PyTorch inference took 0.004013538360595703 sec\n",
      "PyTorch postprocessing took 0.010280609130859375 sec\n",
      "Frame 626 processing time: 0.0392 seconds\n",
      "PyTorch inference took 0.005387306213378906 sec\n",
      "PyTorch postprocessing took 0.014507293701171875 sec\n",
      "Frame 627 processing time: 0.0410 seconds\n",
      "PyTorch inference took 0.0035860538482666016 sec\n",
      "PyTorch postprocessing took 0.009712934494018555 sec\n",
      "Frame 628 processing time: 0.0341 seconds\n",
      "PyTorch inference took 0.0032663345336914062 sec\n",
      "PyTorch postprocessing took 0.009692907333374023 sec\n",
      "Frame 629 processing time: 0.0317 seconds\n",
      "PyTorch inference took 0.007761478424072266 sec\n",
      "PyTorch postprocessing took 0.007471323013305664 sec\n",
      "Frame 630 processing time: 0.0386 seconds\n",
      "PyTorch inference took 0.003920316696166992 sec\n",
      "PyTorch postprocessing took 0.013990402221679688 sec\n",
      "Frame 631 processing time: 0.0446 seconds\n",
      "PyTorch inference took 0.003050565719604492 sec\n",
      "PyTorch postprocessing took 0.010202646255493164 sec\n",
      "Frame 632 processing time: 0.0343 seconds\n",
      "PyTorch inference took 0.002966642379760742 sec\n",
      "PyTorch postprocessing took 0.010097265243530273 sec\n",
      "Frame 633 processing time: 0.0310 seconds\n",
      "PyTorch inference took 0.010809659957885742 sec\n",
      "PyTorch postprocessing took 0.009053230285644531 sec\n",
      "Frame 634 processing time: 0.0512 seconds\n",
      "PyTorch inference took 0.003343343734741211 sec\n",
      "PyTorch postprocessing took 0.009925127029418945 sec\n",
      "Frame 635 processing time: 0.0350 seconds\n",
      "PyTorch inference took 0.0030291080474853516 sec\n",
      "PyTorch postprocessing took 0.010788440704345703 sec\n",
      "Frame 636 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.010656595230102539 sec\n",
      "PyTorch postprocessing took 0.010741233825683594 sec\n",
      "Frame 637 processing time: 0.0476 seconds\n",
      "PyTorch inference took 0.0030825138092041016 sec\n",
      "PyTorch postprocessing took 0.009881973266601562 sec\n",
      "Frame 638 processing time: 0.0292 seconds\n",
      "PyTorch inference took 0.0029306411743164062 sec\n",
      "PyTorch postprocessing took 0.009856462478637695 sec\n",
      "Frame 639 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.005490541458129883 sec\n",
      "PyTorch postprocessing took 0.009646892547607422 sec\n",
      "Frame 640 processing time: 0.0421 seconds\n",
      "PyTorch inference took 0.003945112228393555 sec\n",
      "PyTorch postprocessing took 0.014390945434570312 sec\n",
      "Frame 641 processing time: 0.0318 seconds\n",
      "PyTorch inference took 0.003743410110473633 sec\n",
      "PyTorch postprocessing took 0.010062456130981445 sec\n",
      "Frame 642 processing time: 0.0327 seconds\n",
      "PyTorch inference took 0.012583017349243164 sec\n",
      "PyTorch postprocessing took 0.014956235885620117 sec\n",
      "Frame 643 processing time: 0.0609 seconds\n",
      "PyTorch inference took 0.0031065940856933594 sec\n",
      "PyTorch postprocessing took 0.009805679321289062 sec\n",
      "Frame 644 processing time: 0.0269 seconds\n",
      "PyTorch inference took 0.0029485225677490234 sec\n",
      "PyTorch postprocessing took 0.01016688346862793 sec\n",
      "Frame 645 processing time: 0.0310 seconds\n",
      "PyTorch inference took 0.0032265186309814453 sec\n",
      "PyTorch postprocessing took 0.009902477264404297 sec\n",
      "Frame 646 processing time: 0.0275 seconds\n",
      "PyTorch inference took 0.007040739059448242 sec\n",
      "PyTorch postprocessing took 0.007246255874633789 sec\n",
      "Frame 647 processing time: 0.0313 seconds\n",
      "PyTorch inference took 0.003760814666748047 sec\n",
      "PyTorch postprocessing took 0.009278297424316406 sec\n",
      "Frame 648 processing time: 0.0317 seconds\n",
      "PyTorch inference took 0.00460052490234375 sec\n",
      "PyTorch postprocessing took 0.014172077178955078 sec\n",
      "Frame 649 processing time: 0.0378 seconds\n",
      "PyTorch inference took 0.003204822540283203 sec\n",
      "PyTorch postprocessing took 0.009884357452392578 sec\n",
      "Frame 650 processing time: 0.0296 seconds\n",
      "PyTorch inference took 0.00665593147277832 sec\n",
      "PyTorch postprocessing took 0.006403684616088867 sec\n",
      "Frame 651 processing time: 0.0267 seconds\n",
      "PyTorch inference took 0.003007173538208008 sec\n",
      "PyTorch postprocessing took 0.009891271591186523 sec\n",
      "Frame 652 processing time: 0.0298 seconds\n",
      "PyTorch inference took 0.003983020782470703 sec\n",
      "PyTorch postprocessing took 0.008857488632202148 sec\n",
      "Frame 653 processing time: 0.0370 seconds\n",
      "PyTorch inference took 0.009281635284423828 sec\n",
      "PyTorch postprocessing took 0.007975339889526367 sec\n",
      "Frame 654 processing time: 0.0560 seconds\n",
      "PyTorch inference took 0.02389979362487793 sec\n",
      "PyTorch postprocessing took 0.03163933753967285 sec\n",
      "Frame 655 processing time: 0.0981 seconds\n",
      "PyTorch inference took 0.00417780876159668 sec\n",
      "PyTorch postprocessing took 0.015902996063232422 sec\n",
      "Frame 656 processing time: 0.0411 seconds\n",
      "PyTorch inference took 0.004172325134277344 sec\n",
      "PyTorch postprocessing took 0.014931201934814453 sec\n",
      "Frame 657 processing time: 0.0428 seconds\n",
      "PyTorch inference took 0.004512786865234375 sec\n",
      "PyTorch postprocessing took 0.015518665313720703 sec\n",
      "Frame 658 processing time: 0.0411 seconds\n",
      "PyTorch inference took 0.006326198577880859 sec\n",
      "PyTorch postprocessing took 0.015433073043823242 sec\n",
      "Frame 659 processing time: 0.0505 seconds\n",
      "PyTorch inference took 0.005921602249145508 sec\n",
      "PyTorch postprocessing took 0.022523164749145508 sec\n",
      "Frame 660 processing time: 0.0632 seconds\n",
      "PyTorch inference took 0.004728555679321289 sec\n",
      "PyTorch postprocessing took 0.008249998092651367 sec\n",
      "Frame 661 processing time: 0.0279 seconds\n",
      "PyTorch inference took 0.00541234016418457 sec\n",
      "PyTorch postprocessing took 0.008582592010498047 sec\n",
      "Frame 662 processing time: 0.0453 seconds\n",
      "PyTorch inference took 0.010470390319824219 sec\n",
      "PyTorch postprocessing took 0.010584592819213867 sec\n",
      "Frame 663 processing time: 0.0576 seconds\n",
      "PyTorch inference took 0.010397195816040039 sec\n",
      "PyTorch postprocessing took 0.009895563125610352 sec\n",
      "Frame 664 processing time: 0.0505 seconds\n",
      "PyTorch inference took 0.009069442749023438 sec\n",
      "PyTorch postprocessing took 0.012727022171020508 sec\n",
      "Frame 665 processing time: 0.0640 seconds\n",
      "PyTorch inference took 0.003983259201049805 sec\n",
      "PyTorch postprocessing took 0.012215614318847656 sec\n",
      "Frame 666 processing time: 0.0335 seconds\n",
      "PyTorch inference took 0.022510528564453125 sec\n",
      "PyTorch postprocessing took 0.017386198043823242 sec\n",
      "Frame 667 processing time: 0.0608 seconds\n",
      "PyTorch inference took 0.01154184341430664 sec\n",
      "PyTorch postprocessing took 0.008835554122924805 sec\n",
      "Frame 668 processing time: 0.0378 seconds\n",
      "PyTorch inference took 0.002998828887939453 sec\n",
      "PyTorch postprocessing took 0.00998687744140625 sec\n",
      "Frame 669 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.0037682056427001953 sec\n",
      "PyTorch postprocessing took 0.010168313980102539 sec\n",
      "Frame 670 processing time: 0.0320 seconds\n",
      "PyTorch inference took 0.0037755966186523438 sec\n",
      "PyTorch postprocessing took 0.009392023086547852 sec\n",
      "Frame 671 processing time: 0.0289 seconds\n",
      "PyTorch inference took 0.0030379295349121094 sec\n",
      "PyTorch postprocessing took 0.009938240051269531 sec\n",
      "Frame 672 processing time: 0.0346 seconds\n",
      "PyTorch inference took 0.009256839752197266 sec\n",
      "PyTorch postprocessing took 0.014705181121826172 sec\n",
      "Frame 673 processing time: 0.0563 seconds\n",
      "PyTorch inference took 0.0037488937377929688 sec\n",
      "PyTorch postprocessing took 0.015992164611816406 sec\n",
      "Frame 674 processing time: 0.0422 seconds\n",
      "PyTorch inference took 0.004106044769287109 sec\n",
      "PyTorch postprocessing took 0.009437084197998047 sec\n",
      "Frame 675 processing time: 0.0340 seconds\n",
      "PyTorch inference took 0.007437944412231445 sec\n",
      "PyTorch postprocessing took 0.005577802658081055 sec\n",
      "Frame 676 processing time: 0.0455 seconds\n",
      "PyTorch inference took 0.010516166687011719 sec\n",
      "PyTorch postprocessing took 0.019860029220581055 sec\n",
      "Frame 677 processing time: 0.0664 seconds\n",
      "PyTorch inference took 0.008964300155639648 sec\n",
      "PyTorch postprocessing took 0.009122133255004883 sec\n",
      "Frame 678 processing time: 0.0356 seconds\n",
      "PyTorch inference took 0.003538370132446289 sec\n",
      "PyTorch postprocessing took 0.009727716445922852 sec\n",
      "Frame 679 processing time: 0.0286 seconds\n",
      "PyTorch inference took 0.009152889251708984 sec\n",
      "PyTorch postprocessing took 0.010646343231201172 sec\n",
      "Frame 680 processing time: 0.0445 seconds\n",
      "PyTorch inference took 0.004149675369262695 sec\n",
      "PyTorch postprocessing took 0.008854866027832031 sec\n",
      "Frame 681 processing time: 0.0297 seconds\n",
      "PyTorch inference took 0.004389047622680664 sec\n",
      "PyTorch postprocessing took 0.010234355926513672 sec\n",
      "Frame 682 processing time: 0.0390 seconds\n",
      "PyTorch inference took 0.003793954849243164 sec\n",
      "PyTorch postprocessing took 0.015346288681030273 sec\n",
      "Frame 683 processing time: 0.0386 seconds\n",
      "PyTorch inference took 0.010853052139282227 sec\n",
      "PyTorch postprocessing took 0.017016172409057617 sec\n",
      "Frame 684 processing time: 0.0507 seconds\n",
      "PyTorch inference took 0.0072634220123291016 sec\n",
      "PyTorch postprocessing took 0.006573200225830078 sec\n",
      "Frame 685 processing time: 0.0498 seconds\n",
      "PyTorch inference took 0.010775089263916016 sec\n",
      "PyTorch postprocessing took 0.009968280792236328 sec\n",
      "Frame 686 processing time: 0.0468 seconds\n",
      "PyTorch inference took 0.007746219635009766 sec\n",
      "PyTorch postprocessing took 0.005609989166259766 sec\n",
      "Frame 687 processing time: 0.0473 seconds\n",
      "PyTorch inference took 0.003941774368286133 sec\n",
      "PyTorch postprocessing took 0.012293815612792969 sec\n",
      "Frame 688 processing time: 0.0343 seconds\n",
      "PyTorch inference took 0.008927583694458008 sec\n",
      "PyTorch postprocessing took 0.009885072708129883 sec\n",
      "Frame 689 processing time: 0.0383 seconds\n",
      "PyTorch inference took 0.008799076080322266 sec\n",
      "PyTorch postprocessing took 0.00989842414855957 sec\n",
      "Frame 690 processing time: 0.0481 seconds\n",
      "PyTorch inference took 0.00731968879699707 sec\n",
      "PyTorch postprocessing took 0.012073993682861328 sec\n",
      "Frame 691 processing time: 0.0423 seconds\n",
      "PyTorch inference took 0.004190683364868164 sec\n",
      "PyTorch postprocessing took 0.00925755500793457 sec\n",
      "Frame 692 processing time: 0.0330 seconds\n",
      "PyTorch inference took 0.004811525344848633 sec\n",
      "PyTorch postprocessing took 0.012709617614746094 sec\n",
      "Frame 693 processing time: 0.0322 seconds\n",
      "PyTorch inference took 0.0069539546966552734 sec\n",
      "PyTorch postprocessing took 0.007027387619018555 sec\n",
      "Frame 694 processing time: 0.0361 seconds\n",
      "PyTorch inference took 0.0038895606994628906 sec\n",
      "PyTorch postprocessing took 0.009442329406738281 sec\n",
      "Frame 695 processing time: 0.0313 seconds\n",
      "PyTorch inference took 0.003993034362792969 sec\n",
      "PyTorch postprocessing took 0.009921789169311523 sec\n",
      "Frame 696 processing time: 0.0357 seconds\n",
      "PyTorch inference took 0.008527040481567383 sec\n",
      "PyTorch postprocessing took 0.006569862365722656 sec\n",
      "Frame 697 processing time: 0.0444 seconds\n",
      "PyTorch inference took 0.00405120849609375 sec\n",
      "PyTorch postprocessing took 0.009848594665527344 sec\n",
      "Frame 698 processing time: 0.0288 seconds\n",
      "PyTorch inference took 0.004004478454589844 sec\n",
      "PyTorch postprocessing took 0.010015249252319336 sec\n",
      "Frame 699 processing time: 0.0314 seconds\n",
      "PyTorch inference took 0.006846904754638672 sec\n",
      "PyTorch postprocessing took 0.011287689208984375 sec\n",
      "Frame 700 processing time: 0.0415 seconds\n",
      "PyTorch inference took 0.004202365875244141 sec\n",
      "PyTorch postprocessing took 0.013325691223144531 sec\n",
      "Frame 701 processing time: 0.0325 seconds\n",
      "PyTorch inference took 0.003721952438354492 sec\n",
      "PyTorch postprocessing took 0.010198593139648438 sec\n",
      "Frame 702 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.0037658214569091797 sec\n",
      "PyTorch postprocessing took 0.011416435241699219 sec\n",
      "Frame 703 processing time: 0.0317 seconds\n",
      "PyTorch inference took 0.0042569637298583984 sec\n",
      "PyTorch postprocessing took 0.009603500366210938 sec\n",
      "Frame 704 processing time: 0.0330 seconds\n",
      "PyTorch inference took 0.003788471221923828 sec\n",
      "PyTorch postprocessing took 0.014448404312133789 sec\n",
      "Frame 705 processing time: 0.0327 seconds\n",
      "PyTorch inference took 0.007761240005493164 sec\n",
      "PyTorch postprocessing took 0.0072209835052490234 sec\n",
      "Frame 706 processing time: 0.0426 seconds\n",
      "PyTorch inference took 0.003919124603271484 sec\n",
      "PyTorch postprocessing took 0.014770269393920898 sec\n",
      "Frame 707 processing time: 0.0364 seconds\n",
      "PyTorch inference took 0.006422519683837891 sec\n",
      "PyTorch postprocessing took 0.01256418228149414 sec\n",
      "Frame 708 processing time: 0.0478 seconds\n",
      "PyTorch inference took 0.02282238006591797 sec\n",
      "PyTorch postprocessing took 0.019257068634033203 sec\n",
      "Frame 709 processing time: 0.0598 seconds\n",
      "PyTorch inference took 0.014755010604858398 sec\n",
      "PyTorch postprocessing took 0.006562232971191406 sec\n",
      "Frame 710 processing time: 0.0408 seconds\n",
      "PyTorch inference took 0.004421710968017578 sec\n",
      "PyTorch postprocessing took 0.009905576705932617 sec\n",
      "Frame 711 processing time: 0.0309 seconds\n",
      "PyTorch inference took 0.009709596633911133 sec\n",
      "PyTorch postprocessing took 0.009369611740112305 sec\n",
      "Frame 712 processing time: 0.0394 seconds\n",
      "PyTorch inference took 0.003355264663696289 sec\n",
      "PyTorch postprocessing took 0.010195016860961914 sec\n",
      "Frame 713 processing time: 0.0291 seconds\n",
      "PyTorch inference took 0.003067493438720703 sec\n",
      "PyTorch postprocessing took 0.010063648223876953 sec\n",
      "Frame 714 processing time: 0.0313 seconds\n",
      "PyTorch inference took 0.003207683563232422 sec\n",
      "PyTorch postprocessing took 0.009879350662231445 sec\n",
      "Frame 715 processing time: 0.0284 seconds\n",
      "PyTorch inference took 0.00614619255065918 sec\n",
      "PyTorch postprocessing took 0.011306047439575195 sec\n",
      "Frame 716 processing time: 0.0492 seconds\n",
      "PyTorch inference took 0.003794431686401367 sec\n",
      "PyTorch postprocessing took 0.009602785110473633 sec\n",
      "Frame 717 processing time: 0.0322 seconds\n",
      "PyTorch inference took 0.002984285354614258 sec\n",
      "PyTorch postprocessing took 0.009895801544189453 sec\n",
      "Frame 718 processing time: 0.0290 seconds\n",
      "PyTorch inference took 0.004348278045654297 sec\n",
      "PyTorch postprocessing took 0.009842157363891602 sec\n",
      "Frame 719 processing time: 0.0319 seconds\n",
      "PyTorch inference took 0.0031218528747558594 sec\n",
      "PyTorch postprocessing took 0.009885549545288086 sec\n",
      "Frame 720 processing time: 0.0382 seconds\n",
      "PyTorch inference took 0.0034372806549072266 sec\n",
      "PyTorch postprocessing took 0.009490728378295898 sec\n",
      "Frame 721 processing time: 0.0301 seconds\n",
      "PyTorch inference took 0.004871845245361328 sec\n",
      "PyTorch postprocessing took 0.008324384689331055 sec\n",
      "Frame 722 processing time: 0.0412 seconds\n",
      "PyTorch inference took 0.004387378692626953 sec\n",
      "PyTorch postprocessing took 0.013046503067016602 sec\n",
      "Frame 723 processing time: 0.0433 seconds\n",
      "PyTorch inference took 0.007085561752319336 sec\n",
      "PyTorch postprocessing took 0.010988950729370117 sec\n",
      "Frame 724 processing time: 0.0540 seconds\n",
      "PyTorch inference took 0.004300594329833984 sec\n",
      "PyTorch postprocessing took 0.011452674865722656 sec\n",
      "Frame 725 processing time: 0.0400 seconds\n",
      "PyTorch inference took 0.006185054779052734 sec\n",
      "PyTorch postprocessing took 0.013247013092041016 sec\n",
      "Frame 726 processing time: 0.0502 seconds\n",
      "PyTorch inference took 0.008960962295532227 sec\n",
      "PyTorch postprocessing took 0.011522769927978516 sec\n",
      "Frame 727 processing time: 0.0621 seconds\n",
      "PyTorch inference took 0.004792451858520508 sec\n",
      "PyTorch postprocessing took 0.009344339370727539 sec\n",
      "Frame 728 processing time: 0.0324 seconds\n",
      "PyTorch inference took 0.009031057357788086 sec\n",
      "PyTorch postprocessing took 0.023401737213134766 sec\n",
      "Frame 729 processing time: 0.0630 seconds\n",
      "PyTorch inference took 0.00404047966003418 sec\n",
      "PyTorch postprocessing took 0.009065866470336914 sec\n",
      "Frame 730 processing time: 0.0330 seconds\n",
      "PyTorch inference took 0.003964662551879883 sec\n",
      "PyTorch postprocessing took 0.010202646255493164 sec\n",
      "Frame 731 processing time: 0.0351 seconds\n",
      "PyTorch inference took 0.0069010257720947266 sec\n",
      "PyTorch postprocessing took 0.008505105972290039 sec\n",
      "Frame 732 processing time: 0.0411 seconds\n",
      "PyTorch inference took 0.013643264770507812 sec\n",
      "PyTorch postprocessing took 0.011255979537963867 sec\n",
      "Frame 733 processing time: 0.0680 seconds\n",
      "PyTorch inference took 0.004153013229370117 sec\n",
      "PyTorch postprocessing took 0.009682893753051758 sec\n",
      "Frame 734 processing time: 0.0346 seconds\n",
      "PyTorch inference took 0.007127285003662109 sec\n",
      "PyTorch postprocessing took 0.013721704483032227 sec\n",
      "Frame 735 processing time: 0.0365 seconds\n",
      "PyTorch inference took 0.0065555572509765625 sec\n",
      "PyTorch postprocessing took 0.017736434936523438 sec\n",
      "Frame 736 processing time: 0.0579 seconds\n",
      "PyTorch inference took 0.006516218185424805 sec\n",
      "PyTorch postprocessing took 0.010936737060546875 sec\n",
      "Frame 737 processing time: 0.0558 seconds\n",
      "PyTorch inference took 0.003816366195678711 sec\n",
      "PyTorch postprocessing took 0.009918212890625 sec\n",
      "Frame 738 processing time: 0.0283 seconds\n",
      "PyTorch inference took 0.004264116287231445 sec\n",
      "PyTorch postprocessing took 0.009581327438354492 sec\n",
      "Frame 739 processing time: 0.0315 seconds\n",
      "PyTorch inference took 0.004465818405151367 sec\n",
      "PyTorch postprocessing took 0.008849382400512695 sec\n",
      "Frame 740 processing time: 0.0290 seconds\n",
      "PyTorch inference took 0.003745555877685547 sec\n",
      "PyTorch postprocessing took 0.009558677673339844 sec\n",
      "Frame 741 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.002950906753540039 sec\n",
      "PyTorch postprocessing took 0.011415481567382812 sec\n",
      "Frame 742 processing time: 0.0350 seconds\n",
      "PyTorch inference took 0.008675098419189453 sec\n",
      "PyTorch postprocessing took 0.010167598724365234 sec\n",
      "Frame 743 processing time: 0.0593 seconds\n",
      "PyTorch inference took 0.004791975021362305 sec\n",
      "PyTorch postprocessing took 0.009550333023071289 sec\n",
      "Frame 744 processing time: 0.0312 seconds\n",
      "PyTorch inference took 0.006944417953491211 sec\n",
      "PyTorch postprocessing took 0.027927160263061523 sec\n",
      "Frame 745 processing time: 0.0653 seconds\n",
      "PyTorch inference took 0.004113197326660156 sec\n",
      "PyTorch postprocessing took 0.010929107666015625 sec\n",
      "Frame 746 processing time: 0.0392 seconds\n",
      "PyTorch inference took 0.006991863250732422 sec\n",
      "PyTorch postprocessing took 0.010853290557861328 sec\n",
      "Frame 747 processing time: 0.0541 seconds\n",
      "PyTorch inference took 0.0031266212463378906 sec\n",
      "PyTorch postprocessing took 0.009821176528930664 sec\n",
      "Frame 748 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.004705667495727539 sec\n",
      "PyTorch postprocessing took 0.010733604431152344 sec\n",
      "Frame 749 processing time: 0.0488 seconds\n",
      "PyTorch inference took 0.002956390380859375 sec\n",
      "PyTorch postprocessing took 0.010088205337524414 sec\n",
      "Frame 750 processing time: 0.0278 seconds\n",
      "PyTorch inference took 0.0030803680419921875 sec\n",
      "PyTorch postprocessing took 0.010106086730957031 sec\n",
      "Frame 751 processing time: 0.0313 seconds\n",
      "PyTorch inference took 0.006404876708984375 sec\n",
      "PyTorch postprocessing took 0.01324772834777832 sec\n",
      "Frame 752 processing time: 0.0404 seconds\n",
      "PyTorch inference took 0.0071811676025390625 sec\n",
      "PyTorch postprocessing took 0.012542724609375 sec\n",
      "Frame 753 processing time: 0.0611 seconds\n",
      "PyTorch inference took 0.00757288932800293 sec\n",
      "PyTorch postprocessing took 0.008738279342651367 sec\n",
      "Frame 754 processing time: 0.0511 seconds\n",
      "PyTorch inference took 0.008763313293457031 sec\n",
      "PyTorch postprocessing took 0.008715629577636719 sec\n",
      "Frame 755 processing time: 0.0374 seconds\n",
      "PyTorch inference took 0.014349699020385742 sec\n",
      "PyTorch postprocessing took 0.011449813842773438 sec\n",
      "Frame 756 processing time: 0.0542 seconds\n",
      "PyTorch inference took 0.012135028839111328 sec\n",
      "PyTorch postprocessing took 0.01130986213684082 sec\n",
      "Frame 757 processing time: 0.0466 seconds\n",
      "PyTorch inference took 0.008218765258789062 sec\n",
      "PyTorch postprocessing took 0.010231971740722656 sec\n",
      "Frame 758 processing time: 0.0600 seconds\n",
      "PyTorch inference took 0.006676673889160156 sec\n",
      "PyTorch postprocessing took 0.009818792343139648 sec\n",
      "Frame 759 processing time: 0.0360 seconds\n",
      "PyTorch inference took 0.007321357727050781 sec\n",
      "PyTorch postprocessing took 0.010689973831176758 sec\n",
      "Frame 760 processing time: 0.0378 seconds\n",
      "PyTorch inference took 0.007488727569580078 sec\n",
      "PyTorch postprocessing took 0.01419210433959961 sec\n",
      "Frame 761 processing time: 0.0594 seconds\n",
      "PyTorch inference took 0.00420379638671875 sec\n",
      "PyTorch postprocessing took 0.009982109069824219 sec\n",
      "Frame 762 processing time: 0.0320 seconds\n",
      "PyTorch inference took 0.004100799560546875 sec\n",
      "PyTorch postprocessing took 0.01454019546508789 sec\n",
      "Frame 763 processing time: 0.0453 seconds\n",
      "PyTorch inference took 0.004294633865356445 sec\n",
      "PyTorch postprocessing took 0.009000778198242188 sec\n",
      "Frame 764 processing time: 0.0310 seconds\n",
      "PyTorch inference took 0.004123210906982422 sec\n",
      "PyTorch postprocessing took 0.010020732879638672 sec\n",
      "Frame 765 processing time: 0.0389 seconds\n",
      "PyTorch inference took 0.004064798355102539 sec\n",
      "PyTorch postprocessing took 0.011630773544311523 sec\n",
      "Frame 766 processing time: 0.0348 seconds\n",
      "PyTorch inference took 0.003290891647338867 sec\n",
      "PyTorch postprocessing took 0.015554666519165039 sec\n",
      "Frame 767 processing time: 0.0449 seconds\n",
      "PyTorch inference took 0.005301952362060547 sec\n",
      "PyTorch postprocessing took 0.007682323455810547 sec\n",
      "Frame 768 processing time: 0.0318 seconds\n",
      "PyTorch inference took 0.004403114318847656 sec\n",
      "PyTorch postprocessing took 0.014060497283935547 sec\n",
      "Frame 769 processing time: 0.0357 seconds\n",
      "PyTorch inference took 0.004373788833618164 sec\n",
      "PyTorch postprocessing took 0.00960087776184082 sec\n",
      "Frame 770 processing time: 0.0318 seconds\n",
      "PyTorch inference took 0.012198209762573242 sec\n",
      "PyTorch postprocessing took 0.009189844131469727 sec\n",
      "Frame 771 processing time: 0.0514 seconds\n",
      "PyTorch inference took 0.003687620162963867 sec\n",
      "PyTorch postprocessing took 0.009717941284179688 sec\n",
      "Frame 772 processing time: 0.0326 seconds\n",
      "PyTorch inference took 0.007172822952270508 sec\n",
      "PyTorch postprocessing took 0.009474039077758789 sec\n",
      "Frame 773 processing time: 0.0442 seconds\n",
      "PyTorch inference took 0.0037353038787841797 sec\n",
      "PyTorch postprocessing took 0.009989500045776367 sec\n",
      "Frame 774 processing time: 0.0266 seconds\n",
      "PyTorch inference took 0.0031194686889648438 sec\n",
      "PyTorch postprocessing took 0.00993967056274414 sec\n",
      "Frame 775 processing time: 0.0329 seconds\n",
      "PyTorch inference took 0.008966922760009766 sec\n",
      "PyTorch postprocessing took 0.011368274688720703 sec\n",
      "Frame 776 processing time: 0.0415 seconds\n",
      "PyTorch inference took 0.004523754119873047 sec\n",
      "PyTorch postprocessing took 0.00996541976928711 sec\n",
      "Frame 777 processing time: 0.0369 seconds\n",
      "PyTorch inference took 0.002938985824584961 sec\n",
      "PyTorch postprocessing took 0.010086536407470703 sec\n",
      "Frame 778 processing time: 0.0272 seconds\n",
      "PyTorch inference took 0.002943277359008789 sec\n",
      "PyTorch postprocessing took 0.010035037994384766 sec\n",
      "Frame 779 processing time: 0.0277 seconds\n",
      "PyTorch inference took 0.0030155181884765625 sec\n",
      "PyTorch postprocessing took 0.010106325149536133 sec\n",
      "Frame 780 processing time: 0.0302 seconds\n",
      "PyTorch inference took 0.0029022693634033203 sec\n",
      "PyTorch postprocessing took 0.01027822494506836 sec\n",
      "Frame 781 processing time: 0.0306 seconds\n",
      "PyTorch inference took 0.004745006561279297 sec\n",
      "PyTorch postprocessing took 0.008162736892700195 sec\n",
      "Frame 782 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.0069196224212646484 sec\n",
      "PyTorch postprocessing took 0.02613091468811035 sec\n",
      "Frame 783 processing time: 0.0601 seconds\n",
      "PyTorch inference took 0.003908395767211914 sec\n",
      "PyTorch postprocessing took 0.014858484268188477 sec\n",
      "Frame 784 processing time: 0.0464 seconds\n",
      "PyTorch inference took 0.004202604293823242 sec\n",
      "PyTorch postprocessing took 0.008836030960083008 sec\n",
      "Frame 785 processing time: 0.0331 seconds\n",
      "PyTorch inference took 0.0034220218658447266 sec\n",
      "PyTorch postprocessing took 0.01108098030090332 sec\n",
      "Frame 786 processing time: 0.0391 seconds\n",
      "PyTorch inference took 0.009045600891113281 sec\n",
      "PyTorch postprocessing took 0.008934259414672852 sec\n",
      "Frame 787 processing time: 0.0421 seconds\n",
      "PyTorch inference took 0.0032796859741210938 sec\n",
      "PyTorch postprocessing took 0.014255523681640625 sec\n",
      "Frame 788 processing time: 0.0388 seconds\n",
      "PyTorch inference took 0.0029506683349609375 sec\n",
      "PyTorch postprocessing took 0.010317564010620117 sec\n",
      "Frame 789 processing time: 0.0299 seconds\n",
      "PyTorch inference took 0.007140159606933594 sec\n",
      "PyTorch postprocessing took 0.008899688720703125 sec\n",
      "Frame 790 processing time: 0.0507 seconds\n",
      "PyTorch inference took 0.00374603271484375 sec\n",
      "PyTorch postprocessing took 0.010315418243408203 sec\n",
      "Frame 791 processing time: 0.0290 seconds\n",
      "PyTorch inference took 0.003050088882446289 sec\n",
      "PyTorch postprocessing took 0.009975671768188477 sec\n",
      "Frame 792 processing time: 0.0278 seconds\n",
      "PyTorch inference took 0.008290767669677734 sec\n",
      "PyTorch postprocessing took 0.022464752197265625 sec\n",
      "Frame 793 processing time: 0.0638 seconds\n",
      "PyTorch inference took 0.0038056373596191406 sec\n",
      "PyTorch postprocessing took 0.015273571014404297 sec\n",
      "Frame 794 processing time: 0.0392 seconds\n",
      "PyTorch inference took 0.003340482711791992 sec\n",
      "PyTorch postprocessing took 0.009613752365112305 sec\n",
      "Frame 795 processing time: 0.0303 seconds\n",
      "PyTorch inference took 0.009208917617797852 sec\n",
      "PyTorch postprocessing took 0.007989645004272461 sec\n",
      "Frame 796 processing time: 0.0475 seconds\n",
      "PyTorch inference took 0.008921146392822266 sec\n",
      "PyTorch postprocessing took 0.01455068588256836 sec\n",
      "Frame 797 processing time: 0.0590 seconds\n",
      "PyTorch inference took 0.003475666046142578 sec\n",
      "PyTorch postprocessing took 0.009873151779174805 sec\n",
      "Frame 798 processing time: 0.0322 seconds\n",
      "PyTorch inference took 0.0030715465545654297 sec\n",
      "PyTorch postprocessing took 0.009635448455810547 sec\n",
      "Frame 799 processing time: 0.0321 seconds\n",
      "PyTorch inference took 0.0029256343841552734 sec\n",
      "PyTorch postprocessing took 0.010030031204223633 sec\n",
      "Frame 800 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.002934694290161133 sec\n",
      "PyTorch postprocessing took 0.009930849075317383 sec\n",
      "Frame 801 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.002954244613647461 sec\n",
      "PyTorch postprocessing took 0.010091304779052734 sec\n",
      "Frame 802 processing time: 0.0256 seconds\n",
      "PyTorch inference took 0.003756999969482422 sec\n",
      "PyTorch postprocessing took 0.010237693786621094 sec\n",
      "Frame 803 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.005840778350830078 sec\n",
      "PyTorch postprocessing took 0.006910085678100586 sec\n",
      "Frame 804 processing time: 0.0278 seconds\n",
      "PyTorch inference took 0.008769035339355469 sec\n",
      "PyTorch postprocessing took 0.011740446090698242 sec\n",
      "Frame 805 processing time: 0.0469 seconds\n",
      "PyTorch inference took 0.0056743621826171875 sec\n",
      "PyTorch postprocessing took 0.015600204467773438 sec\n",
      "Frame 806 processing time: 0.0459 seconds\n",
      "PyTorch inference took 0.008274316787719727 sec\n",
      "PyTorch postprocessing took 0.011188507080078125 sec\n",
      "Frame 807 processing time: 0.0421 seconds\n",
      "PyTorch inference took 0.0062122344970703125 sec\n",
      "PyTorch postprocessing took 0.006980419158935547 sec\n",
      "Frame 808 processing time: 0.0363 seconds\n",
      "PyTorch inference took 0.0028922557830810547 sec\n",
      "PyTorch postprocessing took 0.01023244857788086 sec\n",
      "Frame 809 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.002941131591796875 sec\n",
      "PyTorch postprocessing took 0.010195255279541016 sec\n",
      "Frame 810 processing time: 0.0286 seconds\n",
      "PyTorch inference took 0.005304813385009766 sec\n",
      "PyTorch postprocessing took 0.00776362419128418 sec\n",
      "Frame 811 processing time: 0.0359 seconds\n",
      "PyTorch inference took 0.003393411636352539 sec\n",
      "PyTorch postprocessing took 0.009662628173828125 sec\n",
      "Frame 812 processing time: 0.0311 seconds\n",
      "PyTorch inference took 0.00293731689453125 sec\n",
      "PyTorch postprocessing took 0.010344743728637695 sec\n",
      "Frame 813 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.0035033226013183594 sec\n",
      "PyTorch postprocessing took 0.010152578353881836 sec\n",
      "Frame 814 processing time: 0.0266 seconds\n",
      "PyTorch inference took 0.003005504608154297 sec\n",
      "PyTorch postprocessing took 0.010120153427124023 sec\n",
      "Frame 815 processing time: 0.0281 seconds\n",
      "PyTorch inference took 0.0029506683349609375 sec\n",
      "PyTorch postprocessing took 0.010081768035888672 sec\n",
      "Frame 816 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.0050182342529296875 sec\n",
      "PyTorch postprocessing took 0.007950782775878906 sec\n",
      "Frame 817 processing time: 0.0318 seconds\n",
      "PyTorch inference took 0.0031092166900634766 sec\n",
      "PyTorch postprocessing took 0.009560108184814453 sec\n",
      "Frame 818 processing time: 0.0322 seconds\n",
      "PyTorch inference took 0.00995016098022461 sec\n",
      "PyTorch postprocessing took 0.009413480758666992 sec\n",
      "Frame 819 processing time: 0.0479 seconds\n",
      "PyTorch inference took 0.003180265426635742 sec\n",
      "PyTorch postprocessing took 0.009856700897216797 sec\n",
      "Frame 820 processing time: 0.0279 seconds\n",
      "PyTorch inference took 0.003013134002685547 sec\n",
      "PyTorch postprocessing took 0.01068878173828125 sec\n",
      "Frame 821 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.006047725677490234 sec\n",
      "PyTorch postprocessing took 0.010677099227905273 sec\n",
      "Frame 822 processing time: 0.0509 seconds\n",
      "PyTorch inference took 0.004060983657836914 sec\n",
      "PyTorch postprocessing took 0.009630203247070312 sec\n",
      "Frame 823 processing time: 0.0351 seconds\n",
      "PyTorch inference took 0.0038976669311523438 sec\n",
      "PyTorch postprocessing took 0.010144710540771484 sec\n",
      "Frame 824 processing time: 0.0304 seconds\n",
      "PyTorch inference took 0.00407862663269043 sec\n",
      "PyTorch postprocessing took 0.014678239822387695 sec\n",
      "Frame 825 processing time: 0.0371 seconds\n",
      "PyTorch inference took 0.004461050033569336 sec\n",
      "PyTorch postprocessing took 0.00919485092163086 sec\n",
      "Frame 826 processing time: 0.0312 seconds\n",
      "PyTorch inference took 0.0031423568725585938 sec\n",
      "PyTorch postprocessing took 0.015564918518066406 sec\n",
      "Frame 827 processing time: 0.0367 seconds\n",
      "PyTorch inference took 0.007002830505371094 sec\n",
      "PyTorch postprocessing took 0.012966394424438477 sec\n",
      "Frame 828 processing time: 0.0561 seconds\n",
      "PyTorch inference took 0.004608869552612305 sec\n",
      "PyTorch postprocessing took 0.010077953338623047 sec\n",
      "Frame 829 processing time: 0.0370 seconds\n",
      "PyTorch inference took 0.00550532341003418 sec\n",
      "PyTorch postprocessing took 0.009420156478881836 sec\n",
      "Frame 830 processing time: 0.0425 seconds\n",
      "PyTorch inference took 0.0051496028900146484 sec\n",
      "PyTorch postprocessing took 0.014459609985351562 sec\n",
      "Frame 831 processing time: 0.0377 seconds\n",
      "PyTorch inference took 0.004617929458618164 sec\n",
      "PyTorch postprocessing took 0.008636236190795898 sec\n",
      "Frame 832 processing time: 0.0362 seconds\n",
      "PyTorch inference took 0.010251522064208984 sec\n",
      "PyTorch postprocessing took 0.011439085006713867 sec\n",
      "Frame 833 processing time: 0.0631 seconds\n",
      "PyTorch inference took 0.0029821395874023438 sec\n",
      "PyTorch postprocessing took 0.009821176528930664 sec\n",
      "Frame 834 processing time: 0.0276 seconds\n",
      "PyTorch inference took 0.0030939579010009766 sec\n",
      "PyTorch postprocessing took 0.009758710861206055 sec\n",
      "Frame 835 processing time: 0.0265 seconds\n",
      "PyTorch inference took 0.0029668807983398438 sec\n",
      "PyTorch postprocessing took 0.01016998291015625 sec\n",
      "Frame 836 processing time: 0.0265 seconds\n",
      "PyTorch inference took 0.0029010772705078125 sec\n",
      "PyTorch postprocessing took 0.01000666618347168 sec\n",
      "Frame 837 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.002961874008178711 sec\n",
      "PyTorch postprocessing took 0.010055065155029297 sec\n",
      "Frame 838 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.0043942928314208984 sec\n",
      "PyTorch postprocessing took 0.0109100341796875 sec\n",
      "Frame 839 processing time: 0.0338 seconds\n",
      "PyTorch inference took 0.0039014816284179688 sec\n",
      "PyTorch postprocessing took 0.00977468490600586 sec\n",
      "Frame 840 processing time: 0.0287 seconds\n",
      "PyTorch inference took 0.004174709320068359 sec\n",
      "PyTorch postprocessing took 0.008887529373168945 sec\n",
      "Frame 841 processing time: 0.0329 seconds\n",
      "PyTorch inference took 0.005517721176147461 sec\n",
      "PyTorch postprocessing took 0.009023904800415039 sec\n",
      "Frame 842 processing time: 0.0408 seconds\n",
      "PyTorch inference took 0.003468036651611328 sec\n",
      "PyTorch postprocessing took 0.015413999557495117 sec\n",
      "Frame 843 processing time: 0.0462 seconds\n",
      "PyTorch inference took 0.0037946701049804688 sec\n",
      "PyTorch postprocessing took 0.009338140487670898 sec\n",
      "Frame 844 processing time: 0.0306 seconds\n",
      "PyTorch inference took 0.004118919372558594 sec\n",
      "PyTorch postprocessing took 0.014766454696655273 sec\n",
      "Frame 845 processing time: 0.0447 seconds\n",
      "PyTorch inference took 0.003062009811401367 sec\n",
      "PyTorch postprocessing took 0.01025247573852539 sec\n",
      "Frame 846 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.0037627220153808594 sec\n",
      "PyTorch postprocessing took 0.010103225708007812 sec\n",
      "Frame 847 processing time: 0.0289 seconds\n",
      "PyTorch inference took 0.004433393478393555 sec\n",
      "PyTorch postprocessing took 0.014674663543701172 sec\n",
      "Frame 848 processing time: 0.0357 seconds\n",
      "PyTorch inference took 0.004597902297973633 sec\n",
      "PyTorch postprocessing took 0.008406877517700195 sec\n",
      "Frame 849 processing time: 0.0344 seconds\n",
      "PyTorch inference took 0.0037581920623779297 sec\n",
      "PyTorch postprocessing took 0.015244722366333008 sec\n",
      "Frame 850 processing time: 0.0510 seconds\n",
      "PyTorch inference took 0.0057125091552734375 sec\n",
      "PyTorch postprocessing took 0.010699272155761719 sec\n",
      "Frame 851 processing time: 0.0357 seconds\n",
      "PyTorch inference took 0.0029654502868652344 sec\n",
      "PyTorch postprocessing took 0.009735107421875 sec\n",
      "Frame 852 processing time: 0.0275 seconds\n",
      "PyTorch inference took 0.0037763118743896484 sec\n",
      "PyTorch postprocessing took 0.008998394012451172 sec\n",
      "Frame 853 processing time: 0.0344 seconds\n",
      "PyTorch inference took 0.007066965103149414 sec\n",
      "PyTorch postprocessing took 0.013562440872192383 sec\n",
      "Frame 854 processing time: 0.0604 seconds\n",
      "PyTorch inference took 0.004042625427246094 sec\n",
      "PyTorch postprocessing took 0.01003575325012207 sec\n",
      "Frame 855 processing time: 0.0330 seconds\n",
      "PyTorch inference took 0.00529932975769043 sec\n",
      "PyTorch postprocessing took 0.008928060531616211 sec\n",
      "Frame 856 processing time: 0.0308 seconds\n",
      "PyTorch inference took 0.008939266204833984 sec\n",
      "PyTorch postprocessing took 0.009739160537719727 sec\n",
      "Frame 857 processing time: 0.0404 seconds\n",
      "PyTorch inference took 0.004967451095581055 sec\n",
      "PyTorch postprocessing took 0.008049249649047852 sec\n",
      "Frame 858 processing time: 0.0280 seconds\n",
      "PyTorch inference took 0.003119230270385742 sec\n",
      "PyTorch postprocessing took 0.010052919387817383 sec\n",
      "Frame 859 processing time: 0.0311 seconds\n",
      "PyTorch inference took 0.003023386001586914 sec\n",
      "PyTorch postprocessing took 0.009779930114746094 sec\n",
      "Frame 860 processing time: 0.0289 seconds\n",
      "PyTorch inference took 0.0029747486114501953 sec\n",
      "PyTorch postprocessing took 0.010190010070800781 sec\n",
      "Frame 861 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.0037872791290283203 sec\n",
      "PyTorch postprocessing took 0.009782075881958008 sec\n",
      "Frame 862 processing time: 0.0330 seconds\n",
      "PyTorch inference took 0.0033617019653320312 sec\n",
      "PyTorch postprocessing took 0.0101470947265625 sec\n",
      "Frame 863 processing time: 0.0326 seconds\n",
      "PyTorch inference took 0.0029790401458740234 sec\n",
      "PyTorch postprocessing took 0.009771585464477539 sec\n",
      "Frame 864 processing time: 0.0329 seconds\n",
      "PyTorch inference took 0.0032646656036376953 sec\n",
      "PyTorch postprocessing took 0.00994729995727539 sec\n",
      "Frame 865 processing time: 0.0343 seconds\n",
      "PyTorch inference took 0.002939462661743164 sec\n",
      "PyTorch postprocessing took 0.012793779373168945 sec\n",
      "Frame 866 processing time: 0.0390 seconds\n",
      "PyTorch inference took 0.008577346801757812 sec\n",
      "PyTorch postprocessing took 0.008017778396606445 sec\n",
      "Frame 867 processing time: 0.0732 seconds\n",
      "PyTorch inference took 0.010486125946044922 sec\n",
      "PyTorch postprocessing took 0.00422978401184082 sec\n",
      "Frame 868 processing time: 0.0380 seconds\n",
      "PyTorch inference took 0.00882720947265625 sec\n",
      "PyTorch postprocessing took 0.009608983993530273 sec\n",
      "Frame 869 processing time: 0.0394 seconds\n",
      "PyTorch inference took 0.006337404251098633 sec\n",
      "PyTorch postprocessing took 0.023895978927612305 sec\n",
      "Frame 870 processing time: 0.0541 seconds\n",
      "PyTorch inference took 0.003785371780395508 sec\n",
      "PyTorch postprocessing took 0.008985519409179688 sec\n",
      "Frame 871 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.00286865234375 sec\n",
      "PyTorch postprocessing took 0.010527610778808594 sec\n",
      "Frame 872 processing time: 0.0272 seconds\n",
      "PyTorch inference took 0.004377841949462891 sec\n",
      "PyTorch postprocessing took 0.014241456985473633 sec\n",
      "Frame 873 processing time: 0.0353 seconds\n",
      "PyTorch inference took 0.008188247680664062 sec\n",
      "PyTorch postprocessing took 0.024106979370117188 sec\n",
      "Frame 874 processing time: 0.0578 seconds\n",
      "PyTorch inference took 0.0032520294189453125 sec\n",
      "PyTorch postprocessing took 0.009777545928955078 sec\n",
      "Frame 875 processing time: 0.0347 seconds\n",
      "PyTorch inference took 0.0033740997314453125 sec\n",
      "PyTorch postprocessing took 0.009714126586914062 sec\n",
      "Frame 876 processing time: 0.0266 seconds\n",
      "PyTorch inference took 0.004548788070678711 sec\n",
      "PyTorch postprocessing took 0.009458541870117188 sec\n",
      "Frame 877 processing time: 0.0337 seconds\n",
      "PyTorch inference took 0.0029087066650390625 sec\n",
      "PyTorch postprocessing took 0.01022791862487793 sec\n",
      "Frame 878 processing time: 0.0285 seconds\n",
      "PyTorch inference took 0.003573179244995117 sec\n",
      "PyTorch postprocessing took 0.009464025497436523 sec\n",
      "Frame 879 processing time: 0.0343 seconds\n",
      "PyTorch inference took 0.005955934524536133 sec\n",
      "PyTorch postprocessing took 0.00843954086303711 sec\n",
      "Frame 880 processing time: 0.0352 seconds\n",
      "PyTorch inference took 0.003781557083129883 sec\n",
      "PyTorch postprocessing took 0.008989095687866211 sec\n",
      "Frame 881 processing time: 0.0314 seconds\n",
      "PyTorch inference took 0.0037496089935302734 sec\n",
      "PyTorch postprocessing took 0.009617090225219727 sec\n",
      "Frame 882 processing time: 0.0314 seconds\n",
      "PyTorch inference took 0.004445075988769531 sec\n",
      "PyTorch postprocessing took 0.014583349227905273 sec\n",
      "Frame 883 processing time: 0.0410 seconds\n",
      "PyTorch inference took 0.005065441131591797 sec\n",
      "PyTorch postprocessing took 0.00971531867980957 sec\n",
      "Frame 884 processing time: 0.0422 seconds\n",
      "PyTorch inference took 0.004652976989746094 sec\n",
      "PyTorch postprocessing took 0.011765718460083008 sec\n",
      "Frame 885 processing time: 0.0641 seconds\n",
      "PyTorch inference took 0.008605003356933594 sec\n",
      "PyTorch postprocessing took 0.008959770202636719 sec\n",
      "Frame 886 processing time: 0.0612 seconds\n",
      "PyTorch inference took 0.004397153854370117 sec\n",
      "PyTorch postprocessing took 0.016489267349243164 sec\n",
      "Frame 887 processing time: 0.0446 seconds\n",
      "PyTorch inference took 0.005032777786254883 sec\n",
      "PyTorch postprocessing took 0.014170169830322266 sec\n",
      "Frame 888 processing time: 0.0454 seconds\n",
      "PyTorch inference took 0.003896951675415039 sec\n",
      "PyTorch postprocessing took 0.009644508361816406 sec\n",
      "Frame 889 processing time: 0.0306 seconds\n",
      "PyTorch inference took 0.008265256881713867 sec\n",
      "PyTorch postprocessing took 0.0077936649322509766 sec\n",
      "Frame 890 processing time: 0.0372 seconds\n",
      "PyTorch inference took 0.0030374526977539062 sec\n",
      "PyTorch postprocessing took 0.009726524353027344 sec\n",
      "Frame 891 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.004564046859741211 sec\n",
      "PyTorch postprocessing took 0.009494543075561523 sec\n",
      "Frame 892 processing time: 0.0386 seconds\n",
      "PyTorch inference took 0.003978252410888672 sec\n",
      "PyTorch postprocessing took 0.010141849517822266 sec\n",
      "Frame 893 processing time: 0.0344 seconds\n",
      "PyTorch inference took 0.018718242645263672 sec\n",
      "PyTorch postprocessing took 0.007593393325805664 sec\n",
      "Frame 894 processing time: 0.0636 seconds\n",
      "PyTorch inference took 0.011425018310546875 sec\n",
      "PyTorch postprocessing took 0.010369062423706055 sec\n",
      "Frame 895 processing time: 0.0468 seconds\n",
      "PyTorch inference took 0.0032918453216552734 sec\n",
      "PyTorch postprocessing took 0.010979652404785156 sec\n",
      "Frame 896 processing time: 0.0332 seconds\n",
      "PyTorch inference took 0.00565648078918457 sec\n",
      "PyTorch postprocessing took 0.009521007537841797 sec\n",
      "Frame 897 processing time: 0.0438 seconds\n",
      "PyTorch inference took 0.003381013870239258 sec\n",
      "PyTorch postprocessing took 0.011303424835205078 sec\n",
      "Frame 898 processing time: 0.0379 seconds\n",
      "PyTorch inference took 0.007953166961669922 sec\n",
      "PyTorch postprocessing took 0.008670568466186523 sec\n",
      "Frame 899 processing time: 0.0429 seconds\n",
      "PyTorch inference took 0.005979776382446289 sec\n",
      "PyTorch postprocessing took 0.015325784683227539 sec\n",
      "Frame 900 processing time: 0.0730 seconds\n",
      "PyTorch inference took 0.008960247039794922 sec\n",
      "PyTorch postprocessing took 0.008685588836669922 sec\n",
      "Frame 901 processing time: 0.0526 seconds\n",
      "PyTorch inference took 0.007581233978271484 sec\n",
      "PyTorch postprocessing took 0.009912490844726562 sec\n",
      "Frame 902 processing time: 0.0481 seconds\n",
      "PyTorch inference took 0.006192207336425781 sec\n",
      "PyTorch postprocessing took 0.013849020004272461 sec\n",
      "Frame 903 processing time: 0.0490 seconds\n",
      "PyTorch inference took 0.0032596588134765625 sec\n",
      "PyTorch postprocessing took 0.010087251663208008 sec\n",
      "Frame 904 processing time: 0.0280 seconds\n",
      "PyTorch inference took 0.002998828887939453 sec\n",
      "PyTorch postprocessing took 0.009775638580322266 sec\n",
      "Frame 905 processing time: 0.0247 seconds\n",
      "PyTorch inference took 0.002954721450805664 sec\n",
      "PyTorch postprocessing took 0.009505987167358398 sec\n",
      "Frame 906 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.004116535186767578 sec\n",
      "PyTorch postprocessing took 0.010902166366577148 sec\n",
      "Frame 907 processing time: 0.0333 seconds\n",
      "PyTorch inference took 0.004357576370239258 sec\n",
      "PyTorch postprocessing took 0.010974407196044922 sec\n",
      "Frame 908 processing time: 0.0331 seconds\n",
      "PyTorch inference took 0.006280183792114258 sec\n",
      "PyTorch postprocessing took 0.007855892181396484 sec\n",
      "Frame 909 processing time: 0.0324 seconds\n",
      "PyTorch inference took 0.005183696746826172 sec\n",
      "PyTorch postprocessing took 0.010139226913452148 sec\n",
      "Frame 910 processing time: 0.0355 seconds\n",
      "PyTorch inference took 0.007199287414550781 sec\n",
      "PyTorch postprocessing took 0.015259742736816406 sec\n",
      "Frame 911 processing time: 0.0434 seconds\n",
      "PyTorch inference took 0.0029184818267822266 sec\n",
      "PyTorch postprocessing took 0.00952768325805664 sec\n",
      "Frame 912 processing time: 0.0269 seconds\n",
      "PyTorch inference took 0.005126237869262695 sec\n",
      "PyTorch postprocessing took 0.009480953216552734 sec\n",
      "Frame 913 processing time: 0.0321 seconds\n",
      "PyTorch inference took 0.0060617923736572266 sec\n",
      "PyTorch postprocessing took 0.0064203739166259766 sec\n",
      "Frame 914 processing time: 0.0354 seconds\n",
      "PyTorch inference took 0.006092548370361328 sec\n",
      "PyTorch postprocessing took 0.008471488952636719 sec\n",
      "Frame 915 processing time: 0.0419 seconds\n",
      "PyTorch inference took 0.003103971481323242 sec\n",
      "PyTorch postprocessing took 0.009430170059204102 sec\n",
      "Frame 916 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.0037965774536132812 sec\n",
      "PyTorch postprocessing took 0.008682966232299805 sec\n",
      "Frame 917 processing time: 0.0265 seconds\n",
      "PyTorch inference took 0.002995014190673828 sec\n",
      "PyTorch postprocessing took 0.009507894515991211 sec\n",
      "Frame 918 processing time: 0.0275 seconds\n",
      "PyTorch inference took 0.002980470657348633 sec\n",
      "PyTorch postprocessing took 0.009498834609985352 sec\n",
      "Frame 919 processing time: 0.0303 seconds\n",
      "PyTorch inference took 0.003931760787963867 sec\n",
      "PyTorch postprocessing took 0.008644819259643555 sec\n",
      "Frame 920 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.0031423568725585938 sec\n",
      "PyTorch postprocessing took 0.010260581970214844 sec\n",
      "Frame 921 processing time: 0.0355 seconds\n",
      "PyTorch inference took 0.005254983901977539 sec\n",
      "PyTorch postprocessing took 0.015487194061279297 sec\n",
      "Frame 922 processing time: 0.0396 seconds\n",
      "PyTorch inference took 0.007163286209106445 sec\n",
      "PyTorch postprocessing took 0.006736278533935547 sec\n",
      "Frame 923 processing time: 0.0415 seconds\n",
      "PyTorch inference took 0.004372358322143555 sec\n",
      "PyTorch postprocessing took 0.013625860214233398 sec\n",
      "Frame 924 processing time: 0.0331 seconds\n",
      "PyTorch inference took 0.0032939910888671875 sec\n",
      "PyTorch postprocessing took 0.009154081344604492 sec\n",
      "Frame 925 processing time: 0.0242 seconds\n",
      "PyTorch inference took 0.0031080245971679688 sec\n",
      "PyTorch postprocessing took 0.009363651275634766 sec\n",
      "Frame 926 processing time: 0.0245 seconds\n",
      "PyTorch inference took 0.004042863845825195 sec\n",
      "PyTorch postprocessing took 0.011257410049438477 sec\n",
      "Frame 927 processing time: 0.0297 seconds\n",
      "PyTorch inference took 0.003324747085571289 sec\n",
      "PyTorch postprocessing took 0.01577305793762207 sec\n",
      "Frame 928 processing time: 0.0369 seconds\n",
      "PyTorch inference took 0.003763914108276367 sec\n",
      "PyTorch postprocessing took 0.009638071060180664 sec\n",
      "Frame 929 processing time: 0.0267 seconds\n",
      "PyTorch inference took 0.003703594207763672 sec\n",
      "PyTorch postprocessing took 0.009381294250488281 sec\n",
      "Frame 930 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.0030760765075683594 sec\n",
      "PyTorch postprocessing took 0.009419679641723633 sec\n",
      "Frame 931 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.0028862953186035156 sec\n",
      "PyTorch postprocessing took 0.009655237197875977 sec\n",
      "Frame 932 processing time: 0.0276 seconds\n",
      "PyTorch inference took 0.0037174224853515625 sec\n",
      "PyTorch postprocessing took 0.009746551513671875 sec\n",
      "Frame 933 processing time: 0.0290 seconds\n",
      "PyTorch inference took 0.0029282569885253906 sec\n",
      "PyTorch postprocessing took 0.009615421295166016 sec\n",
      "Frame 934 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.004645109176635742 sec\n",
      "PyTorch postprocessing took 0.014287710189819336 sec\n",
      "Frame 935 processing time: 0.0371 seconds\n",
      "PyTorch inference took 0.005221366882324219 sec\n",
      "PyTorch postprocessing took 0.011084556579589844 sec\n",
      "Frame 936 processing time: 0.0498 seconds\n",
      "PyTorch inference took 0.006672382354736328 sec\n",
      "PyTorch postprocessing took 0.011641502380371094 sec\n",
      "Frame 937 processing time: 0.0549 seconds\n",
      "PyTorch inference took 0.011284828186035156 sec\n",
      "PyTorch postprocessing took 0.011504173278808594 sec\n",
      "Frame 938 processing time: 0.0564 seconds\n",
      "PyTorch inference took 0.003998756408691406 sec\n",
      "PyTorch postprocessing took 0.009392976760864258 sec\n",
      "Frame 939 processing time: 0.0318 seconds\n",
      "PyTorch inference took 0.00516057014465332 sec\n",
      "PyTorch postprocessing took 0.008055925369262695 sec\n",
      "Frame 940 processing time: 0.0363 seconds\n",
      "PyTorch inference took 0.003036975860595703 sec\n",
      "PyTorch postprocessing took 0.009523391723632812 sec\n",
      "Frame 941 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.007508516311645508 sec\n",
      "PyTorch postprocessing took 0.01812005043029785 sec\n",
      "Frame 942 processing time: 0.0543 seconds\n",
      "PyTorch inference took 0.003757476806640625 sec\n",
      "PyTorch postprocessing took 0.015067100524902344 sec\n",
      "Frame 943 processing time: 0.0330 seconds\n",
      "PyTorch inference took 0.003007173538208008 sec\n",
      "PyTorch postprocessing took 0.00953221321105957 sec\n",
      "Frame 944 processing time: 0.0293 seconds\n",
      "PyTorch inference took 0.003451108932495117 sec\n",
      "PyTorch postprocessing took 0.009485960006713867 sec\n",
      "Frame 945 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.007681369781494141 sec\n",
      "PyTorch postprocessing took 0.010137557983398438 sec\n",
      "Frame 946 processing time: 0.0453 seconds\n",
      "PyTorch inference took 0.0034437179565429688 sec\n",
      "PyTorch postprocessing took 0.009637594223022461 sec\n",
      "Frame 947 processing time: 0.0289 seconds\n",
      "PyTorch inference took 0.0037415027618408203 sec\n",
      "PyTorch postprocessing took 0.010982275009155273 sec\n",
      "Frame 948 processing time: 0.0356 seconds\n",
      "PyTorch inference took 0.005768775939941406 sec\n",
      "PyTorch postprocessing took 0.008749723434448242 sec\n",
      "Frame 949 processing time: 0.0442 seconds\n",
      "PyTorch inference took 0.00458979606628418 sec\n",
      "PyTorch postprocessing took 0.008720874786376953 sec\n",
      "Frame 950 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.00462031364440918 sec\n",
      "PyTorch postprocessing took 0.013257503509521484 sec\n",
      "Frame 951 processing time: 0.0390 seconds\n",
      "PyTorch inference took 0.003828287124633789 sec\n",
      "PyTorch postprocessing took 0.014815092086791992 sec\n",
      "Frame 952 processing time: 0.0370 seconds\n",
      "PyTorch inference took 0.0030820369720458984 sec\n",
      "PyTorch postprocessing took 0.010027647018432617 sec\n",
      "Frame 953 processing time: 0.0297 seconds\n",
      "PyTorch inference took 0.010129928588867188 sec\n",
      "PyTorch postprocessing took 0.008282661437988281 sec\n",
      "Frame 954 processing time: 0.0578 seconds\n",
      "PyTorch inference took 0.006115436553955078 sec\n",
      "PyTorch postprocessing took 0.008753538131713867 sec\n",
      "Frame 955 processing time: 0.0468 seconds\n",
      "PyTorch inference took 0.0033664703369140625 sec\n",
      "PyTorch postprocessing took 0.009446144104003906 sec\n",
      "Frame 956 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.004472017288208008 sec\n",
      "PyTorch postprocessing took 0.008425712585449219 sec\n",
      "Frame 957 processing time: 0.0255 seconds\n",
      "PyTorch inference took 0.003345966339111328 sec\n",
      "PyTorch postprocessing took 0.00958561897277832 sec\n",
      "Frame 958 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.0030167102813720703 sec\n",
      "PyTorch postprocessing took 0.009828805923461914 sec\n",
      "Frame 959 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.0031108856201171875 sec\n",
      "PyTorch postprocessing took 0.009871959686279297 sec\n",
      "Frame 960 processing time: 0.0266 seconds\n",
      "PyTorch inference took 0.0030264854431152344 sec\n",
      "PyTorch postprocessing took 0.009954214096069336 sec\n",
      "Frame 961 processing time: 0.0319 seconds\n",
      "PyTorch inference took 0.003360748291015625 sec\n",
      "PyTorch postprocessing took 0.00927281379699707 sec\n",
      "Frame 962 processing time: 0.0278 seconds\n",
      "PyTorch inference took 0.003882884979248047 sec\n",
      "PyTorch postprocessing took 0.01184391975402832 sec\n",
      "Frame 963 processing time: 0.0303 seconds\n",
      "PyTorch inference took 0.004388093948364258 sec\n",
      "PyTorch postprocessing took 0.010107755661010742 sec\n",
      "Frame 964 processing time: 0.0447 seconds\n",
      "PyTorch inference took 0.0033469200134277344 sec\n",
      "PyTorch postprocessing took 0.009420156478881836 sec\n",
      "Frame 965 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.004856109619140625 sec\n",
      "PyTorch postprocessing took 0.009450197219848633 sec\n",
      "Frame 966 processing time: 0.0458 seconds\n",
      "PyTorch inference took 0.0029501914978027344 sec\n",
      "PyTorch postprocessing took 0.009516716003417969 sec\n",
      "Frame 967 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.0029141902923583984 sec\n",
      "PyTorch postprocessing took 0.009510278701782227 sec\n",
      "Frame 968 processing time: 0.0272 seconds\n",
      "PyTorch inference took 0.004512310028076172 sec\n",
      "PyTorch postprocessing took 0.009420156478881836 sec\n",
      "Frame 969 processing time: 0.0294 seconds\n",
      "PyTorch inference took 0.0029268264770507812 sec\n",
      "PyTorch postprocessing took 0.009589910507202148 sec\n",
      "Frame 970 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.005220174789428711 sec\n",
      "PyTorch postprocessing took 0.00925445556640625 sec\n",
      "Frame 971 processing time: 0.0353 seconds\n",
      "PyTorch inference took 0.003256082534790039 sec\n",
      "PyTorch postprocessing took 0.010787725448608398 sec\n",
      "Frame 972 processing time: 0.0294 seconds\n",
      "PyTorch inference took 0.0038564205169677734 sec\n",
      "PyTorch postprocessing took 0.014705181121826172 sec\n",
      "Frame 973 processing time: 0.0371 seconds\n",
      "PyTorch inference took 0.005051136016845703 sec\n",
      "PyTorch postprocessing took 0.007848501205444336 sec\n",
      "Frame 974 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.004163026809692383 sec\n",
      "PyTorch postprocessing took 0.009211301803588867 sec\n",
      "Frame 975 processing time: 0.0300 seconds\n",
      "PyTorch inference took 0.0029141902923583984 sec\n",
      "PyTorch postprocessing took 0.00990915298461914 sec\n",
      "Frame 976 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.002966642379760742 sec\n",
      "PyTorch postprocessing took 0.009525299072265625 sec\n",
      "Frame 977 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.004178762435913086 sec\n",
      "PyTorch postprocessing took 0.009542226791381836 sec\n",
      "Frame 978 processing time: 0.0265 seconds\n",
      "PyTorch inference took 0.0038890838623046875 sec\n",
      "PyTorch postprocessing took 0.00857996940612793 sec\n",
      "Frame 979 processing time: 0.0297 seconds\n",
      "PyTorch inference took 0.0031044483184814453 sec\n",
      "PyTorch postprocessing took 0.009393453598022461 sec\n",
      "Frame 980 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.005007743835449219 sec\n",
      "PyTorch postprocessing took 0.008507728576660156 sec\n",
      "Frame 981 processing time: 0.0320 seconds\n",
      "PyTorch inference took 0.002985715866088867 sec\n",
      "PyTorch postprocessing took 0.009533882141113281 sec\n",
      "Frame 982 processing time: 0.0302 seconds\n",
      "PyTorch inference took 0.008123636245727539 sec\n",
      "PyTorch postprocessing took 0.009927034378051758 sec\n",
      "Frame 983 processing time: 0.0346 seconds\n",
      "PyTorch inference took 0.0055446624755859375 sec\n",
      "PyTorch postprocessing took 0.007647514343261719 sec\n",
      "Frame 984 processing time: 0.0334 seconds\n",
      "PyTorch inference took 0.006356239318847656 sec\n",
      "PyTorch postprocessing took 0.011544466018676758 sec\n",
      "Frame 985 processing time: 0.0486 seconds\n",
      "PyTorch inference took 0.0038290023803710938 sec\n",
      "PyTorch postprocessing took 0.011031389236450195 sec\n",
      "Frame 986 processing time: 0.0380 seconds\n",
      "PyTorch inference took 0.004333972930908203 sec\n",
      "PyTorch postprocessing took 0.009402990341186523 sec\n",
      "Frame 987 processing time: 0.0290 seconds\n",
      "PyTorch inference took 0.002951383590698242 sec\n",
      "PyTorch postprocessing took 0.00954747200012207 sec\n",
      "Frame 988 processing time: 0.0291 seconds\n",
      "PyTorch inference took 0.0029578208923339844 sec\n",
      "PyTorch postprocessing took 0.00946497917175293 sec\n",
      "Frame 989 processing time: 0.0277 seconds\n",
      "PyTorch inference took 0.0061187744140625 sec\n",
      "PyTorch postprocessing took 0.010842323303222656 sec\n",
      "Frame 990 processing time: 0.0451 seconds\n",
      "PyTorch inference took 0.0038988590240478516 sec\n",
      "PyTorch postprocessing took 0.009595394134521484 sec\n",
      "Frame 991 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.003949165344238281 sec\n",
      "PyTorch postprocessing took 0.009601116180419922 sec\n",
      "Frame 992 processing time: 0.0276 seconds\n",
      "PyTorch inference took 0.00736546516418457 sec\n",
      "PyTorch postprocessing took 0.009874343872070312 sec\n",
      "Frame 993 processing time: 0.0529 seconds\n",
      "PyTorch inference took 0.004037380218505859 sec\n",
      "PyTorch postprocessing took 0.00986933708190918 sec\n",
      "Frame 994 processing time: 0.0349 seconds\n",
      "PyTorch inference took 0.005791664123535156 sec\n",
      "PyTorch postprocessing took 0.016112089157104492 sec\n",
      "Frame 995 processing time: 0.0429 seconds\n",
      "PyTorch inference took 0.004269599914550781 sec\n",
      "PyTorch postprocessing took 0.008874893188476562 sec\n",
      "Frame 996 processing time: 0.0294 seconds\n",
      "PyTorch inference took 0.0075719356536865234 sec\n",
      "PyTorch postprocessing took 0.012239933013916016 sec\n",
      "Frame 997 processing time: 0.0375 seconds\n",
      "PyTorch inference took 0.0029189586639404297 sec\n",
      "PyTorch postprocessing took 0.009579896926879883 sec\n",
      "Frame 998 processing time: 0.0267 seconds\n",
      "PyTorch inference took 0.010378837585449219 sec\n",
      "PyTorch postprocessing took 0.008696317672729492 sec\n",
      "Frame 999 processing time: 0.0361 seconds\n",
      "PyTorch inference took 0.004467010498046875 sec\n",
      "PyTorch postprocessing took 0.01682114601135254 sec\n",
      "Frame 1000 processing time: 0.0440 seconds\n",
      "PyTorch inference took 0.004548072814941406 sec\n",
      "PyTorch postprocessing took 0.011585474014282227 sec\n",
      "Frame 1001 processing time: 0.0362 seconds\n",
      "PyTorch inference took 0.002976655960083008 sec\n",
      "PyTorch postprocessing took 0.009557723999023438 sec\n",
      "Frame 1002 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.00325775146484375 sec\n",
      "PyTorch postprocessing took 0.009138345718383789 sec\n",
      "Frame 1003 processing time: 0.0246 seconds\n",
      "PyTorch inference took 0.004797935485839844 sec\n",
      "PyTorch postprocessing took 0.008718013763427734 sec\n",
      "Frame 1004 processing time: 0.0315 seconds\n",
      "PyTorch inference took 0.0043027400970458984 sec\n",
      "PyTorch postprocessing took 0.009422063827514648 sec\n",
      "Frame 1005 processing time: 0.0326 seconds\n",
      "PyTorch inference took 0.004378557205200195 sec\n",
      "PyTorch postprocessing took 0.009301185607910156 sec\n",
      "Frame 1006 processing time: 0.0323 seconds\n",
      "PyTorch inference took 0.004677534103393555 sec\n",
      "PyTorch postprocessing took 0.008923768997192383 sec\n",
      "Frame 1007 processing time: 0.0288 seconds\n",
      "PyTorch inference took 0.004793882369995117 sec\n",
      "PyTorch postprocessing took 0.008917570114135742 sec\n",
      "Frame 1008 processing time: 0.0287 seconds\n",
      "PyTorch inference took 0.005270957946777344 sec\n",
      "PyTorch postprocessing took 0.007793903350830078 sec\n",
      "Frame 1009 processing time: 0.0461 seconds\n",
      "PyTorch inference took 0.0077059268951416016 sec\n",
      "PyTorch postprocessing took 0.01654839515686035 sec\n",
      "Frame 1010 processing time: 0.0477 seconds\n",
      "PyTorch inference took 0.00774383544921875 sec\n",
      "PyTorch postprocessing took 0.01814579963684082 sec\n",
      "Frame 1011 processing time: 0.0575 seconds\n",
      "PyTorch inference took 0.002946615219116211 sec\n",
      "PyTorch postprocessing took 0.010150909423828125 sec\n",
      "Frame 1012 processing time: 0.0282 seconds\n",
      "PyTorch inference took 0.0037555694580078125 sec\n",
      "PyTorch postprocessing took 0.008867025375366211 sec\n",
      "Frame 1013 processing time: 0.0293 seconds\n",
      "PyTorch inference took 0.00838327407836914 sec\n",
      "PyTorch postprocessing took 0.012156248092651367 sec\n",
      "Frame 1014 processing time: 0.0449 seconds\n",
      "PyTorch inference took 0.006360292434692383 sec\n",
      "PyTorch postprocessing took 0.00798487663269043 sec\n",
      "Frame 1015 processing time: 0.0341 seconds\n",
      "PyTorch inference took 0.003926992416381836 sec\n",
      "PyTorch postprocessing took 0.012206554412841797 sec\n",
      "Frame 1016 processing time: 0.0324 seconds\n",
      "PyTorch inference took 0.0029151439666748047 sec\n",
      "PyTorch postprocessing took 0.009502649307250977 sec\n",
      "Frame 1017 processing time: 0.0242 seconds\n",
      "PyTorch inference took 0.005096912384033203 sec\n",
      "PyTorch postprocessing took 0.009103775024414062 sec\n",
      "Frame 1018 processing time: 0.0290 seconds\n",
      "PyTorch inference took 0.0070035457611083984 sec\n",
      "PyTorch postprocessing took 0.010718584060668945 sec\n",
      "Frame 1019 processing time: 0.0574 seconds\n",
      "PyTorch inference took 0.0044481754302978516 sec\n",
      "PyTorch postprocessing took 0.014305353164672852 sec\n",
      "Frame 1020 processing time: 0.0408 seconds\n",
      "PyTorch inference took 0.008464813232421875 sec\n",
      "PyTorch postprocessing took 0.011321306228637695 sec\n",
      "Frame 1021 processing time: 0.0404 seconds\n",
      "PyTorch inference took 0.007157325744628906 sec\n",
      "PyTorch postprocessing took 0.006037473678588867 sec\n",
      "Frame 1022 processing time: 0.0302 seconds\n",
      "PyTorch inference took 0.0031290054321289062 sec\n",
      "PyTorch postprocessing took 0.009657621383666992 sec\n",
      "Frame 1023 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.009410619735717773 sec\n",
      "PyTorch postprocessing took 0.0057752132415771484 sec\n",
      "Frame 1024 processing time: 0.0319 seconds\n",
      "PyTorch inference took 0.0028688907623291016 sec\n",
      "PyTorch postprocessing took 0.009657859802246094 sec\n",
      "Frame 1025 processing time: 0.0261 seconds\n",
      "PyTorch inference took 0.005992412567138672 sec\n",
      "PyTorch postprocessing took 0.006453752517700195 sec\n",
      "Frame 1026 processing time: 0.0359 seconds\n",
      "PyTorch inference took 0.005968332290649414 sec\n",
      "PyTorch postprocessing took 0.011257410049438477 sec\n",
      "Frame 1027 processing time: 0.0384 seconds\n",
      "PyTorch inference took 0.00409388542175293 sec\n",
      "PyTorch postprocessing took 0.009890079498291016 sec\n",
      "Frame 1028 processing time: 0.0275 seconds\n",
      "PyTorch inference took 0.002921581268310547 sec\n",
      "PyTorch postprocessing took 0.01014089584350586 sec\n",
      "Frame 1029 processing time: 0.0255 seconds\n",
      "PyTorch inference took 0.005557060241699219 sec\n",
      "PyTorch postprocessing took 0.007850408554077148 sec\n",
      "Frame 1030 processing time: 0.0251 seconds\n",
      "PyTorch inference took 0.0036935806274414062 sec\n",
      "PyTorch postprocessing took 0.014770030975341797 sec\n",
      "Frame 1031 processing time: 0.0354 seconds\n",
      "PyTorch inference took 0.002888202667236328 sec\n",
      "PyTorch postprocessing took 0.009705543518066406 sec\n",
      "Frame 1032 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.0029654502868652344 sec\n",
      "PyTorch postprocessing took 0.009515523910522461 sec\n",
      "Frame 1033 processing time: 0.0252 seconds\n",
      "PyTorch inference took 0.003317117691040039 sec\n",
      "PyTorch postprocessing took 0.00915384292602539 sec\n",
      "Frame 1034 processing time: 0.0255 seconds\n",
      "PyTorch inference took 0.006581783294677734 sec\n",
      "PyTorch postprocessing took 0.006409406661987305 sec\n",
      "Frame 1035 processing time: 0.0303 seconds\n",
      "PyTorch inference took 0.006890296936035156 sec\n",
      "PyTorch postprocessing took 0.014594554901123047 sec\n",
      "Frame 1036 processing time: 0.0597 seconds\n",
      "PyTorch inference took 0.009015083312988281 sec\n",
      "PyTorch postprocessing took 0.016054153442382812 sec\n",
      "Frame 1037 processing time: 0.0659 seconds\n",
      "PyTorch inference took 0.005650043487548828 sec\n",
      "PyTorch postprocessing took 0.006844282150268555 sec\n",
      "Frame 1038 processing time: 0.0275 seconds\n",
      "PyTorch inference took 0.004166841506958008 sec\n",
      "PyTorch postprocessing took 0.009369850158691406 sec\n",
      "Frame 1039 processing time: 0.0371 seconds\n",
      "PyTorch inference took 0.0046885013580322266 sec\n",
      "PyTorch postprocessing took 0.013520479202270508 sec\n",
      "Frame 1040 processing time: 0.0367 seconds\n",
      "PyTorch inference took 0.003169536590576172 sec\n",
      "PyTorch postprocessing took 0.009625673294067383 sec\n",
      "Frame 1041 processing time: 0.0300 seconds\n",
      "PyTorch inference took 0.004486083984375 sec\n",
      "PyTorch postprocessing took 0.009032964706420898 sec\n",
      "Frame 1042 processing time: 0.0325 seconds\n",
      "PyTorch inference took 0.003025531768798828 sec\n",
      "PyTorch postprocessing took 0.009344100952148438 sec\n",
      "Frame 1043 processing time: 0.0265 seconds\n",
      "PyTorch inference took 0.004714488983154297 sec\n",
      "PyTorch postprocessing took 0.007692575454711914 sec\n",
      "Frame 1044 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.012260675430297852 sec\n",
      "PyTorch postprocessing took 0.021219491958618164 sec\n",
      "Frame 1045 processing time: 0.0600 seconds\n",
      "PyTorch inference took 0.007042407989501953 sec\n",
      "PyTorch postprocessing took 0.0073795318603515625 sec\n",
      "Frame 1046 processing time: 0.0379 seconds\n",
      "PyTorch inference took 0.003945827484130859 sec\n",
      "PyTorch postprocessing took 0.009953022003173828 sec\n",
      "Frame 1047 processing time: 0.0287 seconds\n",
      "PyTorch inference took 0.0028710365295410156 sec\n",
      "PyTorch postprocessing took 0.00952911376953125 sec\n",
      "Frame 1048 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.007325649261474609 sec\n",
      "PyTorch postprocessing took 0.013057947158813477 sec\n",
      "Frame 1049 processing time: 0.0386 seconds\n",
      "PyTorch inference took 0.010905265808105469 sec\n",
      "PyTorch postprocessing took 0.02418804168701172 sec\n",
      "Frame 1050 processing time: 0.0594 seconds\n",
      "PyTorch inference took 0.008682727813720703 sec\n",
      "PyTorch postprocessing took 0.018532991409301758 sec\n",
      "Frame 1051 processing time: 0.0426 seconds\n",
      "PyTorch inference took 0.008500814437866211 sec\n",
      "PyTorch postprocessing took 0.01592564582824707 sec\n",
      "Frame 1052 processing time: 0.0396 seconds\n",
      "PyTorch inference took 0.0038754940032958984 sec\n",
      "PyTorch postprocessing took 0.013164043426513672 sec\n",
      "Frame 1053 processing time: 0.0356 seconds\n",
      "PyTorch inference took 0.0033822059631347656 sec\n",
      "PyTorch postprocessing took 0.019884109497070312 sec\n",
      "Frame 1054 processing time: 0.0443 seconds\n",
      "PyTorch inference took 0.003898143768310547 sec\n",
      "PyTorch postprocessing took 0.014296770095825195 sec\n",
      "Frame 1055 processing time: 0.0331 seconds\n",
      "PyTorch inference took 0.019853591918945312 sec\n",
      "PyTorch postprocessing took 0.026278972625732422 sec\n",
      "Frame 1056 processing time: 0.0701 seconds\n",
      "PyTorch inference took 0.004498720169067383 sec\n",
      "PyTorch postprocessing took 0.012701988220214844 sec\n",
      "Frame 1057 processing time: 0.0377 seconds\n",
      "PyTorch inference took 0.0038945674896240234 sec\n",
      "PyTorch postprocessing took 0.014569759368896484 sec\n",
      "Frame 1058 processing time: 0.0375 seconds\n",
      "PyTorch inference took 0.009148597717285156 sec\n",
      "PyTorch postprocessing took 0.017173290252685547 sec\n",
      "Frame 1059 processing time: 0.0587 seconds\n",
      "PyTorch inference took 0.003810882568359375 sec\n",
      "PyTorch postprocessing took 0.014713764190673828 sec\n",
      "Frame 1060 processing time: 0.0343 seconds\n",
      "PyTorch inference took 0.003902435302734375 sec\n",
      "PyTorch postprocessing took 0.011907815933227539 sec\n",
      "Frame 1061 processing time: 0.0309 seconds\n",
      "PyTorch inference took 0.0031137466430664062 sec\n",
      "PyTorch postprocessing took 0.009696245193481445 sec\n",
      "Frame 1062 processing time: 0.0319 seconds\n",
      "PyTorch inference took 0.006757974624633789 sec\n",
      "PyTorch postprocessing took 0.012699365615844727 sec\n",
      "Frame 1063 processing time: 0.0372 seconds\n",
      "PyTorch inference took 0.004055500030517578 sec\n",
      "PyTorch postprocessing took 0.015985488891601562 sec\n",
      "Frame 1064 processing time: 0.0387 seconds\n",
      "PyTorch inference took 0.002931356430053711 sec\n",
      "PyTorch postprocessing took 0.009540796279907227 sec\n",
      "Frame 1065 processing time: 0.0293 seconds\n",
      "PyTorch inference took 0.004019737243652344 sec\n",
      "PyTorch postprocessing took 0.00966787338256836 sec\n",
      "Frame 1066 processing time: 0.0290 seconds\n",
      "PyTorch inference took 0.004771232604980469 sec\n",
      "PyTorch postprocessing took 0.010251998901367188 sec\n",
      "Frame 1067 processing time: 0.0331 seconds\n",
      "PyTorch inference took 0.0038709640502929688 sec\n",
      "PyTorch postprocessing took 0.014359235763549805 sec\n",
      "Frame 1068 processing time: 0.0371 seconds\n",
      "PyTorch inference took 0.0073854923248291016 sec\n",
      "PyTorch postprocessing took 0.007208824157714844 sec\n",
      "Frame 1069 processing time: 0.0430 seconds\n",
      "PyTorch inference took 0.0031969547271728516 sec\n",
      "PyTorch postprocessing took 0.009558916091918945 sec\n",
      "Frame 1070 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.004602670669555664 sec\n",
      "PyTorch postprocessing took 0.007858514785766602 sec\n",
      "Frame 1071 processing time: 0.0265 seconds\n",
      "PyTorch inference took 0.0039033889770507812 sec\n",
      "PyTorch postprocessing took 0.015223503112792969 sec\n",
      "Frame 1072 processing time: 0.0435 seconds\n",
      "PyTorch inference took 0.0030705928802490234 sec\n",
      "PyTorch postprocessing took 0.010092735290527344 sec\n",
      "Frame 1073 processing time: 0.0322 seconds\n",
      "PyTorch inference took 0.007550477981567383 sec\n",
      "PyTorch postprocessing took 0.017001867294311523 sec\n",
      "Frame 1074 processing time: 0.0619 seconds\n",
      "PyTorch inference took 0.0065305233001708984 sec\n",
      "PyTorch postprocessing took 0.007402181625366211 sec\n",
      "Frame 1075 processing time: 0.0286 seconds\n",
      "PyTorch inference took 0.008168935775756836 sec\n",
      "PyTorch postprocessing took 0.008859395980834961 sec\n",
      "Frame 1076 processing time: 0.0435 seconds\n",
      "PyTorch inference took 0.00470733642578125 sec\n",
      "PyTorch postprocessing took 0.013757944107055664 sec\n",
      "Frame 1077 processing time: 0.0342 seconds\n",
      "PyTorch inference took 0.008156061172485352 sec\n",
      "PyTorch postprocessing took 0.013257503509521484 sec\n",
      "Frame 1078 processing time: 0.0504 seconds\n",
      "PyTorch inference took 0.003975391387939453 sec\n",
      "PyTorch postprocessing took 0.00930166244506836 sec\n",
      "Frame 1079 processing time: 0.0295 seconds\n",
      "PyTorch inference took 0.005400896072387695 sec\n",
      "PyTorch postprocessing took 0.009029388427734375 sec\n",
      "Frame 1080 processing time: 0.0359 seconds\n",
      "PyTorch inference took 0.006295680999755859 sec\n",
      "PyTorch postprocessing took 0.009165048599243164 sec\n",
      "Frame 1081 processing time: 0.0523 seconds\n",
      "PyTorch inference took 0.003873109817504883 sec\n",
      "PyTorch postprocessing took 0.009901046752929688 sec\n",
      "Frame 1082 processing time: 0.0299 seconds\n",
      "PyTorch inference took 0.0036764144897460938 sec\n",
      "PyTorch postprocessing took 0.015978097915649414 sec\n",
      "Frame 1083 processing time: 0.0390 seconds\n",
      "PyTorch inference took 0.004603862762451172 sec\n",
      "PyTorch postprocessing took 0.014057636260986328 sec\n",
      "Frame 1084 processing time: 0.0373 seconds\n",
      "PyTorch inference took 0.0037584304809570312 sec\n",
      "PyTorch postprocessing took 0.010305643081665039 sec\n",
      "Frame 1085 processing time: 0.0293 seconds\n",
      "PyTorch inference took 0.0057370662689208984 sec\n",
      "PyTorch postprocessing took 0.01077413558959961 sec\n",
      "Frame 1086 processing time: 0.0468 seconds\n",
      "PyTorch inference took 0.0032494068145751953 sec\n",
      "PyTorch postprocessing took 0.00922083854675293 sec\n",
      "Frame 1087 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.008980035781860352 sec\n",
      "PyTorch postprocessing took 0.009372472763061523 sec\n",
      "Frame 1088 processing time: 0.0380 seconds\n",
      "PyTorch inference took 0.007160186767578125 sec\n",
      "PyTorch postprocessing took 0.020261526107788086 sec\n",
      "Frame 1089 processing time: 0.0573 seconds\n",
      "PyTorch inference took 0.0057506561279296875 sec\n",
      "PyTorch postprocessing took 0.00898122787475586 sec\n",
      "Frame 1090 processing time: 0.0362 seconds\n",
      "PyTorch inference took 0.008748292922973633 sec\n",
      "PyTorch postprocessing took 0.009609460830688477 sec\n",
      "Frame 1091 processing time: 0.0428 seconds\n",
      "PyTorch inference took 0.0038161277770996094 sec\n",
      "PyTorch postprocessing took 0.009586811065673828 sec\n",
      "Frame 1092 processing time: 0.0280 seconds\n",
      "PyTorch inference took 0.007691860198974609 sec\n",
      "PyTorch postprocessing took 0.016519784927368164 sec\n",
      "Frame 1093 processing time: 0.0397 seconds\n",
      "PyTorch inference took 0.0037479400634765625 sec\n",
      "PyTorch postprocessing took 0.009590387344360352 sec\n",
      "Frame 1094 processing time: 0.0317 seconds\n",
      "PyTorch inference took 0.003953695297241211 sec\n",
      "PyTorch postprocessing took 0.009481430053710938 sec\n",
      "Frame 1095 processing time: 0.0330 seconds\n",
      "PyTorch inference took 0.003742218017578125 sec\n",
      "PyTorch postprocessing took 0.013484716415405273 sec\n",
      "Frame 1096 processing time: 0.0320 seconds\n",
      "PyTorch inference took 0.0037229061126708984 sec\n",
      "PyTorch postprocessing took 0.011633157730102539 sec\n",
      "Frame 1097 processing time: 0.0342 seconds\n",
      "PyTorch inference took 0.0031206607818603516 sec\n",
      "PyTorch postprocessing took 0.009803295135498047 sec\n",
      "Frame 1098 processing time: 0.0351 seconds\n",
      "PyTorch inference took 0.0030133724212646484 sec\n",
      "PyTorch postprocessing took 0.009521007537841797 sec\n",
      "Frame 1099 processing time: 0.0267 seconds\n",
      "PyTorch inference took 0.004016876220703125 sec\n",
      "PyTorch postprocessing took 0.015310525894165039 sec\n",
      "Frame 1100 processing time: 0.0373 seconds\n",
      "PyTorch inference took 0.0030617713928222656 sec\n",
      "PyTorch postprocessing took 0.0093841552734375 sec\n",
      "Frame 1101 processing time: 0.0281 seconds\n",
      "PyTorch inference took 0.0028867721557617188 sec\n",
      "PyTorch postprocessing took 0.009513378143310547 sec\n",
      "Frame 1102 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.003759145736694336 sec\n",
      "PyTorch postprocessing took 0.00880289077758789 sec\n",
      "Frame 1103 processing time: 0.0252 seconds\n",
      "PyTorch inference took 0.002933502197265625 sec\n",
      "PyTorch postprocessing took 0.009594440460205078 sec\n",
      "Frame 1104 processing time: 0.0247 seconds\n",
      "PyTorch inference took 0.002936840057373047 sec\n",
      "PyTorch postprocessing took 0.009525537490844727 sec\n",
      "Frame 1105 processing time: 0.0279 seconds\n",
      "PyTorch inference took 0.002991199493408203 sec\n",
      "PyTorch postprocessing took 0.009418487548828125 sec\n",
      "Frame 1106 processing time: 0.0304 seconds\n",
      "PyTorch inference took 0.0035479068756103516 sec\n",
      "PyTorch postprocessing took 0.008895635604858398 sec\n",
      "Frame 1107 processing time: 0.0315 seconds\n",
      "PyTorch inference took 0.004272937774658203 sec\n",
      "PyTorch postprocessing took 0.009093284606933594 sec\n",
      "Frame 1108 processing time: 0.0300 seconds\n",
      "PyTorch inference took 0.002990245819091797 sec\n",
      "PyTorch postprocessing took 0.009476661682128906 sec\n",
      "Frame 1109 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.004975795745849609 sec\n",
      "PyTorch postprocessing took 0.010843038558959961 sec\n",
      "Frame 1110 processing time: 0.0410 seconds\n",
      "PyTorch inference took 0.008504629135131836 sec\n",
      "PyTorch postprocessing took 0.009595155715942383 sec\n",
      "Frame 1111 processing time: 0.0369 seconds\n",
      "PyTorch inference took 0.007456302642822266 sec\n",
      "PyTorch postprocessing took 0.006197214126586914 sec\n",
      "Frame 1112 processing time: 0.0300 seconds\n",
      "PyTorch inference took 0.0028874874114990234 sec\n",
      "PyTorch postprocessing took 0.009584665298461914 sec\n",
      "Frame 1113 processing time: 0.0256 seconds\n",
      "PyTorch inference took 0.008308172225952148 sec\n",
      "PyTorch postprocessing took 0.00949406623840332 sec\n",
      "Frame 1114 processing time: 0.0334 seconds\n",
      "PyTorch inference took 0.0029897689819335938 sec\n",
      "PyTorch postprocessing took 0.009466409683227539 sec\n",
      "Frame 1115 processing time: 0.0298 seconds\n",
      "PyTorch inference took 0.004357337951660156 sec\n",
      "PyTorch postprocessing took 0.00952601432800293 sec\n",
      "Frame 1116 processing time: 0.0326 seconds\n",
      "PyTorch inference took 0.0044972896575927734 sec\n",
      "PyTorch postprocessing took 0.010661602020263672 sec\n",
      "Frame 1117 processing time: 0.0350 seconds\n",
      "PyTorch inference took 0.004997730255126953 sec\n",
      "PyTorch postprocessing took 0.008872509002685547 sec\n",
      "Frame 1118 processing time: 0.0295 seconds\n",
      "PyTorch inference took 0.004873752593994141 sec\n",
      "PyTorch postprocessing took 0.017080068588256836 sec\n",
      "Frame 1119 processing time: 0.0374 seconds\n",
      "PyTorch inference took 0.003010988235473633 sec\n",
      "PyTorch postprocessing took 0.0099639892578125 sec\n",
      "Frame 1120 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.002852201461791992 sec\n",
      "PyTorch postprocessing took 0.011049747467041016 sec\n",
      "Frame 1121 processing time: 0.0275 seconds\n",
      "PyTorch inference took 0.002941131591796875 sec\n",
      "PyTorch postprocessing took 0.009988546371459961 sec\n",
      "Frame 1122 processing time: 0.0308 seconds\n",
      "PyTorch inference took 0.007403850555419922 sec\n",
      "PyTorch postprocessing took 0.01629924774169922 sec\n",
      "Frame 1123 processing time: 0.0576 seconds\n",
      "PyTorch inference took 0.004377841949462891 sec\n",
      "PyTorch postprocessing took 0.008055686950683594 sec\n",
      "Frame 1124 processing time: 0.0261 seconds\n",
      "PyTorch inference took 0.0043735504150390625 sec\n",
      "PyTorch postprocessing took 0.008551836013793945 sec\n",
      "Frame 1125 processing time: 0.0276 seconds\n",
      "PyTorch inference took 0.0031003952026367188 sec\n",
      "PyTorch postprocessing took 0.009520769119262695 sec\n",
      "Frame 1126 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.003571033477783203 sec\n",
      "PyTorch postprocessing took 0.009229183197021484 sec\n",
      "Frame 1127 processing time: 0.0276 seconds\n",
      "PyTorch inference took 0.002942800521850586 sec\n",
      "PyTorch postprocessing took 0.009487152099609375 sec\n",
      "Frame 1128 processing time: 0.0245 seconds\n",
      "PyTorch inference took 0.003949642181396484 sec\n",
      "PyTorch postprocessing took 0.00948333740234375 sec\n",
      "Frame 1129 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.00413966178894043 sec\n",
      "PyTorch postprocessing took 0.015111684799194336 sec\n",
      "Frame 1130 processing time: 0.0389 seconds\n",
      "PyTorch inference took 0.007769107818603516 sec\n",
      "PyTorch postprocessing took 0.011740684509277344 sec\n",
      "Frame 1131 processing time: 0.0531 seconds\n",
      "PyTorch inference took 0.0029370784759521484 sec\n",
      "PyTorch postprocessing took 0.009510278701782227 sec\n",
      "Frame 1132 processing time: 0.0299 seconds\n",
      "PyTorch inference took 0.003738880157470703 sec\n",
      "PyTorch postprocessing took 0.013579607009887695 sec\n",
      "Frame 1133 processing time: 0.0352 seconds\n",
      "PyTorch inference took 0.007925987243652344 sec\n",
      "PyTorch postprocessing took 0.019187450408935547 sec\n",
      "Frame 1134 processing time: 0.0463 seconds\n",
      "PyTorch inference took 0.002907991409301758 sec\n",
      "PyTorch postprocessing took 0.009624719619750977 sec\n",
      "Frame 1135 processing time: 0.0306 seconds\n",
      "PyTorch inference took 0.006707906723022461 sec\n",
      "PyTorch postprocessing took 0.005636453628540039 sec\n",
      "Frame 1136 processing time: 0.0329 seconds\n",
      "PyTorch inference took 0.007311105728149414 sec\n",
      "PyTorch postprocessing took 0.01674962043762207 sec\n",
      "Frame 1137 processing time: 0.0556 seconds\n",
      "PyTorch inference took 0.005557537078857422 sec\n",
      "PyTorch postprocessing took 0.014650344848632812 sec\n",
      "Frame 1138 processing time: 0.0472 seconds\n",
      "PyTorch inference took 0.0053424835205078125 sec\n",
      "PyTorch postprocessing took 0.010207653045654297 sec\n",
      "Frame 1139 processing time: 0.0549 seconds\n",
      "PyTorch inference took 0.002942800521850586 sec\n",
      "PyTorch postprocessing took 0.009536266326904297 sec\n",
      "Frame 1140 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.0037689208984375 sec\n",
      "PyTorch postprocessing took 0.010035514831542969 sec\n",
      "Frame 1141 processing time: 0.0284 seconds\n",
      "PyTorch inference took 0.0038983821868896484 sec\n",
      "PyTorch postprocessing took 0.010121822357177734 sec\n",
      "Frame 1142 processing time: 0.0362 seconds\n",
      "PyTorch inference took 0.005204916000366211 sec\n",
      "PyTorch postprocessing took 0.015310525894165039 sec\n",
      "Frame 1143 processing time: 0.0435 seconds\n",
      "PyTorch inference took 0.0030655860900878906 sec\n",
      "PyTorch postprocessing took 0.00989985466003418 sec\n",
      "Frame 1144 processing time: 0.0342 seconds\n",
      "PyTorch inference took 0.006364583969116211 sec\n",
      "PyTorch postprocessing took 0.008514404296875 sec\n",
      "Frame 1145 processing time: 0.0440 seconds\n",
      "PyTorch inference took 0.032109737396240234 sec\n",
      "PyTorch postprocessing took 0.009845256805419922 sec\n",
      "Frame 1146 processing time: 0.0741 seconds\n",
      "PyTorch inference took 0.007883787155151367 sec\n",
      "PyTorch postprocessing took 0.011865377426147461 sec\n",
      "Frame 1147 processing time: 0.0457 seconds\n",
      "PyTorch inference took 0.0062220096588134766 sec\n",
      "PyTorch postprocessing took 0.009551048278808594 sec\n",
      "Frame 1148 processing time: 0.0332 seconds\n",
      "PyTorch inference took 0.003778219223022461 sec\n",
      "PyTorch postprocessing took 0.015167951583862305 sec\n",
      "Frame 1149 processing time: 0.0401 seconds\n",
      "PyTorch inference took 0.0096588134765625 sec\n",
      "PyTorch postprocessing took 0.019379377365112305 sec\n",
      "Frame 1150 processing time: 0.0705 seconds\n",
      "PyTorch inference took 0.0040285587310791016 sec\n",
      "PyTorch postprocessing took 0.009154319763183594 sec\n",
      "Frame 1151 processing time: 0.0299 seconds\n",
      "PyTorch inference took 0.006926774978637695 sec\n",
      "PyTorch postprocessing took 0.007787942886352539 sec\n",
      "Frame 1152 processing time: 0.0365 seconds\n",
      "PyTorch inference took 0.00296783447265625 sec\n",
      "PyTorch postprocessing took 0.009809494018554688 sec\n",
      "Frame 1153 processing time: 0.0290 seconds\n",
      "PyTorch inference took 0.004391670227050781 sec\n",
      "PyTorch postprocessing took 0.009685277938842773 sec\n",
      "Frame 1154 processing time: 0.0328 seconds\n",
      "PyTorch inference took 0.003277301788330078 sec\n",
      "PyTorch postprocessing took 0.009725332260131836 sec\n",
      "Frame 1155 processing time: 0.0333 seconds\n",
      "PyTorch inference took 0.003111124038696289 sec\n",
      "PyTorch postprocessing took 0.00974583625793457 sec\n",
      "Frame 1156 processing time: 0.0339 seconds\n",
      "PyTorch inference took 0.0049817562103271484 sec\n",
      "PyTorch postprocessing took 0.014845609664916992 sec\n",
      "Frame 1157 processing time: 0.0391 seconds\n",
      "PyTorch inference took 0.0034646987915039062 sec\n",
      "PyTorch postprocessing took 0.009426593780517578 sec\n",
      "Frame 1158 processing time: 0.0277 seconds\n",
      "PyTorch inference took 0.011713504791259766 sec\n",
      "PyTorch postprocessing took 0.01773977279663086 sec\n",
      "Frame 1159 processing time: 0.0475 seconds\n",
      "PyTorch inference took 0.003091573715209961 sec\n",
      "PyTorch postprocessing took 0.009424209594726562 sec\n",
      "Frame 1160 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.0029311180114746094 sec\n",
      "PyTorch postprocessing took 0.009610891342163086 sec\n",
      "Frame 1161 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.003930330276489258 sec\n",
      "PyTorch postprocessing took 0.009100675582885742 sec\n",
      "Frame 1162 processing time: 0.0251 seconds\n",
      "PyTorch inference took 0.0029277801513671875 sec\n",
      "PyTorch postprocessing took 0.009559869766235352 sec\n",
      "Frame 1163 processing time: 0.0279 seconds\n",
      "PyTorch inference took 0.003845691680908203 sec\n",
      "PyTorch postprocessing took 0.009517908096313477 sec\n",
      "Frame 1164 processing time: 0.0280 seconds\n",
      "PyTorch inference took 0.0047414302825927734 sec\n",
      "PyTorch postprocessing took 0.008841753005981445 sec\n",
      "Frame 1165 processing time: 0.0286 seconds\n",
      "PyTorch inference took 0.0028846263885498047 sec\n",
      "PyTorch postprocessing took 0.009648799896240234 sec\n",
      "Frame 1166 processing time: 0.0279 seconds\n",
      "PyTorch inference took 0.0040438175201416016 sec\n",
      "PyTorch postprocessing took 0.015840530395507812 sec\n",
      "Frame 1167 processing time: 0.0374 seconds\n",
      "PyTorch inference took 0.003809690475463867 sec\n",
      "PyTorch postprocessing took 0.009509563446044922 sec\n",
      "Frame 1168 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.003519773483276367 sec\n",
      "PyTorch postprocessing took 0.00896906852722168 sec\n",
      "Frame 1169 processing time: 0.0269 seconds\n",
      "PyTorch inference took 0.0030744075775146484 sec\n",
      "PyTorch postprocessing took 0.009416341781616211 sec\n",
      "Frame 1170 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.0029032230377197266 sec\n",
      "PyTorch postprocessing took 0.009551763534545898 sec\n",
      "Frame 1171 processing time: 0.0286 seconds\n",
      "PyTorch inference took 0.004482746124267578 sec\n",
      "PyTorch postprocessing took 0.009669303894042969 sec\n",
      "Frame 1172 processing time: 0.0341 seconds\n",
      "PyTorch inference took 0.0029668807983398438 sec\n",
      "PyTorch postprocessing took 0.009466886520385742 sec\n",
      "Frame 1173 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.0030562877655029297 sec\n",
      "PyTorch postprocessing took 0.00948476791381836 sec\n",
      "Frame 1174 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.0029554367065429688 sec\n",
      "PyTorch postprocessing took 0.009437084197998047 sec\n",
      "Frame 1175 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.0032105445861816406 sec\n",
      "PyTorch postprocessing took 0.009190797805786133 sec\n",
      "Frame 1176 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.00301361083984375 sec\n",
      "PyTorch postprocessing took 0.009474515914916992 sec\n",
      "Frame 1177 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.004425048828125 sec\n",
      "PyTorch postprocessing took 0.00797128677368164 sec\n",
      "Frame 1178 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.003025054931640625 sec\n",
      "PyTorch postprocessing took 0.00937795639038086 sec\n",
      "Frame 1179 processing time: 0.0257 seconds\n",
      "PyTorch inference took 0.00898122787475586 sec\n",
      "PyTorch postprocessing took 0.008031845092773438 sec\n",
      "Frame 1180 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.005295991897583008 sec\n",
      "PyTorch postprocessing took 0.01106119155883789 sec\n",
      "Frame 1181 processing time: 0.0563 seconds\n",
      "PyTorch inference took 0.0038046836853027344 sec\n",
      "PyTorch postprocessing took 0.009310007095336914 sec\n",
      "Frame 1182 processing time: 0.0269 seconds\n",
      "PyTorch inference took 0.0038695335388183594 sec\n",
      "PyTorch postprocessing took 0.00948643684387207 sec\n",
      "Frame 1183 processing time: 0.0323 seconds\n",
      "PyTorch inference took 0.0032896995544433594 sec\n",
      "PyTorch postprocessing took 0.009145021438598633 sec\n",
      "Frame 1184 processing time: 0.0306 seconds\n",
      "PyTorch inference took 0.0029594898223876953 sec\n",
      "PyTorch postprocessing took 0.009501457214355469 sec\n",
      "Frame 1185 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.005795955657958984 sec\n",
      "PyTorch postprocessing took 0.00768280029296875 sec\n",
      "Frame 1186 processing time: 0.0454 seconds\n",
      "PyTorch inference took 0.003659963607788086 sec\n",
      "PyTorch postprocessing took 0.011108160018920898 sec\n",
      "Frame 1187 processing time: 0.0461 seconds\n",
      "PyTorch inference took 0.002899169921875 sec\n",
      "PyTorch postprocessing took 0.009560823440551758 sec\n",
      "Frame 1188 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.0030202865600585938 sec\n",
      "PyTorch postprocessing took 0.009475231170654297 sec\n",
      "Frame 1189 processing time: 0.0252 seconds\n",
      "PyTorch inference took 0.002917766571044922 sec\n",
      "PyTorch postprocessing took 0.00952458381652832 sec\n",
      "Frame 1190 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.003062725067138672 sec\n",
      "PyTorch postprocessing took 0.009439229965209961 sec\n",
      "Frame 1191 processing time: 0.0278 seconds\n",
      "PyTorch inference took 0.0029904842376708984 sec\n",
      "PyTorch postprocessing took 0.00950169563293457 sec\n",
      "Frame 1192 processing time: 0.0261 seconds\n",
      "PyTorch inference took 0.006011009216308594 sec\n",
      "PyTorch postprocessing took 0.009479999542236328 sec\n",
      "Frame 1193 processing time: 0.0355 seconds\n",
      "PyTorch inference took 0.0029146671295166016 sec\n",
      "PyTorch postprocessing took 0.010662555694580078 sec\n",
      "Frame 1194 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.0029654502868652344 sec\n",
      "PyTorch postprocessing took 0.00955343246459961 sec\n",
      "Frame 1195 processing time: 0.0308 seconds\n",
      "PyTorch inference took 0.0037429332733154297 sec\n",
      "PyTorch postprocessing took 0.009332418441772461 sec\n",
      "Frame 1196 processing time: 0.0303 seconds\n",
      "PyTorch inference took 0.003011941909790039 sec\n",
      "PyTorch postprocessing took 0.00972127914428711 sec\n",
      "Frame 1197 processing time: 0.0296 seconds\n",
      "PyTorch inference took 0.002943754196166992 sec\n",
      "PyTorch postprocessing took 0.00950002670288086 sec\n",
      "Frame 1198 processing time: 0.0267 seconds\n",
      "PyTorch inference took 0.00594019889831543 sec\n",
      "PyTorch postprocessing took 0.006595134735107422 sec\n",
      "Frame 1199 processing time: 0.0418 seconds\n",
      "PyTorch inference took 0.003906965255737305 sec\n",
      "PyTorch postprocessing took 0.00939035415649414 sec\n",
      "Frame 1200 processing time: 0.0279 seconds\n",
      "PyTorch inference took 0.004279375076293945 sec\n",
      "PyTorch postprocessing took 0.009247064590454102 sec\n",
      "Frame 1201 processing time: 0.0382 seconds\n",
      "PyTorch inference took 0.0038139820098876953 sec\n",
      "PyTorch postprocessing took 0.009517431259155273 sec\n",
      "Frame 1202 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.004003286361694336 sec\n",
      "PyTorch postprocessing took 0.009172677993774414 sec\n",
      "Frame 1203 processing time: 0.0316 seconds\n",
      "PyTorch inference took 0.0038175582885742188 sec\n",
      "PyTorch postprocessing took 0.014503002166748047 sec\n",
      "Frame 1204 processing time: 0.0334 seconds\n",
      "PyTorch inference took 0.0036728382110595703 sec\n",
      "PyTorch postprocessing took 0.009698629379272461 sec\n",
      "Frame 1205 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.003760099411010742 sec\n",
      "PyTorch postprocessing took 0.00969696044921875 sec\n",
      "Frame 1206 processing time: 0.0272 seconds\n",
      "PyTorch inference took 0.006348848342895508 sec\n",
      "PyTorch postprocessing took 0.013142824172973633 sec\n",
      "Frame 1207 processing time: 0.0409 seconds\n",
      "PyTorch inference took 0.004555225372314453 sec\n",
      "PyTorch postprocessing took 0.009294986724853516 sec\n",
      "Frame 1208 processing time: 0.0289 seconds\n",
      "PyTorch inference took 0.007254600524902344 sec\n",
      "PyTorch postprocessing took 0.006183147430419922 sec\n",
      "Frame 1209 processing time: 0.0361 seconds\n",
      "PyTorch inference took 0.002888917922973633 sec\n",
      "PyTorch postprocessing took 0.009502172470092773 sec\n",
      "Frame 1210 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.006635189056396484 sec\n",
      "PyTorch postprocessing took 0.012163162231445312 sec\n",
      "Frame 1211 processing time: 0.0343 seconds\n",
      "PyTorch inference took 0.0037217140197753906 sec\n",
      "PyTorch postprocessing took 0.014198780059814453 sec\n",
      "Frame 1212 processing time: 0.0334 seconds\n",
      "PyTorch inference took 0.005722522735595703 sec\n",
      "PyTorch postprocessing took 0.008060455322265625 sec\n",
      "Frame 1213 processing time: 0.0505 seconds\n",
      "PyTorch inference took 0.0028905868530273438 sec\n",
      "PyTorch postprocessing took 0.009563446044921875 sec\n",
      "Frame 1214 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.003748655319213867 sec\n",
      "PyTorch postprocessing took 0.013097524642944336 sec\n",
      "Frame 1215 processing time: 0.0332 seconds\n",
      "PyTorch inference took 0.00879359245300293 sec\n",
      "PyTorch postprocessing took 0.010465383529663086 sec\n",
      "Frame 1216 processing time: 0.0633 seconds\n",
      "PyTorch inference took 0.005489826202392578 sec\n",
      "PyTorch postprocessing took 0.007221221923828125 sec\n",
      "Frame 1217 processing time: 0.0427 seconds\n",
      "PyTorch inference took 0.007898807525634766 sec\n",
      "PyTorch postprocessing took 0.012863397598266602 sec\n",
      "Frame 1218 processing time: 0.0464 seconds\n",
      "PyTorch inference took 0.003676176071166992 sec\n",
      "PyTorch postprocessing took 0.008747100830078125 sec\n",
      "Frame 1219 processing time: 0.0247 seconds\n",
      "PyTorch inference took 0.0038073062896728516 sec\n",
      "PyTorch postprocessing took 0.008678436279296875 sec\n",
      "Frame 1220 processing time: 0.0246 seconds\n",
      "PyTorch inference took 0.0029137134552001953 sec\n",
      "PyTorch postprocessing took 0.009563207626342773 sec\n",
      "Frame 1221 processing time: 0.0244 seconds\n",
      "PyTorch inference took 0.0029315948486328125 sec\n",
      "PyTorch postprocessing took 0.009497404098510742 sec\n",
      "Frame 1222 processing time: 0.0286 seconds\n",
      "PyTorch inference took 0.003851652145385742 sec\n",
      "PyTorch postprocessing took 0.013182401657104492 sec\n",
      "Frame 1223 processing time: 0.0322 seconds\n",
      "PyTorch inference took 0.0029239654541015625 sec\n",
      "PyTorch postprocessing took 0.009510278701782227 sec\n",
      "Frame 1224 processing time: 0.0246 seconds\n",
      "PyTorch inference took 0.0028831958770751953 sec\n",
      "PyTorch postprocessing took 0.009532451629638672 sec\n",
      "Frame 1225 processing time: 0.0293 seconds\n",
      "PyTorch inference took 0.006546497344970703 sec\n",
      "PyTorch postprocessing took 0.01276707649230957 sec\n",
      "Frame 1226 processing time: 0.0441 seconds\n",
      "PyTorch inference took 0.003694772720336914 sec\n",
      "PyTorch postprocessing took 0.008745193481445312 sec\n",
      "Frame 1227 processing time: 0.0252 seconds\n",
      "PyTorch inference took 0.0030138492584228516 sec\n",
      "PyTorch postprocessing took 0.009461402893066406 sec\n",
      "Frame 1228 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.002942800521850586 sec\n",
      "PyTorch postprocessing took 0.009604215621948242 sec\n",
      "Frame 1229 processing time: 0.0293 seconds\n",
      "PyTorch inference took 0.002933025360107422 sec\n",
      "PyTorch postprocessing took 0.009560823440551758 sec\n",
      "Frame 1230 processing time: 0.0266 seconds\n",
      "PyTorch inference took 0.002909421920776367 sec\n",
      "PyTorch postprocessing took 0.009600639343261719 sec\n",
      "Frame 1231 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.0031473636627197266 sec\n",
      "PyTorch postprocessing took 0.009723186492919922 sec\n",
      "Frame 1232 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.0029485225677490234 sec\n",
      "PyTorch postprocessing took 0.009508848190307617 sec\n",
      "Frame 1233 processing time: 0.0257 seconds\n",
      "PyTorch inference took 0.0029425621032714844 sec\n",
      "PyTorch postprocessing took 0.009541988372802734 sec\n",
      "Frame 1234 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.002935647964477539 sec\n",
      "PyTorch postprocessing took 0.009610414505004883 sec\n",
      "Frame 1235 processing time: 0.0248 seconds\n",
      "PyTorch inference took 0.002894163131713867 sec\n",
      "PyTorch postprocessing took 0.009566307067871094 sec\n",
      "Frame 1236 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.0029850006103515625 sec\n",
      "PyTorch postprocessing took 0.00950765609741211 sec\n",
      "Frame 1237 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.0028896331787109375 sec\n",
      "PyTorch postprocessing took 0.009606599807739258 sec\n",
      "Frame 1238 processing time: 0.0248 seconds\n",
      "PyTorch inference took 0.002877473831176758 sec\n",
      "PyTorch postprocessing took 0.009635448455810547 sec\n",
      "Frame 1239 processing time: 0.0308 seconds\n",
      "PyTorch inference took 0.002986907958984375 sec\n",
      "PyTorch postprocessing took 0.009518861770629883 sec\n",
      "Frame 1240 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.0029137134552001953 sec\n",
      "PyTorch postprocessing took 0.00958704948425293 sec\n",
      "Frame 1241 processing time: 0.0247 seconds\n",
      "PyTorch inference took 0.002946615219116211 sec\n",
      "PyTorch postprocessing took 0.009540081024169922 sec\n",
      "Frame 1242 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.0029337406158447266 sec\n",
      "PyTorch postprocessing took 0.009586095809936523 sec\n",
      "Frame 1243 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.0028548240661621094 sec\n",
      "PyTorch postprocessing took 0.009534597396850586 sec\n",
      "Frame 1244 processing time: 0.0245 seconds\n",
      "PyTorch inference took 0.003057241439819336 sec\n",
      "PyTorch postprocessing took 0.009435176849365234 sec\n",
      "Frame 1245 processing time: 0.0247 seconds\n",
      "PyTorch inference took 0.002950429916381836 sec\n",
      "PyTorch postprocessing took 0.009726762771606445 sec\n",
      "Frame 1246 processing time: 0.0248 seconds\n",
      "PyTorch inference took 0.002912759780883789 sec\n",
      "PyTorch postprocessing took 0.009549856185913086 sec\n",
      "Frame 1247 processing time: 0.0246 seconds\n",
      "PyTorch inference took 0.0028705596923828125 sec\n",
      "PyTorch postprocessing took 0.009679794311523438 sec\n",
      "Frame 1248 processing time: 0.0243 seconds\n",
      "PyTorch inference took 0.0029714107513427734 sec\n",
      "PyTorch postprocessing took 0.010065317153930664 sec\n",
      "Frame 1249 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.002902984619140625 sec\n",
      "PyTorch postprocessing took 0.009546041488647461 sec\n",
      "Frame 1250 processing time: 0.0245 seconds\n",
      "PyTorch inference took 0.0028824806213378906 sec\n",
      "PyTorch postprocessing took 0.009641170501708984 sec\n",
      "Frame 1251 processing time: 0.0244 seconds\n",
      "PyTorch inference took 0.0029714107513427734 sec\n",
      "PyTorch postprocessing took 0.009496212005615234 sec\n",
      "Frame 1252 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.002895355224609375 sec\n",
      "PyTorch postprocessing took 0.009501218795776367 sec\n",
      "Frame 1253 processing time: 0.0240 seconds\n",
      "PyTorch inference took 0.003124237060546875 sec\n",
      "PyTorch postprocessing took 0.009431838989257812 sec\n",
      "Frame 1254 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.0028955936431884766 sec\n",
      "PyTorch postprocessing took 0.00955963134765625 sec\n",
      "Frame 1255 processing time: 0.0241 seconds\n",
      "PyTorch inference took 0.002901792526245117 sec\n",
      "PyTorch postprocessing took 0.009626388549804688 sec\n",
      "Frame 1256 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.004338264465332031 sec\n",
      "PyTorch postprocessing took 0.008178949356079102 sec\n",
      "Frame 1257 processing time: 0.0277 seconds\n",
      "PyTorch inference took 0.002900362014770508 sec\n",
      "PyTorch postprocessing took 0.009534835815429688 sec\n",
      "Frame 1258 processing time: 0.0281 seconds\n",
      "PyTorch inference took 0.0029125213623046875 sec\n",
      "PyTorch postprocessing took 0.009525299072265625 sec\n",
      "Frame 1259 processing time: 0.0284 seconds\n",
      "PyTorch inference took 0.0030133724212646484 sec\n",
      "PyTorch postprocessing took 0.009486198425292969 sec\n",
      "Frame 1260 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.0029244422912597656 sec\n",
      "PyTorch postprocessing took 0.009530067443847656 sec\n",
      "Frame 1261 processing time: 0.0251 seconds\n",
      "PyTorch inference took 0.0029439926147460938 sec\n",
      "PyTorch postprocessing took 0.009618997573852539 sec\n",
      "Frame 1262 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.0029561519622802734 sec\n",
      "PyTorch postprocessing took 0.009574413299560547 sec\n",
      "Frame 1263 processing time: 0.0247 seconds\n",
      "PyTorch inference took 0.002912759780883789 sec\n",
      "PyTorch postprocessing took 0.00948643684387207 sec\n",
      "Frame 1264 processing time: 0.0247 seconds\n",
      "PyTorch inference took 0.004332780838012695 sec\n",
      "PyTorch postprocessing took 0.008211135864257812 sec\n",
      "Frame 1265 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.0029511451721191406 sec\n",
      "PyTorch postprocessing took 0.009579896926879883 sec\n",
      "Frame 1266 processing time: 0.0255 seconds\n",
      "PyTorch inference took 0.0029458999633789062 sec\n",
      "PyTorch postprocessing took 0.009525299072265625 sec\n",
      "Frame 1267 processing time: 0.0251 seconds\n",
      "PyTorch inference took 0.0029191970825195312 sec\n",
      "PyTorch postprocessing took 0.010149002075195312 sec\n",
      "Frame 1268 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.002918720245361328 sec\n",
      "PyTorch postprocessing took 0.00960087776184082 sec\n",
      "Frame 1269 processing time: 0.0247 seconds\n",
      "PyTorch inference took 0.002861499786376953 sec\n",
      "PyTorch postprocessing took 0.009541034698486328 sec\n",
      "Frame 1270 processing time: 0.0297 seconds\n",
      "PyTorch inference took 0.0028977394104003906 sec\n",
      "PyTorch postprocessing took 0.009606599807739258 sec\n",
      "Frame 1271 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.0028846263885498047 sec\n",
      "PyTorch postprocessing took 0.009578227996826172 sec\n",
      "Frame 1272 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.003667116165161133 sec\n",
      "PyTorch postprocessing took 0.009378910064697266 sec\n",
      "Frame 1273 processing time: 0.0248 seconds\n",
      "PyTorch inference took 0.0029273033142089844 sec\n",
      "PyTorch postprocessing took 0.009572505950927734 sec\n",
      "Frame 1274 processing time: 0.0252 seconds\n",
      "PyTorch inference took 0.002905130386352539 sec\n",
      "PyTorch postprocessing took 0.00959157943725586 sec\n",
      "Frame 1275 processing time: 0.0282 seconds\n",
      "PyTorch inference took 0.0029456615447998047 sec\n",
      "PyTorch postprocessing took 0.009539365768432617 sec\n",
      "Frame 1276 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.0029306411743164062 sec\n",
      "PyTorch postprocessing took 0.009592533111572266 sec\n",
      "Frame 1277 processing time: 0.0245 seconds\n",
      "PyTorch inference took 0.0029397010803222656 sec\n",
      "PyTorch postprocessing took 0.009430885314941406 sec\n",
      "Frame 1278 processing time: 0.0267 seconds\n",
      "PyTorch inference took 0.0030128955841064453 sec\n",
      "PyTorch postprocessing took 0.009446382522583008 sec\n",
      "Frame 1279 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.0029747486114501953 sec\n",
      "PyTorch postprocessing took 0.009542703628540039 sec\n",
      "Frame 1280 processing time: 0.0299 seconds\n",
      "PyTorch inference took 0.003690481185913086 sec\n",
      "PyTorch postprocessing took 0.017328262329101562 sec\n",
      "Frame 1281 processing time: 0.0450 seconds\n",
      "PyTorch inference took 0.0029108524322509766 sec\n",
      "PyTorch postprocessing took 0.009564876556396484 sec\n",
      "Frame 1282 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.0029366016387939453 sec\n",
      "PyTorch postprocessing took 0.009553670883178711 sec\n",
      "Frame 1283 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.007104158401489258 sec\n",
      "PyTorch postprocessing took 0.009200572967529297 sec\n",
      "Frame 1284 processing time: 0.0321 seconds\n",
      "PyTorch inference took 0.0037953853607177734 sec\n",
      "PyTorch postprocessing took 0.010542154312133789 sec\n",
      "Frame 1285 processing time: 0.0356 seconds\n",
      "PyTorch inference took 0.0037953853607177734 sec\n",
      "PyTorch postprocessing took 0.01444697380065918 sec\n",
      "Frame 1286 processing time: 0.0367 seconds\n",
      "PyTorch inference took 0.0037355422973632812 sec\n",
      "PyTorch postprocessing took 0.01493382453918457 sec\n",
      "Frame 1287 processing time: 0.0391 seconds\n",
      "PyTorch inference took 0.004904270172119141 sec\n",
      "PyTorch postprocessing took 0.021158933639526367 sec\n",
      "Frame 1288 processing time: 0.0434 seconds\n",
      "PyTorch inference took 0.005568742752075195 sec\n",
      "PyTorch postprocessing took 0.010549783706665039 sec\n",
      "Frame 1289 processing time: 0.0301 seconds\n",
      "PyTorch inference took 0.0044803619384765625 sec\n",
      "PyTorch postprocessing took 0.009044885635375977 sec\n",
      "Frame 1290 processing time: 0.0303 seconds\n",
      "PyTorch inference took 0.002910137176513672 sec\n",
      "PyTorch postprocessing took 0.01053166389465332 sec\n",
      "Frame 1291 processing time: 0.0278 seconds\n",
      "PyTorch inference took 0.0029697418212890625 sec\n",
      "PyTorch postprocessing took 0.009429216384887695 sec\n",
      "Frame 1292 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.003108501434326172 sec\n",
      "PyTorch postprocessing took 0.010926246643066406 sec\n",
      "Frame 1293 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.0036482810974121094 sec\n",
      "PyTorch postprocessing took 0.009652853012084961 sec\n",
      "Frame 1294 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.0029702186584472656 sec\n",
      "PyTorch postprocessing took 0.009471654891967773 sec\n",
      "Frame 1295 processing time: 0.0252 seconds\n",
      "PyTorch inference took 0.002958059310913086 sec\n",
      "PyTorch postprocessing took 0.009488582611083984 sec\n",
      "Frame 1296 processing time: 0.0261 seconds\n",
      "PyTorch inference took 0.004450082778930664 sec\n",
      "PyTorch postprocessing took 0.00918126106262207 sec\n",
      "Frame 1297 processing time: 0.0275 seconds\n",
      "PyTorch inference took 0.0028977394104003906 sec\n",
      "PyTorch postprocessing took 0.00952768325805664 sec\n",
      "Frame 1298 processing time: 0.0248 seconds\n",
      "PyTorch inference took 0.0029954910278320312 sec\n",
      "PyTorch postprocessing took 0.009589910507202148 sec\n",
      "Frame 1299 processing time: 0.0255 seconds\n",
      "PyTorch inference took 0.004495859146118164 sec\n",
      "PyTorch postprocessing took 0.00905466079711914 sec\n",
      "Frame 1300 processing time: 0.0275 seconds\n",
      "PyTorch inference took 0.002965688705444336 sec\n",
      "PyTorch postprocessing took 0.009540081024169922 sec\n",
      "Frame 1301 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.002825498580932617 sec\n",
      "PyTorch postprocessing took 0.010260820388793945 sec\n",
      "Frame 1302 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.002959728240966797 sec\n",
      "PyTorch postprocessing took 0.009521961212158203 sec\n",
      "Frame 1303 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.004461050033569336 sec\n",
      "PyTorch postprocessing took 0.009183645248413086 sec\n",
      "Frame 1304 processing time: 0.0277 seconds\n",
      "PyTorch inference took 0.004472255706787109 sec\n",
      "PyTorch postprocessing took 0.008074045181274414 sec\n",
      "Frame 1305 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.00319671630859375 sec\n",
      "PyTorch postprocessing took 0.009831428527832031 sec\n",
      "Frame 1306 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.0029535293579101562 sec\n",
      "PyTorch postprocessing took 0.009439706802368164 sec\n",
      "Frame 1307 processing time: 0.0261 seconds\n",
      "PyTorch inference took 0.003912687301635742 sec\n",
      "PyTorch postprocessing took 0.008611917495727539 sec\n",
      "Frame 1308 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.0029726028442382812 sec\n",
      "PyTorch postprocessing took 0.009510517120361328 sec\n",
      "Frame 1309 processing time: 0.0269 seconds\n",
      "PyTorch inference took 0.0029726028442382812 sec\n",
      "PyTorch postprocessing took 0.009466409683227539 sec\n",
      "Frame 1310 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.0029304027557373047 sec\n",
      "PyTorch postprocessing took 0.009480953216552734 sec\n",
      "Frame 1311 processing time: 0.0265 seconds\n",
      "PyTorch inference took 0.003148317337036133 sec\n",
      "PyTorch postprocessing took 0.00936269760131836 sec\n",
      "Frame 1312 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.0030205249786376953 sec\n",
      "PyTorch postprocessing took 0.009371519088745117 sec\n",
      "Frame 1313 processing time: 0.0256 seconds\n",
      "PyTorch inference took 0.0030024051666259766 sec\n",
      "PyTorch postprocessing took 0.009408712387084961 sec\n",
      "Frame 1314 processing time: 0.0261 seconds\n",
      "PyTorch inference took 0.0031175613403320312 sec\n",
      "PyTorch postprocessing took 0.009295940399169922 sec\n",
      "Frame 1315 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.0029463768005371094 sec\n",
      "PyTorch postprocessing took 0.009551286697387695 sec\n",
      "Frame 1316 processing time: 0.0255 seconds\n",
      "PyTorch inference took 0.0030412673950195312 sec\n",
      "PyTorch postprocessing took 0.009443521499633789 sec\n",
      "Frame 1317 processing time: 0.0251 seconds\n",
      "PyTorch inference took 0.0030448436737060547 sec\n",
      "PyTorch postprocessing took 0.00945138931274414 sec\n",
      "Frame 1318 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.0029668807983398438 sec\n",
      "PyTorch postprocessing took 0.009482145309448242 sec\n",
      "Frame 1319 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.0029582977294921875 sec\n",
      "PyTorch postprocessing took 0.00954437255859375 sec\n",
      "Frame 1320 processing time: 0.0251 seconds\n",
      "PyTorch inference took 0.002897024154663086 sec\n",
      "PyTorch postprocessing took 0.009665727615356445 sec\n",
      "Frame 1321 processing time: 0.0255 seconds\n",
      "PyTorch inference took 0.0028679370880126953 sec\n",
      "PyTorch postprocessing took 0.00956106185913086 sec\n",
      "Frame 1322 processing time: 0.0252 seconds\n",
      "PyTorch inference took 0.0029532909393310547 sec\n",
      "PyTorch postprocessing took 0.009561300277709961 sec\n",
      "Frame 1323 processing time: 0.0248 seconds\n",
      "PyTorch inference took 0.002901792526245117 sec\n",
      "PyTorch postprocessing took 0.009549617767333984 sec\n",
      "Frame 1324 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.002945423126220703 sec\n",
      "PyTorch postprocessing took 0.009503841400146484 sec\n",
      "Frame 1325 processing time: 0.0257 seconds\n",
      "PyTorch inference took 0.0029325485229492188 sec\n",
      "PyTorch postprocessing took 0.009530782699584961 sec\n",
      "Frame 1326 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.0029196739196777344 sec\n",
      "PyTorch postprocessing took 0.009496212005615234 sec\n",
      "Frame 1327 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.002892732620239258 sec\n",
      "PyTorch postprocessing took 0.009536981582641602 sec\n",
      "Frame 1328 processing time: 0.0257 seconds\n",
      "PyTorch inference took 0.0036516189575195312 sec\n",
      "PyTorch postprocessing took 0.009693384170532227 sec\n",
      "Frame 1329 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.0029168128967285156 sec\n",
      "PyTorch postprocessing took 0.009488821029663086 sec\n",
      "Frame 1330 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.0029294490814208984 sec\n",
      "PyTorch postprocessing took 0.009528160095214844 sec\n",
      "Frame 1331 processing time: 0.0281 seconds\n",
      "PyTorch inference took 0.002857685089111328 sec\n",
      "PyTorch postprocessing took 0.009783267974853516 sec\n",
      "Frame 1332 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.0028727054595947266 sec\n",
      "PyTorch postprocessing took 0.009663820266723633 sec\n",
      "Frame 1333 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.0029129981994628906 sec\n",
      "PyTorch postprocessing took 0.009559869766235352 sec\n",
      "Frame 1334 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.002933502197265625 sec\n",
      "PyTorch postprocessing took 0.009583711624145508 sec\n",
      "Frame 1335 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.0028634071350097656 sec\n",
      "PyTorch postprocessing took 0.009683847427368164 sec\n",
      "Frame 1336 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.002943754196166992 sec\n",
      "PyTorch postprocessing took 0.009589433670043945 sec\n",
      "Frame 1337 processing time: 0.0280 seconds\n",
      "PyTorch inference took 0.0028934478759765625 sec\n",
      "PyTorch postprocessing took 0.00964808464050293 sec\n",
      "Frame 1338 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.002913951873779297 sec\n",
      "PyTorch postprocessing took 0.00958704948425293 sec\n",
      "Frame 1339 processing time: 0.0282 seconds\n",
      "PyTorch inference took 0.0029256343841552734 sec\n",
      "PyTorch postprocessing took 0.009592056274414062 sec\n",
      "Frame 1340 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.0028748512268066406 sec\n",
      "PyTorch postprocessing took 0.009666204452514648 sec\n",
      "Frame 1341 processing time: 0.0241 seconds\n",
      "PyTorch inference took 0.002917766571044922 sec\n",
      "PyTorch postprocessing took 0.009615182876586914 sec\n",
      "Frame 1342 processing time: 0.0251 seconds\n",
      "PyTorch inference took 0.002989053726196289 sec\n",
      "PyTorch postprocessing took 0.009457588195800781 sec\n",
      "Frame 1343 processing time: 0.0255 seconds\n",
      "PyTorch inference took 0.0028870105743408203 sec\n",
      "PyTorch postprocessing took 0.009561777114868164 sec\n",
      "Frame 1344 processing time: 0.0299 seconds\n",
      "PyTorch inference took 0.005893707275390625 sec\n",
      "PyTorch postprocessing took 0.007268428802490234 sec\n",
      "Frame 1345 processing time: 0.0433 seconds\n",
      "PyTorch inference took 0.003682851791381836 sec\n",
      "PyTorch postprocessing took 0.010630130767822266 sec\n",
      "Frame 1346 processing time: 0.0325 seconds\n",
      "PyTorch inference took 0.0034863948822021484 sec\n",
      "PyTorch postprocessing took 0.00903630256652832 sec\n",
      "Frame 1347 processing time: 0.0266 seconds\n",
      "PyTorch inference took 0.003229379653930664 sec\n",
      "PyTorch postprocessing took 0.009392738342285156 sec\n",
      "Frame 1348 processing time: 0.0246 seconds\n",
      "PyTorch inference took 0.002932310104370117 sec\n",
      "PyTorch postprocessing took 0.009586572647094727 sec\n",
      "Frame 1349 processing time: 0.0242 seconds\n",
      "PyTorch inference took 0.0029916763305664062 sec\n",
      "PyTorch postprocessing took 0.009485483169555664 sec\n",
      "Frame 1350 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.0031757354736328125 sec\n",
      "PyTorch postprocessing took 0.009364604949951172 sec\n",
      "Frame 1351 processing time: 0.0252 seconds\n",
      "PyTorch inference took 0.0028808116912841797 sec\n",
      "PyTorch postprocessing took 0.00963902473449707 sec\n",
      "Frame 1352 processing time: 0.0245 seconds\n",
      "PyTorch inference took 0.002884387969970703 sec\n",
      "PyTorch postprocessing took 0.009542465209960938 sec\n",
      "Frame 1353 processing time: 0.0247 seconds\n",
      "PyTorch inference took 0.0030336380004882812 sec\n",
      "PyTorch postprocessing took 0.009537696838378906 sec\n",
      "Frame 1354 processing time: 0.0252 seconds\n",
      "PyTorch inference took 0.0029420852661132812 sec\n",
      "PyTorch postprocessing took 0.009556293487548828 sec\n",
      "Frame 1355 processing time: 0.0295 seconds\n",
      "PyTorch inference took 0.0037097930908203125 sec\n",
      "PyTorch postprocessing took 0.009391069412231445 sec\n",
      "Frame 1356 processing time: 0.0267 seconds\n",
      "PyTorch inference took 0.0031790733337402344 sec\n",
      "PyTorch postprocessing took 0.009287118911743164 sec\n",
      "Frame 1357 processing time: 0.0298 seconds\n",
      "PyTorch inference took 0.01700568199157715 sec\n",
      "PyTorch postprocessing took 0.0235445499420166 sec\n",
      "Frame 1358 processing time: 0.0584 seconds\n",
      "PyTorch inference took 0.0029859542846679688 sec\n",
      "PyTorch postprocessing took 0.009450197219848633 sec\n",
      "Frame 1359 processing time: 0.0257 seconds\n",
      "PyTorch inference took 0.0071942806243896484 sec\n",
      "PyTorch postprocessing took 0.014837265014648438 sec\n",
      "Frame 1360 processing time: 0.0506 seconds\n",
      "PyTorch inference took 0.0029294490814208984 sec\n",
      "PyTorch postprocessing took 0.009486675262451172 sec\n",
      "Frame 1361 processing time: 0.0285 seconds\n",
      "PyTorch inference took 0.0029723644256591797 sec\n",
      "PyTorch postprocessing took 0.009490728378295898 sec\n",
      "Frame 1362 processing time: 0.0248 seconds\n",
      "PyTorch inference took 0.004982471466064453 sec\n",
      "PyTorch postprocessing took 0.010023355484008789 sec\n",
      "Frame 1363 processing time: 0.0339 seconds\n",
      "PyTorch inference took 0.002933979034423828 sec\n",
      "PyTorch postprocessing took 0.010427713394165039 sec\n",
      "Frame 1364 processing time: 0.0288 seconds\n",
      "PyTorch inference took 0.004927873611450195 sec\n",
      "PyTorch postprocessing took 0.007441043853759766 sec\n",
      "Frame 1365 processing time: 0.0279 seconds\n",
      "PyTorch inference took 0.00710296630859375 sec\n",
      "PyTorch postprocessing took 0.009242773056030273 sec\n",
      "Frame 1366 processing time: 0.0583 seconds\n",
      "PyTorch inference took 0.004830360412597656 sec\n",
      "PyTorch postprocessing took 0.008561372756958008 sec\n",
      "Frame 1367 processing time: 0.0316 seconds\n",
      "PyTorch inference took 0.005095005035400391 sec\n",
      "PyTorch postprocessing took 0.008320331573486328 sec\n",
      "Frame 1368 processing time: 0.0295 seconds\n",
      "PyTorch inference took 0.002927064895629883 sec\n",
      "PyTorch postprocessing took 0.009531974792480469 sec\n",
      "Frame 1369 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.005998134613037109 sec\n",
      "PyTorch postprocessing took 0.02254796028137207 sec\n",
      "Frame 1370 processing time: 0.0476 seconds\n",
      "PyTorch inference took 0.0033435821533203125 sec\n",
      "PyTorch postprocessing took 0.009082317352294922 sec\n",
      "Frame 1371 processing time: 0.0246 seconds\n",
      "PyTorch inference took 0.0028944015502929688 sec\n",
      "PyTorch postprocessing took 0.009602546691894531 sec\n",
      "Frame 1372 processing time: 0.0243 seconds\n",
      "PyTorch inference took 0.0030968189239501953 sec\n",
      "PyTorch postprocessing took 0.009417533874511719 sec\n",
      "Frame 1373 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.002925395965576172 sec\n",
      "PyTorch postprocessing took 0.010370254516601562 sec\n",
      "Frame 1374 processing time: 0.0252 seconds\n",
      "PyTorch inference took 0.0028657913208007812 sec\n",
      "PyTorch postprocessing took 0.009592056274414062 sec\n",
      "Frame 1375 processing time: 0.0267 seconds\n",
      "PyTorch inference took 0.002933025360107422 sec\n",
      "PyTorch postprocessing took 0.009620189666748047 sec\n",
      "Frame 1376 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.0029098987579345703 sec\n",
      "PyTorch postprocessing took 0.010067939758300781 sec\n",
      "Frame 1377 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.002925395965576172 sec\n",
      "PyTorch postprocessing took 0.009586811065673828 sec\n",
      "Frame 1378 processing time: 0.0240 seconds\n",
      "PyTorch inference took 0.0028905868530273438 sec\n",
      "PyTorch postprocessing took 0.01011204719543457 sec\n",
      "Frame 1379 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.0029144287109375 sec\n",
      "PyTorch postprocessing took 0.009472846984863281 sec\n",
      "Frame 1380 processing time: 0.0290 seconds\n",
      "PyTorch inference took 0.005614280700683594 sec\n",
      "PyTorch postprocessing took 0.007130622863769531 sec\n",
      "Frame 1381 processing time: 0.0477 seconds\n",
      "PyTorch inference took 0.007188320159912109 sec\n",
      "PyTorch postprocessing took 0.00532984733581543 sec\n",
      "Frame 1382 processing time: 0.0292 seconds\n",
      "PyTorch inference took 0.003077983856201172 sec\n",
      "PyTorch postprocessing took 0.009478092193603516 sec\n",
      "Frame 1383 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.002952098846435547 sec\n",
      "PyTorch postprocessing took 0.009477853775024414 sec\n",
      "Frame 1384 processing time: 0.0289 seconds\n",
      "PyTorch inference took 0.005334615707397461 sec\n",
      "PyTorch postprocessing took 0.010920286178588867 sec\n",
      "Frame 1385 processing time: 0.0511 seconds\n",
      "PyTorch inference took 0.002938985824584961 sec\n",
      "PyTorch postprocessing took 0.009508371353149414 sec\n",
      "Frame 1386 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.0029375553131103516 sec\n",
      "PyTorch postprocessing took 0.00951075553894043 sec\n",
      "Frame 1387 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.0029633045196533203 sec\n",
      "PyTorch postprocessing took 0.009499073028564453 sec\n",
      "Frame 1388 processing time: 0.0257 seconds\n",
      "PyTorch inference took 0.003574371337890625 sec\n",
      "PyTorch postprocessing took 0.0089569091796875 sec\n",
      "Frame 1389 processing time: 0.0291 seconds\n",
      "PyTorch inference took 0.0038042068481445312 sec\n",
      "PyTorch postprocessing took 0.008666753768920898 sec\n",
      "Frame 1390 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.0029354095458984375 sec\n",
      "PyTorch postprocessing took 0.010588884353637695 sec\n",
      "Frame 1391 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.003680706024169922 sec\n",
      "PyTorch postprocessing took 0.009139299392700195 sec\n",
      "Frame 1392 processing time: 0.0292 seconds\n",
      "PyTorch inference took 0.002893209457397461 sec\n",
      "PyTorch postprocessing took 0.009498119354248047 sec\n",
      "Frame 1393 processing time: 0.0246 seconds\n",
      "PyTorch inference took 0.0029256343841552734 sec\n",
      "PyTorch postprocessing took 0.009556293487548828 sec\n",
      "Frame 1394 processing time: 0.0256 seconds\n",
      "PyTorch inference took 0.008669614791870117 sec\n",
      "PyTorch postprocessing took 0.00923466682434082 sec\n",
      "Frame 1395 processing time: 0.0360 seconds\n",
      "PyTorch inference took 0.002894878387451172 sec\n",
      "PyTorch postprocessing took 0.009634971618652344 sec\n",
      "Frame 1396 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.0038454532623291016 sec\n",
      "PyTorch postprocessing took 0.009550333023071289 sec\n",
      "Frame 1397 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.0057468414306640625 sec\n",
      "PyTorch postprocessing took 0.01246333122253418 sec\n",
      "Frame 1398 processing time: 0.0315 seconds\n",
      "PyTorch inference took 0.003682851791381836 sec\n",
      "PyTorch postprocessing took 0.009788990020751953 sec\n",
      "Frame 1399 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.00487828254699707 sec\n",
      "PyTorch postprocessing took 0.0145416259765625 sec\n",
      "Frame 1400 processing time: 0.0391 seconds\n",
      "PyTorch inference took 0.007422208786010742 sec\n",
      "PyTorch postprocessing took 0.006360054016113281 sec\n",
      "Frame 1401 processing time: 0.0312 seconds\n",
      "PyTorch inference took 0.006407022476196289 sec\n",
      "PyTorch postprocessing took 0.008798599243164062 sec\n",
      "Frame 1402 processing time: 0.0327 seconds\n",
      "PyTorch inference took 0.008002042770385742 sec\n",
      "PyTorch postprocessing took 0.009725093841552734 sec\n",
      "Frame 1403 processing time: 0.0436 seconds\n",
      "PyTorch inference took 0.003917217254638672 sec\n",
      "PyTorch postprocessing took 0.009493112564086914 sec\n",
      "Frame 1404 processing time: 0.0283 seconds\n",
      "PyTorch inference took 0.0037915706634521484 sec\n",
      "PyTorch postprocessing took 0.009849071502685547 sec\n",
      "Frame 1405 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.002939462661743164 sec\n",
      "PyTorch postprocessing took 0.009612321853637695 sec\n",
      "Frame 1406 processing time: 0.0257 seconds\n",
      "PyTorch inference took 0.0031616687774658203 sec\n",
      "PyTorch postprocessing took 0.00985264778137207 sec\n",
      "Frame 1407 processing time: 0.0315 seconds\n",
      "PyTorch inference took 0.006401777267456055 sec\n",
      "PyTorch postprocessing took 0.009781837463378906 sec\n",
      "Frame 1408 processing time: 0.0384 seconds\n",
      "PyTorch inference took 0.002956867218017578 sec\n",
      "PyTorch postprocessing took 0.009566545486450195 sec\n",
      "Frame 1409 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.012542247772216797 sec\n",
      "PyTorch postprocessing took 0.018052101135253906 sec\n",
      "Frame 1410 processing time: 0.0465 seconds\n",
      "PyTorch inference took 0.0037262439727783203 sec\n",
      "PyTorch postprocessing took 0.010921001434326172 sec\n",
      "Frame 1411 processing time: 0.0312 seconds\n",
      "PyTorch inference took 0.0029060840606689453 sec\n",
      "PyTorch postprocessing took 0.009560585021972656 sec\n",
      "Frame 1412 processing time: 0.0244 seconds\n",
      "PyTorch inference took 0.0039064884185791016 sec\n",
      "PyTorch postprocessing took 0.009450674057006836 sec\n",
      "Frame 1413 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.0029075145721435547 sec\n",
      "PyTorch postprocessing took 0.009591817855834961 sec\n",
      "Frame 1414 processing time: 0.0296 seconds\n",
      "PyTorch inference took 0.0036897659301757812 sec\n",
      "PyTorch postprocessing took 0.009704113006591797 sec\n",
      "Frame 1415 processing time: 0.0265 seconds\n",
      "PyTorch inference took 0.00452113151550293 sec\n",
      "PyTorch postprocessing took 0.009555578231811523 sec\n",
      "Frame 1416 processing time: 0.0281 seconds\n",
      "PyTorch inference took 0.0036640167236328125 sec\n",
      "PyTorch postprocessing took 0.009009122848510742 sec\n",
      "Frame 1417 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.0029075145721435547 sec\n",
      "PyTorch postprocessing took 0.00948333740234375 sec\n",
      "Frame 1418 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.003579854965209961 sec\n",
      "PyTorch postprocessing took 0.008866071701049805 sec\n",
      "Frame 1419 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.0029587745666503906 sec\n",
      "PyTorch postprocessing took 0.009492874145507812 sec\n",
      "Frame 1420 processing time: 0.0257 seconds\n",
      "PyTorch inference took 0.0037436485290527344 sec\n",
      "PyTorch postprocessing took 0.009638786315917969 sec\n",
      "Frame 1421 processing time: 0.0321 seconds\n",
      "PyTorch inference took 0.00684356689453125 sec\n",
      "PyTorch postprocessing took 0.010287046432495117 sec\n",
      "Frame 1422 processing time: 0.0387 seconds\n",
      "PyTorch inference took 0.003993034362792969 sec\n",
      "PyTorch postprocessing took 0.009508609771728516 sec\n",
      "Frame 1423 processing time: 0.0327 seconds\n",
      "PyTorch inference took 0.002838134765625 sec\n",
      "PyTorch postprocessing took 0.009706497192382812 sec\n",
      "Frame 1424 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.0031163692474365234 sec\n",
      "PyTorch postprocessing took 0.009464025497436523 sec\n",
      "Frame 1425 processing time: 0.0290 seconds\n",
      "PyTorch inference took 0.0030372142791748047 sec\n",
      "PyTorch postprocessing took 0.00946044921875 sec\n",
      "Frame 1426 processing time: 0.0252 seconds\n",
      "PyTorch inference took 0.0029592514038085938 sec\n",
      "PyTorch postprocessing took 0.009521245956420898 sec\n",
      "Frame 1427 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.003284931182861328 sec\n",
      "PyTorch postprocessing took 0.009768962860107422 sec\n",
      "Frame 1428 processing time: 0.0315 seconds\n",
      "PyTorch inference took 0.005013465881347656 sec\n",
      "PyTorch postprocessing took 0.007849931716918945 sec\n",
      "Frame 1429 processing time: 0.0293 seconds\n",
      "PyTorch inference took 0.00390625 sec\n",
      "PyTorch postprocessing took 0.010107040405273438 sec\n",
      "Frame 1430 processing time: 0.0380 seconds\n",
      "PyTorch inference took 0.0030739307403564453 sec\n",
      "PyTorch postprocessing took 0.00938725471496582 sec\n",
      "Frame 1431 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.002872467041015625 sec\n",
      "PyTorch postprocessing took 0.009579181671142578 sec\n",
      "Frame 1432 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.0030176639556884766 sec\n",
      "PyTorch postprocessing took 0.009469747543334961 sec\n",
      "Frame 1433 processing time: 0.0248 seconds\n",
      "PyTorch inference took 0.002913236618041992 sec\n",
      "PyTorch postprocessing took 0.009620428085327148 sec\n",
      "Frame 1434 processing time: 0.0251 seconds\n",
      "PyTorch inference took 0.0031163692474365234 sec\n",
      "PyTorch postprocessing took 0.009406566619873047 sec\n",
      "Frame 1435 processing time: 0.0255 seconds\n",
      "PyTorch inference took 0.0029408931732177734 sec\n",
      "PyTorch postprocessing took 0.009513616561889648 sec\n",
      "Frame 1436 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.0028924942016601562 sec\n",
      "PyTorch postprocessing took 0.009614944458007812 sec\n",
      "Frame 1437 processing time: 0.0266 seconds\n",
      "PyTorch inference took 0.002901315689086914 sec\n",
      "PyTorch postprocessing took 0.009588479995727539 sec\n",
      "Frame 1438 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.004623889923095703 sec\n",
      "PyTorch postprocessing took 0.007841348648071289 sec\n",
      "Frame 1439 processing time: 0.0269 seconds\n",
      "PyTorch inference took 0.002913236618041992 sec\n",
      "PyTorch postprocessing took 0.00950002670288086 sec\n",
      "Frame 1440 processing time: 0.0257 seconds\n",
      "PyTorch inference took 0.002958536148071289 sec\n",
      "PyTorch postprocessing took 0.009510040283203125 sec\n",
      "Frame 1441 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.002931356430053711 sec\n",
      "PyTorch postprocessing took 0.009446382522583008 sec\n",
      "Frame 1442 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.0029180049896240234 sec\n",
      "PyTorch postprocessing took 0.009532451629638672 sec\n",
      "Frame 1443 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.005293607711791992 sec\n",
      "PyTorch postprocessing took 0.007836103439331055 sec\n",
      "Frame 1444 processing time: 0.0361 seconds\n",
      "PyTorch inference took 0.01249241828918457 sec\n",
      "PyTorch postprocessing took 0.020141124725341797 sec\n",
      "Frame 1445 processing time: 0.0557 seconds\n",
      "PyTorch inference took 0.005232095718383789 sec\n",
      "PyTorch postprocessing took 0.010010242462158203 sec\n",
      "Frame 1446 processing time: 0.0445 seconds\n",
      "PyTorch inference took 0.007287263870239258 sec\n",
      "PyTorch postprocessing took 0.006747722625732422 sec\n",
      "Frame 1447 processing time: 0.0497 seconds\n",
      "PyTorch inference took 0.0029420852661132812 sec\n",
      "PyTorch postprocessing took 0.009444475173950195 sec\n",
      "Frame 1448 processing time: 0.0307 seconds\n",
      "PyTorch inference took 0.00571894645690918 sec\n",
      "PyTorch postprocessing took 0.007181644439697266 sec\n",
      "Frame 1449 processing time: 0.0362 seconds\n",
      "PyTorch inference took 0.009233474731445312 sec\n",
      "PyTorch postprocessing took 0.011130571365356445 sec\n",
      "Frame 1450 processing time: 0.0420 seconds\n",
      "PyTorch inference took 0.005372047424316406 sec\n",
      "PyTorch postprocessing took 0.008398771286010742 sec\n",
      "Frame 1451 processing time: 0.0388 seconds\n",
      "PyTorch inference took 0.002890348434448242 sec\n",
      "PyTorch postprocessing took 0.009520769119262695 sec\n",
      "Frame 1452 processing time: 0.0248 seconds\n",
      "PyTorch inference took 0.0029561519622802734 sec\n",
      "PyTorch postprocessing took 0.009528636932373047 sec\n",
      "Frame 1453 processing time: 0.0246 seconds\n",
      "PyTorch inference took 0.003150463104248047 sec\n",
      "PyTorch postprocessing took 0.009284019470214844 sec\n",
      "Frame 1454 processing time: 0.0252 seconds\n",
      "PyTorch inference took 0.002963542938232422 sec\n",
      "PyTorch postprocessing took 0.009467363357543945 sec\n",
      "Frame 1455 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.0029184818267822266 sec\n",
      "PyTorch postprocessing took 0.009569883346557617 sec\n",
      "Frame 1456 processing time: 0.0245 seconds\n",
      "PyTorch inference took 0.0035958290100097656 sec\n",
      "PyTorch postprocessing took 0.008895635604858398 sec\n",
      "Frame 1457 processing time: 0.0277 seconds\n",
      "PyTorch inference took 0.002966165542602539 sec\n",
      "PyTorch postprocessing took 0.009503602981567383 sec\n",
      "Frame 1458 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.002913236618041992 sec\n",
      "PyTorch postprocessing took 0.009563446044921875 sec\n",
      "Frame 1459 processing time: 0.0248 seconds\n",
      "PyTorch inference took 0.0034253597259521484 sec\n",
      "PyTorch postprocessing took 0.009391546249389648 sec\n",
      "Frame 1460 processing time: 0.0251 seconds\n",
      "PyTorch inference took 0.0029189586639404297 sec\n",
      "PyTorch postprocessing took 0.00958561897277832 sec\n",
      "Frame 1461 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.0029382705688476562 sec\n",
      "PyTorch postprocessing took 0.009474039077758789 sec\n",
      "Frame 1462 processing time: 0.0248 seconds\n",
      "PyTorch inference took 0.003690481185913086 sec\n",
      "PyTorch postprocessing took 0.009566783905029297 sec\n",
      "Frame 1463 processing time: 0.0273 seconds\n",
      "PyTorch inference took 0.0029098987579345703 sec\n",
      "PyTorch postprocessing took 0.009481430053710938 sec\n",
      "Frame 1464 processing time: 0.0297 seconds\n",
      "PyTorch inference took 0.0029821395874023438 sec\n",
      "PyTorch postprocessing took 0.009486913681030273 sec\n",
      "Frame 1465 processing time: 0.0272 seconds\n",
      "PyTorch inference took 0.006545543670654297 sec\n",
      "PyTorch postprocessing took 0.009209632873535156 sec\n",
      "Frame 1466 processing time: 0.0428 seconds\n",
      "PyTorch inference took 0.003919124603271484 sec\n",
      "PyTorch postprocessing took 0.00896453857421875 sec\n",
      "Frame 1467 processing time: 0.0293 seconds\n",
      "PyTorch inference took 0.0030145645141601562 sec\n",
      "PyTorch postprocessing took 0.00947427749633789 sec\n",
      "Frame 1468 processing time: 0.0315 seconds\n",
      "PyTorch inference took 0.003079652786254883 sec\n",
      "PyTorch postprocessing took 0.009383678436279297 sec\n",
      "Frame 1469 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.0030434131622314453 sec\n",
      "PyTorch postprocessing took 0.009433746337890625 sec\n",
      "Frame 1470 processing time: 0.0245 seconds\n",
      "PyTorch inference took 0.0029761791229248047 sec\n",
      "PyTorch postprocessing took 0.009557008743286133 sec\n",
      "Frame 1471 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.0029447078704833984 sec\n",
      "PyTorch postprocessing took 0.009562969207763672 sec\n",
      "Frame 1472 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.003025531768798828 sec\n",
      "PyTorch postprocessing took 0.009425640106201172 sec\n",
      "Frame 1473 processing time: 0.0259 seconds\n",
      "PyTorch inference took 0.0029075145721435547 sec\n",
      "PyTorch postprocessing took 0.009523153305053711 sec\n",
      "Frame 1474 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.0029146671295166016 sec\n",
      "PyTorch postprocessing took 0.015485048294067383 sec\n",
      "Frame 1475 processing time: 0.0368 seconds\n",
      "PyTorch inference took 0.0030059814453125 sec\n",
      "PyTorch postprocessing took 0.009479522705078125 sec\n",
      "Frame 1476 processing time: 0.0255 seconds\n",
      "PyTorch inference took 0.004747152328491211 sec\n",
      "PyTorch postprocessing took 0.008401155471801758 sec\n",
      "Frame 1477 processing time: 0.0354 seconds\n",
      "PyTorch inference took 0.002946615219116211 sec\n",
      "PyTorch postprocessing took 0.009560823440551758 sec\n",
      "Frame 1478 processing time: 0.0268 seconds\n",
      "PyTorch inference took 0.002901315689086914 sec\n",
      "PyTorch postprocessing took 0.009560585021972656 sec\n",
      "Frame 1479 processing time: 0.0292 seconds\n",
      "PyTorch inference took 0.002922534942626953 sec\n",
      "PyTorch postprocessing took 0.009493827819824219 sec\n",
      "Frame 1480 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.0028753280639648438 sec\n",
      "PyTorch postprocessing took 0.00959920883178711 sec\n",
      "Frame 1481 processing time: 0.0249 seconds\n",
      "PyTorch inference took 0.0028922557830810547 sec\n",
      "PyTorch postprocessing took 0.009495973587036133 sec\n",
      "Frame 1482 processing time: 0.0278 seconds\n",
      "PyTorch inference took 0.003459453582763672 sec\n",
      "PyTorch postprocessing took 0.009491920471191406 sec\n",
      "Frame 1483 processing time: 0.0291 seconds\n",
      "PyTorch inference took 0.0029344558715820312 sec\n",
      "PyTorch postprocessing took 0.009467840194702148 sec\n",
      "Frame 1484 processing time: 0.0269 seconds\n",
      "PyTorch inference took 0.0030717849731445312 sec\n",
      "PyTorch postprocessing took 0.009345293045043945 sec\n",
      "Frame 1485 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.003187417984008789 sec\n",
      "PyTorch postprocessing took 0.009347677230834961 sec\n",
      "Frame 1486 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.0032265186309814453 sec\n",
      "PyTorch postprocessing took 0.009267330169677734 sec\n",
      "Frame 1487 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.004299640655517578 sec\n",
      "PyTorch postprocessing took 0.009402275085449219 sec\n",
      "Frame 1488 processing time: 0.0306 seconds\n",
      "PyTorch inference took 0.0028755664825439453 sec\n",
      "PyTorch postprocessing took 0.009696245193481445 sec\n",
      "Frame 1489 processing time: 0.0252 seconds\n",
      "PyTorch inference took 0.0029726028442382812 sec\n",
      "PyTorch postprocessing took 0.00958561897277832 sec\n",
      "Frame 1490 processing time: 0.0247 seconds\n",
      "PyTorch inference took 0.003008604049682617 sec\n",
      "PyTorch postprocessing took 0.009474992752075195 sec\n",
      "Frame 1491 processing time: 0.0251 seconds\n",
      "PyTorch inference took 0.0029914379119873047 sec\n",
      "PyTorch postprocessing took 0.009629011154174805 sec\n",
      "Frame 1492 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.003047943115234375 sec\n",
      "PyTorch postprocessing took 0.00946664810180664 sec\n",
      "Frame 1493 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.0029990673065185547 sec\n",
      "PyTorch postprocessing took 0.009491443634033203 sec\n",
      "Frame 1494 processing time: 0.0247 seconds\n",
      "PyTorch inference took 0.0029544830322265625 sec\n",
      "PyTorch postprocessing took 0.009545087814331055 sec\n",
      "Frame 1495 processing time: 0.0264 seconds\n",
      "PyTorch inference took 0.0047876834869384766 sec\n",
      "PyTorch postprocessing took 0.00807499885559082 sec\n",
      "Frame 1496 processing time: 0.0270 seconds\n",
      "PyTorch inference took 0.002995014190673828 sec\n",
      "PyTorch postprocessing took 0.009570837020874023 sec\n",
      "Frame 1497 processing time: 0.0248 seconds\n",
      "PyTorch inference took 0.003007650375366211 sec\n",
      "PyTorch postprocessing took 0.009928703308105469 sec\n",
      "Frame 1498 processing time: 0.0287 seconds\n",
      "PyTorch inference took 0.0032982826232910156 sec\n",
      "PyTorch postprocessing took 0.009514093399047852 sec\n",
      "Frame 1499 processing time: 0.0263 seconds\n",
      "PyTorch inference took 0.0030269622802734375 sec\n",
      "PyTorch postprocessing took 0.009503602981567383 sec\n",
      "Frame 1500 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.003504514694213867 sec\n",
      "PyTorch postprocessing took 0.010026931762695312 sec\n",
      "Frame 1501 processing time: 0.0269 seconds\n",
      "PyTorch inference took 0.0030045509338378906 sec\n",
      "PyTorch postprocessing took 0.009400606155395508 sec\n",
      "Frame 1502 processing time: 0.0246 seconds\n",
      "PyTorch inference took 0.0029065608978271484 sec\n",
      "PyTorch postprocessing took 0.00952911376953125 sec\n",
      "Frame 1503 processing time: 0.0254 seconds\n",
      "PyTorch inference took 0.0036802291870117188 sec\n",
      "PyTorch postprocessing took 0.008793115615844727 sec\n",
      "Frame 1504 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.005881547927856445 sec\n",
      "PyTorch postprocessing took 0.009048223495483398 sec\n",
      "Frame 1505 processing time: 0.0325 seconds\n",
      "PyTorch inference took 0.005547046661376953 sec\n",
      "PyTorch postprocessing took 0.007907390594482422 sec\n",
      "Frame 1506 processing time: 0.0412 seconds\n",
      "PyTorch inference took 0.0038771629333496094 sec\n",
      "PyTorch postprocessing took 0.011367559432983398 sec\n",
      "Frame 1507 processing time: 0.0341 seconds\n",
      "PyTorch inference took 0.007272958755493164 sec\n",
      "PyTorch postprocessing took 0.006066799163818359 sec\n",
      "Frame 1508 processing time: 0.0351 seconds\n",
      "PyTorch inference took 0.006757259368896484 sec\n",
      "PyTorch postprocessing took 0.00803065299987793 sec\n",
      "Frame 1509 processing time: 0.0302 seconds\n",
      "PyTorch inference took 0.004431009292602539 sec\n",
      "PyTorch postprocessing took 0.009981393814086914 sec\n",
      "Frame 1510 processing time: 0.0314 seconds\n",
      "PyTorch inference took 0.002952098846435547 sec\n",
      "PyTorch postprocessing took 0.0095977783203125 sec\n",
      "Frame 1511 processing time: 0.0246 seconds\n",
      "PyTorch inference took 0.003129720687866211 sec\n",
      "PyTorch postprocessing took 0.009495019912719727 sec\n",
      "Frame 1512 processing time: 0.0272 seconds\n",
      "PyTorch inference took 0.002856016159057617 sec\n",
      "PyTorch postprocessing took 0.015527009963989258 sec\n",
      "Frame 1513 processing time: 0.0376 seconds\n",
      "PyTorch inference took 0.0028769969940185547 sec\n",
      "PyTorch postprocessing took 0.009604692459106445 sec\n",
      "Frame 1514 processing time: 0.0262 seconds\n",
      "PyTorch inference took 0.005471706390380859 sec\n",
      "PyTorch postprocessing took 0.007008790969848633 sec\n",
      "Frame 1515 processing time: 0.0292 seconds\n",
      "PyTorch inference took 0.005771636962890625 sec\n",
      "PyTorch postprocessing took 0.008081436157226562 sec\n",
      "Frame 1516 processing time: 0.0297 seconds\n",
      "PyTorch inference took 0.002984285354614258 sec\n",
      "PyTorch postprocessing took 0.009446144104003906 sec\n",
      "Frame 1517 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.002865314483642578 sec\n",
      "PyTorch postprocessing took 0.00954127311706543 sec\n",
      "Frame 1518 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.002961874008178711 sec\n",
      "PyTorch postprocessing took 0.009517431259155273 sec\n",
      "Frame 1519 processing time: 0.0311 seconds\n",
      "PyTorch inference took 0.0028884410858154297 sec\n",
      "PyTorch postprocessing took 0.009606599807739258 sec\n",
      "Frame 1520 processing time: 0.0253 seconds\n",
      "PyTorch inference took 0.002867460250854492 sec\n",
      "PyTorch postprocessing took 0.009603500366210938 sec\n",
      "Frame 1521 processing time: 0.0250 seconds\n",
      "PyTorch inference took 0.0029249191284179688 sec\n",
      "PyTorch postprocessing took 0.0096893310546875 sec\n",
      "Frame 1522 processing time: 0.0260 seconds\n",
      "PyTorch inference took 0.002979278564453125 sec\n",
      "PyTorch postprocessing took 0.010529518127441406 sec\n",
      "Frame 1523 processing time: 0.0265 seconds\n",
      "PyTorch inference took 0.003708362579345703 sec\n",
      "PyTorch postprocessing took 0.008731842041015625 sec\n",
      "Frame 1524 processing time: 0.0274 seconds\n",
      "PyTorch inference took 0.002963542938232422 sec\n",
      "PyTorch postprocessing took 0.01083064079284668 sec\n",
      "Frame 1525 processing time: 0.0287 seconds\n",
      "PyTorch inference took 0.004675865173339844 sec\n",
      "PyTorch postprocessing took 0.008238792419433594 sec\n",
      "Frame 1526 processing time: 0.0267 seconds\n",
      "PyTorch inference took 0.00399327278137207 sec\n",
      "PyTorch postprocessing took 0.008987903594970703 sec\n",
      "Frame 1527 processing time: 0.0305 seconds\n",
      "PyTorch inference took 0.005307435989379883 sec\n",
      "PyTorch postprocessing took 0.007294654846191406 sec\n",
      "Frame 1528 processing time: 0.0302 seconds\n",
      "PyTorch inference took 0.0039942264556884766 sec\n",
      "PyTorch postprocessing took 0.009151935577392578 sec\n",
      "Frame 1529 processing time: 0.0328 seconds\n",
      "PyTorch inference took 0.00414276123046875 sec\n",
      "PyTorch postprocessing took 0.008921623229980469 sec\n",
      "Frame 1530 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.009765148162841797 sec\n",
      "PyTorch postprocessing took 0.010235786437988281 sec\n",
      "Frame 1531 processing time: 0.0559 seconds\n",
      "PyTorch inference took 0.011683225631713867 sec\n",
      "PyTorch postprocessing took 0.013234615325927734 sec\n",
      "Frame 1532 processing time: 0.0464 seconds\n",
      "PyTorch inference took 0.0048029422760009766 sec\n",
      "PyTorch postprocessing took 0.009040594100952148 sec\n",
      "Frame 1533 processing time: 0.0319 seconds\n",
      "PyTorch inference took 0.012044668197631836 sec\n",
      "PyTorch postprocessing took 0.019856691360473633 sec\n",
      "Frame 1534 processing time: 0.0780 seconds\n",
      "PyTorch inference took 0.006997585296630859 sec\n",
      "PyTorch postprocessing took 0.012540102005004883 sec\n",
      "Frame 1535 processing time: 0.0562 seconds\n",
      "PyTorch inference took 0.009409189224243164 sec\n",
      "PyTorch postprocessing took 0.009634733200073242 sec\n",
      "Frame 1536 processing time: 0.0630 seconds\n",
      "PyTorch inference took 0.004511356353759766 sec\n",
      "PyTorch postprocessing took 0.00956583023071289 sec\n",
      "Frame 1537 processing time: 0.0392 seconds\n",
      "PyTorch inference took 0.0038275718688964844 sec\n",
      "PyTorch postprocessing took 0.014333963394165039 sec\n",
      "Frame 1538 processing time: 0.0374 seconds\n",
      "PyTorch inference took 0.007140398025512695 sec\n",
      "PyTorch postprocessing took 0.009455680847167969 sec\n",
      "Frame 1539 processing time: 0.0416 seconds\n",
      "PyTorch inference took 0.003801584243774414 sec\n",
      "PyTorch postprocessing took 0.009819507598876953 sec\n",
      "Frame 1540 processing time: 0.0298 seconds\n",
      "PyTorch inference took 0.0035953521728515625 sec\n",
      "PyTorch postprocessing took 0.00936269760131836 sec\n",
      "Frame 1541 processing time: 0.0316 seconds\n",
      "PyTorch inference took 0.004753828048706055 sec\n",
      "PyTorch postprocessing took 0.012854576110839844 sec\n",
      "Frame 1542 processing time: 0.0388 seconds\n",
      "PyTorch inference took 0.0035996437072753906 sec\n",
      "PyTorch postprocessing took 0.009901285171508789 sec\n",
      "Frame 1543 processing time: 0.0287 seconds\n",
      "PyTorch inference took 0.01479792594909668 sec\n",
      "PyTorch postprocessing took 0.010091066360473633 sec\n",
      "Frame 1544 processing time: 0.0746 seconds\n",
      "PyTorch inference took 0.00437474250793457 sec\n",
      "PyTorch postprocessing took 0.01351165771484375 sec\n",
      "Frame 1545 processing time: 0.0365 seconds\n",
      "PyTorch inference took 0.016158103942871094 sec\n",
      "PyTorch postprocessing took 0.02462601661682129 sec\n",
      "Frame 1546 processing time: 0.0696 seconds\n",
      "PyTorch inference took 0.023713350296020508 sec\n",
      "PyTorch postprocessing took 0.015440225601196289 sec\n",
      "Frame 1547 processing time: 0.0791 seconds\n",
      "PyTorch inference took 0.006379842758178711 sec\n",
      "PyTorch postprocessing took 0.011359930038452148 sec\n",
      "Frame 1548 processing time: 0.0698 seconds\n",
      "PyTorch inference took 0.010306596755981445 sec\n",
      "PyTorch postprocessing took 0.021244049072265625 sec\n",
      "Frame 1549 processing time: 0.0747 seconds\n",
      "PyTorch inference took 0.003012418746948242 sec\n",
      "PyTorch postprocessing took 0.010033369064331055 sec\n",
      "Frame 1550 processing time: 0.0308 seconds\n",
      "PyTorch inference took 0.003191709518432617 sec\n",
      "PyTorch postprocessing took 0.009440898895263672 sec\n",
      "Frame 1551 processing time: 0.0258 seconds\n",
      "PyTorch inference took 0.005038261413574219 sec\n",
      "PyTorch postprocessing took 0.009145498275756836 sec\n",
      "Frame 1552 processing time: 0.0325 seconds\n",
      "PyTorch inference took 0.0061168670654296875 sec\n",
      "PyTorch postprocessing took 0.009514808654785156 sec\n",
      "Frame 1553 processing time: 0.0479 seconds\n",
      "PyTorch inference took 0.004246234893798828 sec\n",
      "PyTorch postprocessing took 0.009814023971557617 sec\n",
      "Frame 1554 processing time: 0.0359 seconds\n",
      "PyTorch inference took 0.0037026405334472656 sec\n",
      "PyTorch postprocessing took 0.015104293823242188 sec\n",
      "Frame 1555 processing time: 0.0392 seconds\n",
      "PyTorch inference took 0.011140584945678711 sec\n",
      "PyTorch postprocessing took 0.022060871124267578 sec\n",
      "Frame 1556 processing time: 0.0589 seconds\n",
      "PyTorch inference took 0.006349325180053711 sec\n",
      "PyTorch postprocessing took 0.01256251335144043 sec\n",
      "Frame 1557 processing time: 0.0379 seconds\n",
      "PyTorch inference took 0.0036280155181884766 sec\n",
      "PyTorch postprocessing took 0.009661436080932617 sec\n",
      "Frame 1558 processing time: 0.0271 seconds\n",
      "PyTorch inference took 0.0047147274017333984 sec\n",
      "PyTorch postprocessing took 0.016991853713989258 sec\n",
      "Frame 1559 processing time: 0.0550 seconds\n",
      "{'host_name': 'dikra', 'op_sys': 'Linux-6.8.0-39-generic-x86_64-with-glibc2.39', 'python': 'dlc-live', 'device_type': 'GPU', 'device': ['NVIDIA GeForce RTX 3050 6GB Laptop GPU'], 'freeze': ['albumentations==1.4.3', 'anyio==4.4.0', 'argon2-cffi==23.1.0', 'argon2-cffi-bindings==21.2.0', 'arrow==1.3.0', 'asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1698341106958/work', 'async-lru==2.0.4', 'attrs==24.2.0', 'autocommand==2.2.2', 'babel==2.16.0', 'backports.tarfile==1.2.0', 'beautifulsoup4==4.12.3', 'black==24.8.0', 'bleach==6.1.0', 'blosc2==2.0.0', 'certifi==2024.7.4', 'cffi==1.17.0', 'charset-normalizer==3.3.2', 'click==8.1.7', 'colorcet==3.1.0', 'coloredlogs==15.0.1', 'comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1710320294760/work', 'contourpy==1.2.1', 'cycler==0.12.1', 'Cython==3.0.11', 'debugpy @ file:///croot/debugpy_1690905042057/work', 'decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work', '-e git+https://github.com/dikraMasrour/DeepLabCut.git@b56591667346340faaba325b9bf38ffa58b3c29e#egg=deeplabcut', 'defusedxml==0.7.1', 'dlclibrary==0.0.6', 'einops==0.8.0', 'exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1720869315914/work', 'executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1698579936712/work', 'fastjsonschema==2.20.0', 'filelock==3.13.1', 'filterpy==1.4.5', 'flatbuffers==24.3.25', 'fonttools==4.53.1', 'fqdn==1.5.1', 'fsspec==2024.2.0', 'h11==0.14.0', 'h5py==3.11.0', 'httpcore==1.0.5', 'httpx==0.27.0', 'huggingface-hub==0.24.6', 'humanfriendly==10.0', 'idna==3.7', 'imageio==2.35.1', 'imageio-ffmpeg==0.5.1', 'imgaug==0.4.0', 'importlib_metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1724124505563/work', 'importlib_resources==6.4.0', 'inflect==7.3.1', 'ipykernel @ file:///home/conda/feedstock_root/build_artifacts/ipykernel_1719845459717/work', 'ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1719582526268/work', 'ipython-genutils==0.2.0', 'ipywidgets==7.7.1', 'isoduration==20.11.0', 'isort==5.13.2', 'jaraco.context==5.3.0', 'jaraco.functools==4.0.1', 'jaraco.text==3.12.1', 'jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1696326070614/work', 'Jinja2==3.1.3', 'joblib==1.4.2', 'json5==0.9.25', 'jsonpointer==3.0.0', 'jsonschema==4.23.0', 'jsonschema-specifications==2023.12.1', 'jupyter-events==0.10.0', 'jupyter-lsp==2.2.5', 'jupyter_client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1716472197302/work', 'jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1710257277185/work', 'jupyter_server==2.14.2', 'jupyter_server_terminals==0.5.3', 'jupyterlab==4.2.5', 'jupyterlab_pygments==0.3.0', 'jupyterlab_server==2.27.3', 'jupyterlab_widgets==3.0.13', 'kiwisolver==1.4.5', 'lazy_loader==0.4', 'llvmlite==0.43.0', 'MarkupSafe==2.1.5', 'matplotlib==3.8.4', 'matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1713250518406/work', 'mistune==3.0.2', 'more-itertools==10.3.0', 'mpmath==1.3.0', 'msgpack==1.0.8', 'mypy-extensions==1.0.0', 'nbclient==0.10.0', 'nbconvert==7.16.4', 'nbformat==5.10.4', 'nest_asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1705850609492/work', 'networkx==3.2.1', 'notebook==7.2.1', 'notebook_shim==0.2.4', 'numba==0.60.0', 'numexpr==2.10.1', 'numpy==1.26.3', 'nvidia-cublas-cu11==11.11.3.6', 'nvidia-cuda-cupti-cu11==11.8.87', 'nvidia-cuda-nvrtc-cu11==11.8.89', 'nvidia-cuda-runtime-cu11==11.8.89', 'nvidia-cuda-runtime-cu12==12.6.37', 'nvidia-cudnn-cu11==9.1.0.70', 'nvidia-cufft-cu11==10.9.0.58', 'nvidia-curand-cu11==10.3.0.86', 'nvidia-cusolver-cu11==11.4.1.48', 'nvidia-cusparse-cu11==11.7.5.86', 'nvidia-nccl-cu11==2.20.5', 'nvidia-nvtx-cu11==11.8.86', 'onnx==1.16.2', 'onnxconverter-common==1.14.0', 'onnxruntime-gpu==1.19.0', 'opencv-python==4.10.0.84', 'opencv-python-headless==4.10.0.84', 'ordered-set==4.1.0', 'overrides==7.7.0', 'packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1718189413536/work', 'pandas==2.2.2', 'pandocfilters==1.5.1', 'parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1712320355065/work', 'pathspec==0.12.1', 'patsy==0.5.6', 'pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1706113125309/work', 'pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work', 'pillow==10.2.0', 'pip @ file:///croot/pip_1723484598856/work', 'platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1715777629804/work', 'prometheus_client==0.20.0', 'prompt_toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1718047967974/work', 'protobuf==3.20.2', 'psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1719274566094/work', 'ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl', 'pure_eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1721585709575/work', 'py-cpuinfo==9.0.0', 'pycocotools==2.0.8', 'pycparser==2.22', 'Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1714846767233/work', 'pyparsing==3.1.2', 'python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1709299778482/work', 'python-json-logger==2.0.7', 'pytz==2024.1', 'PyYAML==6.0.2', 'pyzmq @ file:///croot/pyzmq_1705605076900/work', 'referencing==0.35.1', 'requests==2.32.3', 'rfc3339-validator==0.1.4', 'rfc3986-validator==0.1.1', 'rpds-py==0.20.0', 'ruamel.yaml==0.18.6', 'ruamel.yaml.clib==0.2.8', 'safetensors==0.4.4', 'scikit-image==0.24.0', 'scikit-learn==1.5.1', 'scipy==1.10.1', 'Send2Trash==1.8.3', 'setuptools==72.1.0', 'shapely==2.0.6', 'six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work', 'snakeviz==2.2.0', 'sniffio==1.3.1', 'soupsieve==2.6', 'stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work', 'statsmodels==0.14.2', 'sympy==1.12', 'tables==3.8.0', 'tensorrt==10.1.0', 'tensorrt-cu12==10.3.0', 'tensorrt-cu12-bindings==10.1.0', 'tensorrt-cu12-libs==10.1.0', 'terminado==0.18.1', 'threadpoolctl==3.5.0', 'tifffile==2024.8.10', 'timm==1.0.8', 'tinycss2==1.3.0', 'tokenize-rt==6.0.0', 'tomli==2.0.1', 'torch==2.4.0+cu118', 'torch_tensorrt==2.4.0+cu118', 'torchvision==0.19.0+cu118', 'tornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1717722796999/work', 'tqdm==4.66.5', 'traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1713535121073/work', 'triton==3.0.0', 'typeguard==4.3.0', 'types-python-dateutil==2.9.0.20240821', 'typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1717802530399/work', 'tzdata==2024.1', 'uri-template==1.3.0', 'urllib3==2.2.2', 'wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1704731205417/work', 'webcolors==24.8.0', 'webencodings==0.5.1', 'websocket-client==1.8.0', 'wheel==0.43.0', 'widgetsnbextension==3.6.8', 'zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1723591248676/work'], 'python_version': '3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]', 'git_hash': '1284c59227702eccc5a5d5abcc48d1ced3f64e06', 'dlclive_version': '1.0.4'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# short video\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_1s.avi'\u001b[39;00m\n\u001b[1;32m     14\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird_ssdlite/long_bird.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m poses, times \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpytorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnapshot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msnapshot-100.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# precision=\"FP16\",\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cropping= [50, 250, 100, 450], # manually set the cropping to specific pixels\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dynamic=(True,0.5,10,),\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird_ssdlite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_poses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird_ssdlite/out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdraw_keypoint_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MyHub/Code/DLC24_Hub/DLC_AI2024/DeepLabCut-live/dlclive/benchmark_pytorch.py:288\u001b[0m, in \u001b[0;36manalyze_video\u001b[0;34m(video_path, model_path, model_type, device, precision, snapshot, display, pcutoff, display_radius, resize, cropping, dynamic, save_poses, save_dir, draw_keypoint_names, cmap, get_sys_info)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(get_system_info())\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_poses:\n\u001b[0;32m--> 288\u001b[0m     \u001b[43msave_poses_to_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbodyparts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m poses, times\n",
      "File \u001b[0;32m~/MyHub/Code/DLC24_Hub/DLC_AI2024/DeepLabCut-live/dlclive/benchmark_pytorch.py:325\u001b[0m, in \u001b[0;36msave_poses_to_files\u001b[0;34m(video_path, save_dir, bodyparts, poses)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m poses:\n\u001b[1;32m    324\u001b[0m     frame_num \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 325\u001b[0m     pose \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mposes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    326\u001b[0m     row \u001b[38;5;241m=\u001b[39m [frame_num] \u001b[38;5;241m+\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m kp \u001b[38;5;129;01min\u001b[39;00m pose \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m kp]\n\u001b[1;32m    327\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow(row)\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# test download of benchmarking dataset\n",
    "# OBS link it not working, waiting for updated link to benchmarking dataset\n",
    "\n",
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird_ssdlite\",\n",
    "    device=\"cuda\",\n",
    "    snapshot=\"snapshot-100.pt\",\n",
    "    model_type=\"onnx\",\n",
    "    display=True,\n",
    "    precision=\"FP16\",\n",
    ")\n",
    "# short video\n",
    "# video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_1s.avi'\n",
    "video_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird_ssdlite/long_bird.mp4\"\n",
    "\n",
    "poses, times = analyze_video(\n",
    "    video_path=video_path,\n",
    "    model_type=\"pytorch\",\n",
    "    snapshot = \"snapshot-100.pt\",\n",
    "    device=\"cuda\",\n",
    "    # precision=\"FP16\",\n",
    "    resize=0.1,\n",
    "    # cropping= [50, 250, 100, 450], # manually set the cropping to specific pixels\n",
    "    # dynamic=(True,0.5,10,),\n",
    "    model_path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird_ssdlite\",\n",
    "    display=True,\n",
    "    save_poses=True,\n",
    "    save_dir=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird_ssdlite/out\",\n",
    "    draw_keypoint_names=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [p[\"pose\"][1] for p in poses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean ± std inference time per frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean inference time excluding 1st inference  61.88 ms ± 1.43\n",
      "Mean inference time including 1st inference  66.34 ms ± 108.21\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Mean inference time excluding 1st inference \",\n",
    "    np.round(np.mean(times[1:]) * 1000, 2),\n",
    "    \"ms ±\",\n",
    "    np.round(np.std(times[1:]) * 1000, 2),\n",
    ")\n",
    "print(\n",
    "    \"Mean inference time including 1st inference \",\n",
    "    np.round(np.mean(times) * 1000, 2),\n",
    "    \"ms ±\",\n",
    "    np.round(np.std(times) * 1000, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x72886977dc30>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(times[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute fps from inference times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average FPS: 16.142290172732356\n",
      "Standard Deviation of FPS: 0.7541644897674918\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_fps_stats(inference_times):\n",
    "    \"\"\"\n",
    "    Calculates the average FPS rate and its standard deviation from a list of inference times.\n",
    "\n",
    "    Args:\n",
    "        inference_times (list): A list of inference times in seconds.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the average FPS rate and its standard deviation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate FPS for each frame\n",
    "    fps_values = [1 / time for time in inference_times]\n",
    "\n",
    "    # Calculate average FPS\n",
    "    average_fps = np.mean(fps_values)\n",
    "\n",
    "    # Calculate standard deviation of FPS\n",
    "    std_dev_fps = np.std(fps_values)\n",
    "\n",
    "    return average_fps, std_dev_fps\n",
    "\n",
    "\n",
    "average_fps, std_dev_fps = calculate_fps_stats(times)\n",
    "\n",
    "print(\"Average FPS:\", average_fps)\n",
    "print(\"Standard Deviation of FPS:\", std_dev_fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'46.16 MB'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_model_size(model_path):\n",
    "    \"\"\"\n",
    "    Calculates the size of an ONNX model in bytes.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): The path to the ONNX model file.\n",
    "\n",
    "    Returns:\n",
    "        int: The size of the model in bytes.\n",
    "    \"\"\"\n",
    "    if \".onnx\" in model_path:\n",
    "        model = onnx.load(model_path)\n",
    "        size_bytes = len(model.SerializeToString())\n",
    "    elif \".pt\" in model_path:\n",
    "        model = torch.load(model_path)\n",
    "        params = list(model.parameters())\n",
    "        size_bytes = sum([p.numel() * p.element_size() for p in params])\n",
    "\n",
    "    # Convert to KB, MB, GB, etc.\n",
    "    if size_bytes < 1024:\n",
    "        size_str = f\"{size_bytes} B\"\n",
    "    elif size_bytes < 1024 * 1024:\n",
    "        size_str = f\"{size_bytes / 1024:.2f} KB\"\n",
    "    elif size_bytes < 1024 * 1024 * 1024:\n",
    "        size_str = f\"{size_bytes / (1024 * 1024):.2f} MB\"\n",
    "    else:\n",
    "        size_str = f\"{size_bytes / (1024 * 1024 * 1024):.2f} GB\"\n",
    "\n",
    "    return size_str\n",
    "\n",
    "\n",
    "get_model_size(\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/resnet_fp16.onnx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get video characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 25.0\n",
      "Number of frames: 500.0\n",
      "Video length (seconds): 20.0\n",
      "Frame size: (480.0, 270.0)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def get_video_characteristics(video_path):\n",
    "    \"\"\"\n",
    "    Extracts the FPS, number of frames, length in seconds, and frame size of a video.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): The path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the FPS, number of frames, length in seconds, and frame size.\n",
    "    \"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) * 0.25\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) * 0.25\n",
    "\n",
    "    # Calculate video length in seconds\n",
    "    video_length = frame_count / fps\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return fps, frame_count, video_length, (frame_width, frame_height)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "video_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/superbird_ssdlite/4782303-hd_1920_1080_25fps.mp4\"\n",
    "fps, frame_count, video_length, frame_size = get_video_characteristics(video_path)\n",
    "\n",
    "print(\"FPS:\", fps)\n",
    "print(\"Number of frames:\", frame_count)\n",
    "print(\"Video length (seconds):\", video_length)\n",
    "print(\"Frame size:\", frame_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc-live",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
