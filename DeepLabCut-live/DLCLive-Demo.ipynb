{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLC Live PyTorch Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlclive import DLCLive\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from onnxruntime import quantization\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = [\"fly-kevin\", \"hand-track\", \"superbird\", \"ventral-gait\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you do not have a .onnx model exported, use this cell to export your DLC3.0 snapshot\n",
    "\n",
    "from deeplabcut.pose_estimation_pytorch.config import read_config_as_dict\n",
    "from deeplabcut.pose_estimation_pytorch.models import PoseModel\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Dikra\n",
    "root = Path(\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3])\n",
    "model_cfg = read_config_as_dict(root / \"pytorch_config.yaml\")\n",
    "weights_path = root / \"snapshot-263.pt\"\n",
    "\n",
    "# Anna\n",
    "# root = Path(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\")\n",
    "# model_cfg = read_config_as_dict(root / \"pytorch_config.yaml\")\n",
    "# weights_path = root / \"snapshot-263.pt\"\n",
    "\n",
    "model = PoseModel.build(model_cfg[\"model\"])\n",
    "weights = torch.load(weights_path, map_location=device)\n",
    "model.load_state_dict(weights[\"model\"])\n",
    "\n",
    "dummy_input = torch.zeros((1, 3, 224, 224))\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3] + \"/resnet.onnx\",\n",
    "    verbose=False,\n",
    "    input_names=[\"input\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quant ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32 to FP16\n",
    "from onnxconverter_common import float16\n",
    "\n",
    "onnx_fp32_model_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3] + \"/resnet.onnx\"\n",
    ")\n",
    "onnx_fp16_model_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\"\n",
    "    + projects[3]\n",
    "    + \"/resnet_fp16.onnx\"\n",
    ")\n",
    "\n",
    "model_fp32 = onnx.load(onnx_fp32_model_path)\n",
    "model_fp16 = float16.convert_float_to_float16(model_fp32)\n",
    "onnx.save(model_fp16, onnx_fp16_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_fp32_model_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3] + \"/resnet.onnx\"\n",
    ")\n",
    "model_prep_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\"\n",
    "    + projects[3]\n",
    "    + \"/resnet_quant_prep.onnx\"\n",
    ")\n",
    "\n",
    "# prep for quantisation\n",
    "quantization.shape_inference.quant_pre_process(\n",
    "    onnx_fp32_model_path, model_prep_path, skip_symbolic_shape=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test frame\n",
    "img = cv2.imread(\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3] + \"/img0006.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with ONNX exported DLC 3.0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dikra\n",
    "onnx_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3],\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cuda\",\n",
    "    display=True,\n",
    "    precision=\"FP16\",\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# onnx_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# onnx_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "onnx_pose = onnx_dlc_live.init_inference(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot from 2024-08-20 14-29-53.png](./docs/assets/Screenshot%20from%202024-08-20%2014-36-00.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected = onnx_pose[0][\"poses\"][0][0][:, 2] > 0.9\n",
    "print(torch.any(detected))\n",
    "x = onnx_pose[0][\"poses\"][0][0][detected, 0]\n",
    "y = onnx_pose[0][\"poses\"][0][0][detected, 1]\n",
    "onnx_pose[0][\"poses\"][:, :, :, 1][:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_pose = onnx_dlc_live.get_pose(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with snaptshot of DLC 3.0 model (.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dikra\n",
    "pytorch_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait\",\n",
    "    snapshot=\"snapshot-263.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# pytorch_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# pytorch_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "pytorch_pose = pytorch_dlc_live.init_inference(frame=img)\n",
    "pytorch_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PyTorch model inference](./docs/assets/Screenshot%20from%202024-08-20%2014-29-53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "root = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait\"\n",
    "test_images = glob.glob(os.path.normpath(root + \"/*.png\"))\n",
    "\n",
    "\n",
    "def mean_time_inference(dlc_live, images):\n",
    "    times = []\n",
    "    for i, img_p in enumerate(images):\n",
    "        img = cv2.imread(img_p)\n",
    "\n",
    "        if i == 0:\n",
    "            start = time.time()\n",
    "            dlc_live.init_inference(img)\n",
    "            end = time.time()\n",
    "        else:\n",
    "            start = time.time()\n",
    "            dlc_live.get_pose(img)\n",
    "            end = time.time()\n",
    "        times.append(end - start)\n",
    "    print(times)\n",
    "\n",
    "    return np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"onnx\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images)\n",
    "print(\n",
    "    f\"TOTAL Inference of ONNX model took on average {mean_time} seconds for {len(test_images)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    snapshot=\"snapshot-200.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\",\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images)\n",
    "print(\n",
    "    f\"Inference of PyTorch model took on average {mean_time} seconds for {len(test_images)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3],\n",
    "    device=\"tensorrt\",\n",
    "    model_type=\"onnx\",\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images)\n",
    "print(\n",
    "    f\"Inference of PyTorch model took on average {mean_time} seconds for {len(test_images)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dlc_live.get_pose(img)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "Currently the benchmark_pytorch.py script serves to provide a function for analyzing a preexisting video to test PyTorch for running video inference in DLC-Live. Code for running video inference on a live video feed is WIP.\n",
    "\n",
    "For true benchmarking purposes, we aim to add feature for recording the time it takes to analyze each frame / how many frames can be analyzed per second. Discuss what measure to use and consult the DLC Live paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import the analyze_video function from the file where it's defined\n",
    "from dlclive.benchmark_pytorch import analyze_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New version with DLCLive object included in the code\n",
    "\n",
    "\n",
    "# Define the paths\n",
    "video_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/1_20cms_0degUP_first_03s.avi\"\n",
    "model_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait\"\n",
    "\n",
    "# import cProfile\n",
    "# import io\n",
    "# import pstats\n",
    "\n",
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "\n",
    "# Call the analyze_video function with the appropriate arguments\n",
    "poses = analyze_video(\n",
    "    video_path=video_path,\n",
    "    model_path=model_path,\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cuda\",\n",
    "    display=True,\n",
    "    save_poses=True,\n",
    "    resize=0.5,\n",
    "    precision = \"FP16\",\n",
    "    # cropping= [50, 250, 100, 450], # manually set the cropping to specific pixels\n",
    "    dynamic=(\n",
    "        True,\n",
    "        0.5,\n",
    "        10,\n",
    "    ),  # True = we want to apply dynamic cropping, 0.5 = the threshold for accepting a KP as detected, 10 = the margin to expand the calculatted cropping window by so it is not too narrow\n",
    "    save_dir=\"output_directory\",\n",
    "    get_sys_info=True,\n",
    "    draw_keypoint_names=True,\n",
    ")\n",
    "\n",
    "# #'poses' will contain the list of poses detected\n",
    "\n",
    "# # Create a stream to capture the profiler's output\n",
    "# s = io.StringIO()\n",
    "# sortby = 'cumulative'\n",
    "# ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "# ps.print_stats()\n",
    "\n",
    "# # Print the profiling output\n",
    "# print(s.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand model and video\n",
    "\n",
    "# Define the paths\n",
    "video_path = \"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Hand-AnnaStuckert-2024-08-21/videos/Hand.avi\"\n",
    "model_path = \"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Hand-AnnaStuckert-2024-08-21/dlc-models-pytorch/iteration-0/HandAug21-trainset95shuffle101/train\"\n",
    "\n",
    "\n",
    "# Call the analyze_video function with the appropriate arguments\n",
    "poses = analyze_video(\n",
    "    video_path=video_path,\n",
    "    model_path=model_path,\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cuda\",\n",
    "    display=True,\n",
    "    save_poses=True,\n",
    "    resize=0.4,\n",
    "    # cropping= [50, 250, 100, 450], # manually set the cropping to specific pixels\n",
    "    dynamic=(\n",
    "        True,\n",
    "        0.5,\n",
    "        10,\n",
    "    ),  # True = we want to apply dynamic cropping, 0.5 = the threshold for accepting a KP as detected, 10 = the margin to expand the calculatted cropping window by so it is not too narrow\n",
    "    save_dir=\"output_directory\",\n",
    "    get_sys_info=True,\n",
    "    draw_keypoint_names=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlclive import DLCLive\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from onnxruntime import quantization\n",
    "import onnx\n",
    "from dlclive.benchmark_pytorch import analyze_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test download of benchmarking dataset\n",
    "# OBS link it not working, waiting for updated link to benchmarking dataset\n",
    "\n",
    "dlc_live = DLCLive(\n",
    "    path=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\",\n",
    "    device=\"cpu\",\n",
    "    # snapshot=\"snapshot-263.pt\",\n",
    "    model_type=\"onnx\",\n",
    "    display=True,\n",
    "    precision=\"FP16\",\n",
    ")\n",
    "# short video\n",
    "video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first.avi'\n",
    "#video_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/1_20cms_0degUP_first.avi\"\n",
    "\n",
    "poses, times = analyze_video(\n",
    "    video_path=video_path,\n",
    "    model_type=\"pytorch\",\n",
    "    snapshot = \"snapshot-263.pt\",\n",
    "    device=\"cpu\",\n",
    "    #precision=\"FP16\",\n",
    "    model_path=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\",\n",
    "    display=True,\n",
    "    save_poses=False,\n",
    "    save_dir=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/out\",\n",
    "    draw_keypoint_names=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [p[\"pose\"][1] for p in poses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Mean inference time excluding 1st inference \",\n",
    "    np.round(np.mean(times[1:]) * 1000, 2),\n",
    "    \"ms ±\",\n",
    "    np.round(np.std(times[1:]) * 1000, 2),\n",
    ")\n",
    "print(\n",
    "    \"Mean inference time including 1st inference \",\n",
    "    np.round(np.mean(times) * 1000, 2),\n",
    "    \"ms ±\",\n",
    "    np.round(np.std(times) * 1000, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(times[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_fps_stats(inference_times):\n",
    "    \"\"\"\n",
    "    Calculates the average FPS rate and its standard deviation from a list of inference times.\n",
    "\n",
    "    Args:\n",
    "        inference_times (list): A list of inference times in seconds.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the average FPS rate and its standard deviation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate FPS for each frame\n",
    "    fps_values = [1 / time for time in inference_times]\n",
    "\n",
    "    # Calculate average FPS\n",
    "    average_fps = np.mean(fps_values)\n",
    "\n",
    "    # Calculate standard deviation of FPS\n",
    "    std_dev_fps = np.std(fps_values)\n",
    "\n",
    "    return average_fps, std_dev_fps\n",
    "\n",
    "\n",
    "average_fps, std_dev_fps = calculate_fps_stats(times)\n",
    "\n",
    "print(\"Average FPS:\", average_fps)\n",
    "print(\"Standard Deviation of FPS:\", std_dev_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_model_size(model_path):\n",
    "    \"\"\"\n",
    "    Calculates the size of an ONNX model in bytes.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): The path to the ONNX model file.\n",
    "\n",
    "    Returns:\n",
    "        int: The size of the model in bytes.\n",
    "    \"\"\"\n",
    "    if \".onnx\" in model_path:\n",
    "        model = onnx.load(model_path)\n",
    "        size_bytes = len(model.SerializeToString())\n",
    "    elif \".pt\" in model_path:\n",
    "        model = torch.load(model_path)\n",
    "        print(model[\"model\"].keys())\n",
    "        params = list(model.parameters())\n",
    "        size_bytes = sum([p.numel() * p.element_size() for p in params])\n",
    "\n",
    "    # Convert to KB, MB, GB, etc.\n",
    "    if size_bytes < 1024:\n",
    "        size_str = f\"{size_bytes} B\"\n",
    "    elif size_bytes < 1024 * 1024:\n",
    "        size_str = f\"{size_bytes / 1024:.2f} KB\"\n",
    "    elif size_bytes < 1024 * 1024 * 1024:\n",
    "        size_str = f\"{size_bytes / (1024 * 1024):.2f} MB\"\n",
    "    else:\n",
    "        size_str = f\"{size_bytes / (1024 * 1024 * 1024):.2f} GB\"\n",
    "\n",
    "    return size_str\n",
    "\n",
    "\n",
    "get_model_size(\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/resnet_fp16.onnx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def get_video_characteristics(video_path):\n",
    "    \"\"\"\n",
    "    Extracts the FPS, number of frames, length in seconds, and frame size of a video.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): The path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the FPS, number of frames, length in seconds, and frame size.\n",
    "    \"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Calculate video length in seconds\n",
    "    video_length = frame_count / fps\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return fps, frame_count, video_length, (frame_width, frame_height)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "video_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/1_20cms_0degUP_first.avi\"\n",
    "fps, frame_count, video_length, frame_size = get_video_characteristics(video_path)\n",
    "\n",
    "print(\"FPS:\", fps)\n",
    "print(\"Number of frames:\", frame_count)\n",
    "print(\"Video length (seconds):\", video_length)\n",
    "print(\"Frame size:\", frame_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live video analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annastuckert/anaconda3/envs/deeplabcut3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/annastuckert/anaconda3/envs/deeplabcut3/lib/python3.10/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2024-08-27 13:41:18.859 python[41063:553854] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model took 3.9745020866394043 sec\n",
      "PyTorch inference took 0.36063694953918457 sec\n",
      "PyTorch postprocessing took 0.017808198928833008 sec\n",
      "Frame 0 processing time: 5.1871 seconds\n",
      "PyTorch inference took 0.31470799446105957 sec\n",
      "PyTorch postprocessing took 0.0017690658569335938 sec\n",
      "Frame 1 processing time: 0.3718 seconds\n",
      "PyTorch inference took 0.26961493492126465 sec\n",
      "PyTorch postprocessing took 0.001071929931640625 sec\n",
      "Frame 2 processing time: 0.3053 seconds\n",
      "PyTorch inference took 0.20059490203857422 sec\n",
      "PyTorch postprocessing took 0.0009739398956298828 sec\n",
      "Frame 3 processing time: 0.2298 seconds\n",
      "PyTorch inference took 0.19417786598205566 sec\n",
      "PyTorch postprocessing took 0.0009419918060302734 sec\n",
      "Frame 4 processing time: 0.2310 seconds\n",
      "PyTorch inference took 0.18033409118652344 sec\n",
      "PyTorch postprocessing took 0.0012412071228027344 sec\n",
      "Frame 5 processing time: 0.2273 seconds\n",
      "PyTorch inference took 0.21974921226501465 sec\n",
      "PyTorch postprocessing took 0.001628875732421875 sec\n",
      "Frame 6 processing time: 0.2778 seconds\n",
      "PyTorch inference took 0.33596324920654297 sec\n",
      "PyTorch postprocessing took 0.003962993621826172 sec\n",
      "Frame 7 processing time: 0.3916 seconds\n",
      "PyTorch inference took 0.3463170528411865 sec\n",
      "PyTorch postprocessing took 0.002479076385498047 sec\n",
      "Frame 8 processing time: 0.4071 seconds\n",
      "PyTorch inference took 0.3336350917816162 sec\n",
      "PyTorch postprocessing took 0.004568815231323242 sec\n",
      "Frame 9 processing time: 0.9710 seconds\n",
      "PyTorch inference took 0.2311239242553711 sec\n",
      "PyTorch postprocessing took 0.0015552043914794922 sec\n",
      "Frame 10 processing time: 0.2781 seconds\n",
      "PyTorch inference took 0.3000330924987793 sec\n",
      "PyTorch postprocessing took 0.0020689964294433594 sec\n",
      "Frame 11 processing time: 0.3460 seconds\n",
      "{'host_name': 'MCKDQTN4YXDV', 'op_sys': 'macOS-14.5-arm64-arm-64bit', 'python': 'deeplabcut3', 'device_type': 'CPU', 'device': ['Apple M2'], 'freeze': ['absl-py==2.1.0', 'alabaster==0.7.16', 'albumentations==1.4.3', 'app-model==0.2.7', 'appdirs==1.4.4', 'appnope==0.1.4', 'asttokens==2.4.1', 'astunparse==1.6.3', 'attrs==23.2.0', 'Babel==2.15.0', 'blosc2==2.0.0', 'build==1.2.1', 'cachetools==5.4.0', 'cachey==0.2.1', 'certifi==2024.7.4', 'charset-normalizer==3.3.2', 'click==8.1.7', 'cloudpickle==3.0.0', 'colorcet==3.1.0', 'coloredlogs==15.0.1', 'comm==0.2.2', 'contourpy==1.2.1', 'cycler==0.12.1', 'Cython==3.0.10', 'dask==2024.7.0', 'dask-expr==1.1.7', 'dask-image==2024.5.3', 'debugpy==1.8.2', 'decorator==5.1.1', 'deeplabcut @ git+https://github.com/DeepLabCut/DeepLabCut.git@83f1acb9c179caf46353cdb211318b29832ff6ec', 'deeplabcut-live==1.0.2', 'deeplabcut-live-gui @ file:///Users/annastuckert/Documents/DeepLabCut-live-GUI', 'dill==0.3.8', 'dlclibrary==0.0.6', 'docker-pycreds==0.4.0', 'docstring_parser==0.16', 'docutils==0.17.1', 'einops==0.8.0', 'exceptiongroup==1.2.2', 'executing==2.0.1', 'filelock==3.15.4', 'filterpy==1.4.5', 'flatbuffers==24.3.25', 'flexcache==0.3', 'flexparser==0.3.1', 'fonttools==4.53.1', 'freetype-py==2.4.0', 'fsspec==2024.6.1', 'gast==0.4.0', 'gitdb==4.0.11', 'GitPython==3.1.43', 'google-auth==2.32.0', 'google-auth-oauthlib==0.4.6', 'google-pasta==0.2.0', 'grpcio==1.65.4', 'h5py==3.11.0', 'HeapDict==1.0.1', 'hsluv==5.0.4', 'huggingface-hub==0.23.5', 'humanfriendly==10.0', 'idna==3.7', 'imageio==2.34.2', 'imageio-ffmpeg==0.5.1', 'imagesize==1.4.1', 'imgaug==0.4.0', 'importlib_metadata==8.0.0', 'imutils==0.5.4', 'in-n-out==0.2.1', 'ipykernel==6.29.5', 'ipython==8.26.0', 'jedi==0.19.1', 'Jinja2==3.1.4', 'joblib==1.4.2', 'jsonschema==4.23.0', 'jsonschema-specifications==2023.12.1', 'jupyter_client==8.6.2', 'jupyter_core==5.7.2', 'keras==2.11.0', 'kiwisolver==1.4.5', 'lazy_loader==0.4', 'libclang==18.1.1', 'llvmlite==0.43.0', 'locket==1.0.0', 'magicgui==0.8.3', 'Markdown==3.6', 'markdown-it-py==3.0.0', 'MarkupSafe==2.1.5', 'matplotlib==3.8.4', 'matplotlib-inline==0.1.7', 'mdurl==0.1.2', 'memory-profiler==0.61.0', 'ml-dtypes==0.4.0', 'mpmath==1.3.0', 'msgpack==1.0.8', 'msgpack-numpy==0.4.8', 'multiprocess==0.70.16', 'namex==0.0.8', 'napari==0.4.18', 'napari-console==0.0.9', 'napari-deeplabcut==0.2.1.7', 'napari-plugin-engine==0.2.0', 'napari-svg==0.2.0', 'natsort==8.4.0', 'nest-asyncio==1.6.0', 'networkx==3.3', 'npe2==0.7.6', 'numba==0.60.0', 'numexpr @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_45yefq0kt6/croot/numexpr_1696515289183/work', 'numpy==1.26.4', 'numpydoc==1.5.0', 'oauthlib==3.2.2', 'onnx==1.16.2', 'onnxruntime==1.18.1', 'onnxruntime-tools==1.7.0', 'onnxscript==0.1.0.dev20240813', 'opencv-python==4.10.0.84', 'opencv-python-headless==4.10.0.84', 'opt-einsum==3.3.0', 'optree==0.12.1', 'packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1718189413536/work', 'pandas==1.5.3', 'parso==0.8.4', 'partd==1.4.2', 'patsy==0.5.6', 'pexpect==4.9.0', 'pillow==10.4.0', 'PIMS==0.7', 'Pint==0.24.3', 'pip==24.0', 'platformdirs==4.2.2', 'pooch==1.8.2', 'prompt_toolkit==3.0.47', 'protobuf==5.27.3', 'psutil==6.0.0', 'psygnal==0.11.1', 'ptyprocess==0.7.0', 'pure-eval==0.2.2', 'py-cpuinfo @ file:///home/conda/feedstock_root/build_artifacts/py-cpuinfo_1666774466606/work', 'py3nvml==0.2.7', 'pyarrow==17.0.0', 'pyasn1==0.6.0', 'pyasn1_modules==0.4.0', 'pycocotools==2.0.8', 'pyconify==0.1.6', 'pydantic==1.10.17', 'pydantic-compat==0.1.2', 'Pygments==2.18.0', 'PyOpenGL==3.1.7', 'pyparsing==3.1.2', 'pyproject_hooks==1.1.0', 'pyserial==3.5', 'PySide6==6.4.2', 'PySide6-Addons==6.4.2', 'PySide6-Essentials==6.4.2', 'python-dateutil==2.9.0.post0', 'pytz==2024.1', 'PyYAML==6.0.1', 'pyzmq==26.0.3', 'QDarkStyle==3.1', 'qtconsole==5.5.2', 'QtPy==2.4.1', 'referencing==0.35.1', 'requests==2.32.3', 'requests-oauthlib==2.0.0', 'rich==13.7.1', 'rpds-py==0.19.0', 'rsa==4.9', 'ruamel.yaml==0.17.40', 'ruamel.yaml.clib==0.2.8', 'safetensors==0.4.3', 'scikit-image==0.24.0', 'scikit-learn==1.5.1', 'scipy==1.10.1', 'sentry-sdk==2.10.0', 'setproctitle==1.3.3', 'setuptools==69.5.1', 'shapely==2.0.5', 'shellingham==1.5.4', 'shiboken6==6.4.2', 'six==1.16.0', 'slicerator==1.1.0', 'smmap==5.0.1', 'snowballstemmer==2.2.0', 'Sphinx==4.5.0', 'sphinxcontrib-applehelp==1.0.8', 'sphinxcontrib-devhelp==1.0.6', 'sphinxcontrib-htmlhelp==2.0.5', 'sphinxcontrib-jsmath==1.0.1', 'sphinxcontrib-qthelp==1.0.7', 'sphinxcontrib-serializinghtml==1.1.10', 'stack-data==0.6.3', 'statsmodels==0.14.2', 'superqt==0.6.7', 'sympy==1.13.0', 'tables @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_81qnps6056/croot/pytables_1691621471831/work', 'tabulate==0.9.0', 'tensorboard==2.11.2', 'tensorboard-data-server==0.6.1', 'tensorboard-plugin-wit==1.8.1', 'tensorflow-estimator==2.11.0', 'tensorflow-io-gcs-filesystem==0.37.1', 'tensorflow-macos==2.11.0', 'tensorflow-metal==1.1.0', 'tensorpack==0.11', 'termcolor==2.4.0', 'tf-slim==1.1.0', 'threadpoolctl==3.5.0', 'tifffile==2024.7.2', 'timm==1.0.7', 'tomli==2.0.1', 'tomli_w==1.0.0', 'toolz==0.12.1', 'torch==2.3.1', 'torchvision==0.18.1', 'tornado==6.4.1', 'tqdm==4.66.4', 'traitlets==5.14.3', 'typer==0.12.3', 'typing_extensions==4.12.2', 'tzdata==2024.1', 'urllib3==2.2.2', 'vispy==0.12.2', 'wandb==0.17.4', 'wcwidth==0.2.13', 'Werkzeug==3.0.3', 'wheel==0.43.0', 'wrapt==1.16.0', 'xmltodict==0.13.0', 'zipp==3.19.2'], 'python_version': '3.10.14 (main, May  6 2024, 14:42:37) [Clang 14.0.6 ]', 'git_hash': '2051232e8e7be8a108e5bb872c23b573737861fe', 'dlclive_version': '1.0.4'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Hand-AnnaStuckert-2024-08-21/dlc-models-pytorch/iteration-0/HandAug21-trainset95shuffle101/train\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Call the analyze_live_video function with the appropriate arguments\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m poses \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_live_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcamera\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpytorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnapshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msnapshot-200.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_poses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#precision=\"FP16\",\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#cropping=[50, 250, 100, 450],  # manually set the cropping to specific pixels\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#dynamic=(\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#    True,\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#    0.5,\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#    10,\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#),  # True = apply dynamic cropping, 0.5 = threshold for KP detection, 10 = margin for cropping\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_directory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_sys_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdraw_keypoint_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/dlclive/LiveVideoInference.py:254\u001b[0m, in \u001b[0;36manalyze_live_video\u001b[0;34m(model_path, model_type, device, camera, experiment_name, precision, snapshot, display, pcutoff, display_radius, resize, cropping, dynamic, save_poses, save_dir, draw_keypoint_names, cmap, get_sys_info)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mprint\u001b[39m(get_system_info())\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_poses:\n\u001b[0;32m--> 254\u001b[0m     \u001b[43msave_poses_to_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbodyparts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m poses, times\n",
      "File \u001b[0;32m~/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/dlclive/LiveVideoInference.py:291\u001b[0m, in \u001b[0;36msave_poses_to_files\u001b[0;34m(experiment_name, save_dir, bodyparts, poses)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m poses:\n\u001b[1;32m    290\u001b[0m     frame_num \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 291\u001b[0m     pose_data \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Adjusted to use indices instead of keys\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     row \u001b[38;5;241m=\u001b[39m [frame_num] \u001b[38;5;241m+\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m kp \u001b[38;5;129;01min\u001b[39;00m pose_data \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m kp]\n\u001b[1;32m    293\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow(row)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "\n",
    "# Running the analyze_live_video function in a Jupyter notebook\n",
    "\n",
    "from dlclive.LiveVideoInference import analyze_live_video\n",
    "\n",
    "# Define the paths\n",
    "model_path = \"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Hand-AnnaStuckert-2024-08-21/dlc-models-pytorch/iteration-0/HandAug21-trainset95shuffle101/train\"\n",
    "\n",
    "# Call the analyze_live_video function with the appropriate arguments\n",
    "poses = analyze_live_video(\n",
    "    camera=0,\n",
    "    model_path=model_path,\n",
    "    model_type=\"pytorch\",\n",
    "    snapshot=\"snapshot-200.pt\",\n",
    "    device=\"cpu\",\n",
    "    display=True,\n",
    "    save_poses=True,\n",
    "    resize=0.5,\n",
    "    #precision=\"FP16\",\n",
    "    #cropping=[50, 250, 100, 450],  # manually set the cropping to specific pixels\n",
    "    #dynamic=(\n",
    "    #    True,\n",
    "    #    0.5,\n",
    "    #    10,\n",
    "    #),  # True = apply dynamic cropping, 0.5 = threshold for KP detection, 10 = margin for cropping\n",
    "    save_dir=\"output_directory\",\n",
    "    get_sys_info=True,\n",
    "    draw_keypoint_names=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc-live",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
