{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLC Live PyTorch Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc4...\n",
      "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dikra/miniconda3/envs/dlc-live/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dlclive import DLCLive, Processor\n",
    "from dlclive.display import Display\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshot to ONNX model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34268/968774635.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(weights_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# In case you do not have a .onnx model exported, use this cell to export your DLC3.0 snapshot\n",
    "\n",
    "from deeplabcut.pose_estimation_pytorch.config import read_config_as_dict\n",
    "from deeplabcut.pose_estimation_pytorch.models import PoseModel\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "root = Path(\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\")\n",
    "model_cfg = read_config_as_dict(root / \"pytorch_config.yaml\")\n",
    "weights_path = root / \"snapshot-200.pt\"\n",
    "\n",
    "model = PoseModel.build(model_cfg[\"model\"])\n",
    "weights = torch.load(weights_path, map_location=device)\n",
    "model.load_state_dict(weights[\"model\"])\n",
    "\n",
    "dummy_input = torch.zeros((1, 3, 224, 224))\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/resnet.onnx\",\n",
    "    verbose=False,\n",
    "    input_names=[\"input\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test frame\n",
    "img = cv2.imread(\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/img008.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with ONNX exported DLC 3.0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poses': tensor([[[[7.3832e+01, 2.0015e+02, 8.8095e-02],\n",
       "           [6.9048e+01, 2.9947e+02, 2.2019e-01],\n",
       "           [1.1621e+02, 3.4326e+02, 3.1128e-01],\n",
       "           [1.1751e+02, 3.3694e+02, 5.3471e-01],\n",
       "           [1.1515e+02, 3.2886e+02, 6.5886e-01],\n",
       "           [1.3284e+02, 3.1905e+02, 3.0138e-01],\n",
       "           [7.9400e+01, 2.1545e+02, 6.3844e-01],\n",
       "           [5.0567e+01, 3.0373e+02, 2.5542e-01],\n",
       "           [1.2382e+02, 2.5162e+02, 7.6581e-01],\n",
       "           [1.3796e+02, 2.2743e+02, 6.9886e-01],\n",
       "           [1.4197e+02, 2.0919e+02, 6.3363e-01],\n",
       "           [1.6022e+02, 2.7524e+02, 6.3262e-01],\n",
       "           [9.4027e+01, 2.5252e+02, 1.9836e-01],\n",
       "           [1.4154e+02, 3.7739e+02, 8.7275e-01],\n",
       "           [1.4946e+02, 3.6513e+02, 6.4267e-01],\n",
       "           [2.7488e+01, 2.0230e+02, 8.3611e-01],\n",
       "           [2.6656e+01, 3.4699e+02, 8.0565e-01],\n",
       "           [3.1491e+01, 3.9521e+02, 8.7074e-01],\n",
       "           [2.5922e+01, 1.5689e+02, 7.5119e-01],\n",
       "           [3.0520e+01, 1.7030e+02, 8.1288e-01],\n",
       "           [3.6773e+01, 2.3040e+02, 6.7201e-01],\n",
       "           [9.8596e+01, 2.6144e+02, 5.8880e-01],\n",
       "           [1.2013e+02, 3.0822e+02, 6.6743e-01],\n",
       "           [1.0080e+02, 1.9202e+02, 6.6554e-01],\n",
       "           [1.3621e+02, 3.0187e+02, 2.4326e-01]]]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dikra\n",
    "onnx_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cuda\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# onnx_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# onnx_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "onnx_pose = onnx_dlc_live.init_inference(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot from 2024-08-20 14-29-53.png](./docs/assets/Screenshot%20from%202024-08-20%2014-36-00.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poses': tensor([[[[3.6219e+02, 4.1237e+02, 5.0502e-01],\n",
       "           [3.6271e+02, 3.9584e+02, 5.8567e-01],\n",
       "           [3.5676e+02, 3.7446e+02, 7.0505e-01],\n",
       "           [3.4043e+02, 3.5935e+02, 7.1149e-01],\n",
       "           [3.3437e+02, 4.0497e+02, 6.5122e-01],\n",
       "           [3.3772e+02, 4.0689e+02, 4.4919e-01],\n",
       "           [2.0013e+02, 4.3547e+02, 5.2071e-01],\n",
       "           [3.4130e+02, 4.3089e+02, 3.4862e-01],\n",
       "           [2.1540e+02, 3.9427e+02, 7.8372e-01],\n",
       "           [1.9974e+02, 3.7728e+02, 5.2794e-01],\n",
       "           [1.8255e+02, 3.6741e+02, 6.4688e-01],\n",
       "           [2.7519e+02, 3.3175e+02, 5.7645e-01],\n",
       "           [3.5014e+02, 4.3089e+02, 4.1499e-01],\n",
       "           [4.3459e+02, 3.1566e+02, 8.1675e-01],\n",
       "           [4.2561e+02, 2.9912e+02, 8.8807e-01],\n",
       "           [2.4359e+02, 5.0565e+02, 5.7659e-01],\n",
       "           [3.5792e+02, 4.8275e+02, 3.0052e-01],\n",
       "           [4.0234e+02, 4.1449e+02, 5.4220e-01],\n",
       "           [1.7142e+02, 4.8666e+02, 4.5839e-01],\n",
       "           [3.2110e+02, 5.1144e+02, 2.5084e-01],\n",
       "           [3.9914e+02, 4.1933e+02, 6.2746e-01],\n",
       "           [2.3576e+02, 4.3456e+02, 6.5916e-01],\n",
       "           [3.1890e+02, 4.2855e+02, 6.0924e-01],\n",
       "           [1.7677e+02, 4.1291e+02, 3.5336e-01],\n",
       "           [3.1286e+02, 4.2730e+02, 6.1907e-01]]]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_pose = onnx_dlc_live.get_pose(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with snaptshot of DLC 3.0 model (.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dikra/MyHub/Code/DLC24_Hub/DLC_AI2024/DeepLabCut-live/dlclive/dlclive.py:260: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(model_path, map_location=torch.device(self.device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'poses': tensor([[[[4.6099e+02, 3.0565e+02, 2.2497e-01],\n",
       "           [4.6334e+02, 2.9737e+02, 5.2393e-01],\n",
       "           [4.7105e+02, 2.9226e+02, 5.7827e-01],\n",
       "           [4.7816e+02, 2.9004e+02, 6.5970e-01],\n",
       "           [5.7700e+02, 3.4287e+02, 6.7695e-01],\n",
       "           [4.4610e+02, 3.3317e+02, 6.4596e-01],\n",
       "           [5.3632e+02, 4.2195e+02, 5.4373e-01],\n",
       "           [4.5563e+02, 3.5570e+02, 2.5123e-01],\n",
       "           [5.5438e+02, 3.6476e+02, 6.2622e-01],\n",
       "           [5.3584e+02, 3.6377e+02, 6.4735e-01],\n",
       "           [5.1307e+02, 3.6640e+02, 6.0587e-01],\n",
       "           [5.2159e+02, 2.9548e+02, 4.5409e-01],\n",
       "           [4.8214e+02, 3.8471e+02, 2.4118e-01],\n",
       "           [4.8334e+02, 2.1336e+02, 7.4011e-01],\n",
       "           [4.5919e+02, 2.0870e+02, 6.2153e-01],\n",
       "           [6.0561e+02, 4.7224e+02, 5.8598e-01],\n",
       "           [5.9742e+02, 4.2868e+02, 4.8355e-01],\n",
       "           [5.7910e+02, 4.2470e+02, 2.6089e-01],\n",
       "           [4.6775e+02, 4.9880e+02, 8.5475e-01],\n",
       "           [4.4069e+02, 4.2392e+02, 6.7025e-01],\n",
       "           [4.3888e+02, 4.0817e+02, 6.0956e-01],\n",
       "           [5.7074e+02, 4.0371e+02, 5.8136e-01],\n",
       "           [5.8398e+02, 3.5102e+02, 9.8475e-01],\n",
       "           [4.8705e+02, 4.1552e+02, 7.0512e-01],\n",
       "           [4.4030e+02, 3.6436e+02, 7.3079e-01]]]], grad_fn=<CopySlices>)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dikra\n",
    "pytorch_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    snapshot=\"snapshot-200.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# pytorch_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# pytorch_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "pytorch_pose = pytorch_dlc_live.init_inference(frame=img)\n",
    "pytorch_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PyTorch model inference](./docs/assets/Screenshot%20from%202024-08-20%2014-29-53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "root = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\"\n",
    "test_images = glob.glob(os.path.normpath(root + \"/*.png\"))\n",
    "\n",
    "def mean_time_inference(dlc_live, images):\n",
    "    times = []\n",
    "    for i, img_p in enumerate(images):\n",
    "        img = cv2.imread(img_p)\n",
    "        \n",
    "        if i == 0: \n",
    "            start = time.time()\n",
    "            dlc_live.init_inference(img)\n",
    "            end = time.time()\n",
    "        else:\n",
    "            start = time.time()\n",
    "            dlc_live.get_pose(img)\n",
    "            end = time.time()\n",
    "        times.append(end-start)\n",
    "    print(times)\n",
    "        \n",
    "    return np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model took 0.17346596717834473 sec\n",
      "ONNX inference took 1.6851527690887451 sec\n",
      "ONNX inference took 0.04920220375061035 sec\n",
      "ONNX inference took 0.0482325553894043 sec\n",
      "ONNX inference took 0.04711294174194336 sec\n",
      "ONNX inference took 0.046735286712646484 sec\n",
      "ONNX inference took 0.045974016189575195 sec\n",
      "ONNX inference took 0.04752397537231445 sec\n",
      "ONNX inference took 0.046564340591430664 sec\n",
      "ONNX inference took 0.04615926742553711 sec\n",
      "ONNX inference took 0.046762704849243164 sec\n",
      "ONNX inference took 0.04644441604614258 sec\n",
      "ONNX inference took 0.046222686767578125 sec\n",
      "ONNX inference took 0.046541452407836914 sec\n",
      "ONNX inference took 0.04626131057739258 sec\n",
      "ONNX inference took 0.04604172706604004 sec\n",
      "ONNX inference took 0.04759335517883301 sec\n",
      "ONNX inference took 0.046645402908325195 sec\n",
      "ONNX inference took 0.04614067077636719 sec\n",
      "ONNX inference took 0.04676055908203125 sec\n",
      "ONNX inference took 0.04682660102844238 sec\n",
      "ONNX inference took 0.0465548038482666 sec\n",
      "ONNX inference took 0.046376705169677734 sec\n",
      "ONNX inference took 0.04623556137084961 sec\n",
      "ONNX inference took 0.046425580978393555 sec\n",
      "ONNX inference took 0.04755902290344238 sec\n",
      "ONNX inference took 0.046710968017578125 sec\n",
      "ONNX inference took 0.04794597625732422 sec\n",
      "ONNX inference took 0.04674696922302246 sec\n",
      "ONNX inference took 0.04719376564025879 sec\n",
      "ONNX inference took 0.046510934829711914 sec\n",
      "ONNX inference took 0.04696512222290039 sec\n",
      "ONNX inference took 0.04720306396484375 sec\n",
      "ONNX inference took 0.04622650146484375 sec\n",
      "ONNX inference took 0.046260833740234375 sec\n",
      "ONNX inference took 0.046245574951171875 sec\n",
      "ONNX inference took 0.04661297798156738 sec\n",
      "ONNX inference took 0.04659891128540039 sec\n",
      "ONNX inference took 0.04720282554626465 sec\n",
      "[1.9850878715515137, 0.06644725799560547, 0.061945199966430664, 0.060353994369506836, 0.0621340274810791, 0.07187914848327637, 0.07168197631835938, 0.06368231773376465, 0.06199502944946289, 0.05839681625366211, 0.06230640411376953, 0.0626535415649414, 0.06188154220581055, 0.05843377113342285, 0.06157350540161133, 0.06630277633666992, 0.060095787048339844, 0.06838130950927734, 0.06496858596801758, 0.1365342140197754, 0.06504464149475098, 0.0613551139831543, 0.061731815338134766, 0.06416821479797363, 0.06530451774597168, 0.10387659072875977, 0.06398320198059082, 0.08550095558166504, 0.06295943260192871, 0.07075834274291992, 0.06714916229248047, 0.06315469741821289, 0.06544995307922363, 0.05960869789123535, 0.05986213684082031, 0.060530900955200195, 0.06686210632324219, 0.07365798950195312]\n",
      "TOTAL Inference of ONNX model took on average 0.11809719863690828 seconds for 38 images\n"
     ]
    }
   ],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"onnx\",\n",
    "    display=True\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images) \n",
    "print(f\"TOTAL Inference of ONNX model took on average {mean_time} seconds for {len(test_images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dikra/MyHub/Code/DLC24_Hub/DLC_AI2024/DeepLabCut-live/dlclive/dlclive.py:256: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(model_path, map_location=torch.device(self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model took 0.29868006706237793 sec\n",
      "PyTorch inference took 0.00905299186706543 sec\n",
      "PyTorch inference took 0.01348423957824707 sec\n",
      "PyTorch inference took 0.04098796844482422 sec\n",
      "PyTorch inference took 0.026604652404785156 sec\n",
      "PyTorch inference took 0.03290820121765137 sec\n",
      "PyTorch inference took 0.013537883758544922 sec\n",
      "PyTorch inference took 0.015102624893188477 sec\n",
      "PyTorch inference took 0.01038050651550293 sec\n",
      "PyTorch inference took 0.013216733932495117 sec\n",
      "PyTorch inference took 0.01572728157043457 sec\n",
      "PyTorch inference took 0.0166168212890625 sec\n",
      "PyTorch inference took 0.03399467468261719 sec\n",
      "PyTorch inference took 0.03161883354187012 sec\n",
      "PyTorch inference took 0.021871089935302734 sec\n",
      "PyTorch inference took 0.014591455459594727 sec\n",
      "PyTorch inference took 0.012326240539550781 sec\n",
      "PyTorch inference took 0.031171083450317383 sec\n",
      "PyTorch inference took 0.009276866912841797 sec\n",
      "PyTorch inference took 0.009023189544677734 sec\n",
      "PyTorch inference took 0.038542985916137695 sec\n",
      "PyTorch inference took 0.015506982803344727 sec\n",
      "PyTorch inference took 0.016647815704345703 sec\n",
      "PyTorch inference took 0.012881994247436523 sec\n",
      "PyTorch inference took 0.014306068420410156 sec\n",
      "PyTorch inference took 0.01612687110900879 sec\n",
      "PyTorch inference took 0.012797117233276367 sec\n",
      "PyTorch inference took 0.01181483268737793 sec\n",
      "PyTorch inference took 0.015043973922729492 sec\n",
      "PyTorch inference took 0.035890817642211914 sec\n",
      "PyTorch inference took 0.016307830810546875 sec\n",
      "PyTorch inference took 0.048410654067993164 sec\n",
      "PyTorch inference took 0.027475595474243164 sec\n",
      "PyTorch inference took 0.004244327545166016 sec\n",
      "PyTorch inference took 0.004690408706665039 sec\n",
      "PyTorch inference took 0.005373716354370117 sec\n",
      "PyTorch inference took 0.007262468338012695 sec\n",
      "PyTorch inference took 0.003587484359741211 sec\n",
      "PyTorch inference took 0.003683805465698242 sec\n",
      "[0.6278479099273682, 0.08278083801269531, 0.09976029396057129, 0.08005380630493164, 0.09908223152160645, 0.07729458808898926, 0.10276460647583008, 0.05671191215515137, 0.08460450172424316, 0.10910654067993164, 0.10513424873352051, 0.0843665599822998, 0.10838437080383301, 0.09122014045715332, 0.10998225212097168, 0.051966190338134766, 0.09129953384399414, 0.06490683555603027, 0.05529308319091797, 0.0994558334350586, 0.1063694953918457, 0.11370992660522461, 0.09168267250061035, 0.09639382362365723, 0.09421038627624512, 0.08558034896850586, 0.07834959030151367, 0.06937289237976074, 0.07900738716125488, 0.08463335037231445, 0.11763596534729004, 0.06942367553710938, 0.05702996253967285, 0.05011177062988281, 0.05707502365112305, 0.046692609786987305, 0.04632258415222168, 0.04564833641052246]\n",
      "Inference of PyTorch model took on average 0.09661226523549933 seconds for 38 images\n"
     ]
    }
   ],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    snapshot=\"snapshot-200.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\",\n",
    "    display=True\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images) \n",
    "print(f\"Inference of PyTorch model took on average {mean_time} seconds for {len(test_images)} images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc-live",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
