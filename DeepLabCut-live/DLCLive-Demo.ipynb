{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLC Live PyTorch Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annastuckert/anaconda3/envs/deeplabcut3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dlclive import DLCLive\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from onnxruntime import quantization\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = [\"fly-kevin\", \"hand-track\", \"superbird\", \"ventral-gait\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/pytorch_config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Dikra\u001b[39;00m\n\u001b[1;32m     12\u001b[0m root \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m projects[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m model_cfg \u001b[38;5;241m=\u001b[39m \u001b[43mread_config_as_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpytorch_config.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m weights_path \u001b[38;5;241m=\u001b[39m root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnapshot-263.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Anna\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# root = Path(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\")\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# model_cfg = read_config_as_dict(root / \"pytorch_config.yaml\")\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# weights_path = root / \"snapshot-263.pt\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplabcut3/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/config/utils.py:186\u001b[0m, in \u001b[0;36mread_config_as_dict\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_config_as_dict\u001b[39m(config_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m        config_path: the path to the configuration file to load\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m        The configuration file with pure Python classes\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    187\u001b[0m         cfg \u001b[38;5;241m=\u001b[39m YAML(typ\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msafe\u001b[39m\u001b[38;5;124m'\u001b[39m, pure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cfg\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/pytorch_config.yaml'"
     ]
    }
   ],
   "source": [
    "# In case you do not have a .onnx model exported, use this cell to export your DLC3.0 snapshot\n",
    "\n",
    "from deeplabcut.pose_estimation_pytorch.config import read_config_as_dict\n",
    "from deeplabcut.pose_estimation_pytorch.models import PoseModel\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Dikra\n",
    "root = Path(\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3])\n",
    "model_cfg = read_config_as_dict(root / \"pytorch_config.yaml\")\n",
    "weights_path = root / \"snapshot-263.pt\"\n",
    "\n",
    "# Anna\n",
    "# root = Path(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\")\n",
    "# model_cfg = read_config_as_dict(root / \"pytorch_config.yaml\")\n",
    "# weights_path = root / \"snapshot-263.pt\"\n",
    "\n",
    "model = PoseModel.build(model_cfg[\"model\"])\n",
    "weights = torch.load(weights_path, map_location=device)\n",
    "model.load_state_dict(weights[\"model\"])\n",
    "\n",
    "dummy_input = torch.zeros((1, 3, 224, 224))\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3] + \"/resnet.onnx\",\n",
    "    verbose=False,\n",
    "    input_names=[\"input\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quant ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32 to FP16\n",
    "from onnxconverter_common import float16\n",
    "\n",
    "onnx_fp32_model_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3] + \"/resnet.onnx\"\n",
    ")\n",
    "onnx_fp16_model_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\"\n",
    "    + projects[3]\n",
    "    + \"/resnet_fp16.onnx\"\n",
    ")\n",
    "\n",
    "model_fp32 = onnx.load(onnx_fp32_model_path)\n",
    "model_fp16 = float16.convert_float_to_float16(model_fp32)\n",
    "onnx.save(model_fp16, onnx_fp16_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_fp32_model_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3] + \"/resnet.onnx\"\n",
    ")\n",
    "model_prep_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\"\n",
    "    + projects[3]\n",
    "    + \"/resnet_quant_prep.onnx\"\n",
    ")\n",
    "\n",
    "# prep for quantisation\n",
    "quantization.shape_inference.quant_pre_process(\n",
    "    onnx_fp32_model_path, model_prep_path, skip_symbolic_shape=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test frame\n",
    "img = cv2.imread(\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3] + \"/img0006.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with ONNX exported DLC 3.0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dikra\n",
    "onnx_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3],\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cuda\",\n",
    "    display=True,\n",
    "    precision=\"FP16\",\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# onnx_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# onnx_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "onnx_pose = onnx_dlc_live.init_inference(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot from 2024-08-20 14-29-53.png](./docs/assets/Screenshot%20from%202024-08-20%2014-36-00.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected = onnx_pose[0][\"poses\"][0][0][:, 2] > 0.9\n",
    "print(torch.any(detected))\n",
    "x = onnx_pose[0][\"poses\"][0][0][detected, 0]\n",
    "y = onnx_pose[0][\"poses\"][0][0][detected, 1]\n",
    "onnx_pose[0][\"poses\"][:, :, :, 1][:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_pose = onnx_dlc_live.get_pose(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with snaptshot of DLC 3.0 model (.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dikra\n",
    "pytorch_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait\",\n",
    "    snapshot=\"snapshot-263.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# pytorch_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# pytorch_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "pytorch_pose = pytorch_dlc_live.init_inference(frame=img)\n",
    "pytorch_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PyTorch model inference](./docs/assets/Screenshot%20from%202024-08-20%2014-29-53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "root = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait\"\n",
    "test_images = glob.glob(os.path.normpath(root + \"/*.png\"))\n",
    "\n",
    "\n",
    "def mean_time_inference(dlc_live, images):\n",
    "    times = []\n",
    "    for i, img_p in enumerate(images):\n",
    "        img = cv2.imread(img_p)\n",
    "\n",
    "        if i == 0:\n",
    "            start = time.time()\n",
    "            dlc_live.init_inference(img)\n",
    "            end = time.time()\n",
    "        else:\n",
    "            start = time.time()\n",
    "            dlc_live.get_pose(img)\n",
    "            end = time.time()\n",
    "        times.append(end - start)\n",
    "    print(times)\n",
    "\n",
    "    return np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"onnx\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images)\n",
    "print(\n",
    "    f\"TOTAL Inference of ONNX model took on average {mean_time} seconds for {len(test_images)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    snapshot=\"snapshot-200.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\",\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images)\n",
    "print(\n",
    "    f\"Inference of PyTorch model took on average {mean_time} seconds for {len(test_images)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3],\n",
    "    device=\"tensorrt\",\n",
    "    model_type=\"onnx\",\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images)\n",
    "print(\n",
    "    f\"Inference of PyTorch model took on average {mean_time} seconds for {len(test_images)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dlc_live.get_pose(img)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "Currently the benchmark_pytorch.py script serves to provide a function for analyzing a preexisting video to test PyTorch for running video inference in DLC-Live. Code for running video inference on a live video feed is WIP.\n",
    "\n",
    "For true benchmarking purposes, we aim to add feature for recording the time it takes to analyze each frame / how many frames can be analyzed per second. Discuss what measure to use and consult the DLC Live paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import the analyze_video function from the file where it's defined\n",
    "from dlclive.benchmark_pytorch import analyze_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New version with DLCLive object included in the code\n",
    "\n",
    "\n",
    "# Define the paths\n",
    "video_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/1_20cms_0degUP_first_03s.avi\"\n",
    "model_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait\"\n",
    "\n",
    "# import cProfile\n",
    "# import io\n",
    "# import pstats\n",
    "\n",
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "\n",
    "# Call the analyze_video function with the appropriate arguments\n",
    "poses = analyze_video(\n",
    "    video_path=video_path,\n",
    "    model_path=model_path,\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cuda\",\n",
    "    display=True,\n",
    "    save_poses=True,\n",
    "    resize=0.5,\n",
    "    precision = \"FP16\",\n",
    "    # cropping= [50, 250, 100, 450], # manually set the cropping to specific pixels\n",
    "    dynamic=(\n",
    "        True,\n",
    "        0.5,\n",
    "        10,\n",
    "    ),  # True = we want to apply dynamic cropping, 0.5 = the threshold for accepting a KP as detected, 10 = the margin to expand the calculatted cropping window by so it is not too narrow\n",
    "    save_dir=\"output_directory\",\n",
    "    get_sys_info=True,\n",
    "    draw_keypoint_names=True,\n",
    ")\n",
    "\n",
    "# #'poses' will contain the list of poses detected\n",
    "\n",
    "# # Create a stream to capture the profiler's output\n",
    "# s = io.StringIO()\n",
    "# sortby = 'cumulative'\n",
    "# ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "# ps.print_stats()\n",
    "\n",
    "# # Print the profiling output\n",
    "# print(s.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand model and video\n",
    "\n",
    "# Define the paths\n",
    "video_path = \"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Hand-AnnaStuckert-2024-08-21/videos/Hand.avi\"\n",
    "model_path = \"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Hand-AnnaStuckert-2024-08-21/dlc-models-pytorch/iteration-0/HandAug21-trainset95shuffle101/train\"\n",
    "\n",
    "\n",
    "# Call the analyze_video function with the appropriate arguments\n",
    "poses = analyze_video(\n",
    "    video_path=video_path,\n",
    "    model_path=model_path,\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cuda\", =\"tensorrt\"\n",
    "    display=True,\n",
    "    save_poses=True,\n",
    "    resize=0.4,\n",
    "    # cropping= [50, 250, 100, 450], # manually set the cropping to specific pixels\n",
    "    dynamic=(\n",
    "        True,\n",
    "        0.5,\n",
    "        10,\n",
    "    ),  # True = we want to apply dynamic cropping, 0.5 = the threshold for accepting a KP as detected, 10 = the margin to expand the calculatted cropping window by so it is not too narrow\n",
    "    save_dir=\"output_directory\",\n",
    "    get_sys_info=True,\n",
    "    draw_keypoint_names=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model took 0.581294059753418 sec\n",
      "PyTorch inference took 0.2523162364959717 sec\n",
      "PyTorch postprocessing took 0.022411108016967773 sec\n",
      "Frame 0 processing time: 1.9604 seconds\n",
      "PyTorch inference took 0.3194701671600342 sec\n",
      "PyTorch postprocessing took 0.0045092105865478516 sec\n",
      "Frame 1 processing time: 0.3742 seconds\n",
      "PyTorch inference took 0.23167109489440918 sec\n",
      "PyTorch postprocessing took 0.0009129047393798828 sec\n",
      "Frame 2 processing time: 0.2579 seconds\n",
      "PyTorch inference took 0.20815587043762207 sec\n",
      "PyTorch postprocessing took 0.0009920597076416016 sec\n",
      "Frame 3 processing time: 0.2354 seconds\n",
      "PyTorch inference took 0.27262210845947266 sec\n",
      "PyTorch postprocessing took 0.001378774642944336 sec\n",
      "Frame 4 processing time: 0.3105 seconds\n",
      "PyTorch inference took 0.3453950881958008 sec\n",
      "PyTorch postprocessing took 0.002165079116821289 sec\n",
      "Frame 5 processing time: 0.3846 seconds\n",
      "PyTorch inference took 0.4133799076080322 sec\n",
      "PyTorch postprocessing took 0.003525972366333008 sec\n",
      "Frame 6 processing time: 0.4535 seconds\n",
      "PyTorch inference took 0.7065131664276123 sec\n",
      "PyTorch postprocessing took 0.0016870498657226562 sec\n",
      "Frame 7 processing time: 0.7474 seconds\n",
      "PyTorch inference took 0.48069286346435547 sec\n",
      "PyTorch postprocessing took 0.002135753631591797 sec\n",
      "Frame 8 processing time: 0.5191 seconds\n",
      "PyTorch inference took 0.43644285202026367 sec\n",
      "PyTorch postprocessing took 0.0025441646575927734 sec\n",
      "Frame 9 processing time: 0.4781 seconds\n",
      "PyTorch inference took 0.36519289016723633 sec\n",
      "PyTorch postprocessing took 0.0024209022521972656 sec\n",
      "Frame 10 processing time: 0.4045 seconds\n",
      "PyTorch inference took 0.3155698776245117 sec\n",
      "PyTorch postprocessing took 0.0016078948974609375 sec\n",
      "Frame 11 processing time: 0.3552 seconds\n",
      "PyTorch inference took 0.2633399963378906 sec\n",
      "PyTorch postprocessing took 0.0024459362030029297 sec\n",
      "Frame 12 processing time: 0.3029 seconds\n",
      "PyTorch inference took 0.2680678367614746 sec\n",
      "PyTorch postprocessing took 0.001959085464477539 sec\n",
      "Frame 13 processing time: 0.3059 seconds\n",
      "PyTorch inference took 0.276043176651001 sec\n",
      "PyTorch postprocessing took 0.0015869140625 sec\n",
      "Frame 14 processing time: 0.3151 seconds\n",
      "PyTorch inference took 0.268435001373291 sec\n",
      "PyTorch postprocessing took 0.0015223026275634766 sec\n",
      "Frame 15 processing time: 0.3060 seconds\n",
      "PyTorch inference took 0.2662231922149658 sec\n",
      "PyTorch postprocessing took 0.0014290809631347656 sec\n",
      "Frame 16 processing time: 0.3043 seconds\n",
      "PyTorch inference took 0.2732250690460205 sec\n",
      "PyTorch postprocessing took 0.002020120620727539 sec\n",
      "Frame 17 processing time: 0.3125 seconds\n",
      "PyTorch inference took 0.27051591873168945 sec\n",
      "PyTorch postprocessing took 0.0015401840209960938 sec\n",
      "Frame 18 processing time: 0.3098 seconds\n",
      "PyTorch inference took 0.2759218215942383 sec\n",
      "PyTorch postprocessing took 0.0014317035675048828 sec\n",
      "Frame 19 processing time: 0.3160 seconds\n",
      "PyTorch inference took 0.28646111488342285 sec\n",
      "PyTorch postprocessing took 0.0020987987518310547 sec\n",
      "Frame 20 processing time: 0.3247 seconds\n",
      "PyTorch inference took 0.312938928604126 sec\n",
      "PyTorch postprocessing took 0.0016131401062011719 sec\n",
      "Frame 21 processing time: 0.3526 seconds\n",
      "PyTorch inference took 0.28145527839660645 sec\n",
      "PyTorch postprocessing took 0.0016300678253173828 sec\n",
      "Frame 22 processing time: 0.3197 seconds\n",
      "PyTorch inference took 0.30995678901672363 sec\n",
      "PyTorch postprocessing took 0.0018982887268066406 sec\n",
      "Frame 23 processing time: 0.3502 seconds\n",
      "PyTorch inference took 0.25719285011291504 sec\n",
      "PyTorch postprocessing took 0.0018162727355957031 sec\n",
      "Frame 24 processing time: 0.2966 seconds\n",
      "PyTorch inference took 0.28175902366638184 sec\n",
      "PyTorch postprocessing took 0.0017957687377929688 sec\n",
      "Frame 25 processing time: 0.3201 seconds\n",
      "PyTorch inference took 0.291949987411499 sec\n",
      "PyTorch postprocessing took 0.002192974090576172 sec\n",
      "Frame 26 processing time: 0.3301 seconds\n",
      "PyTorch inference took 0.4162790775299072 sec\n",
      "PyTorch postprocessing took 0.0018000602722167969 sec\n",
      "Frame 27 processing time: 0.4567 seconds\n",
      "PyTorch inference took 0.25855302810668945 sec\n",
      "PyTorch postprocessing took 0.0023398399353027344 sec\n",
      "Frame 28 processing time: 0.2984 seconds\n",
      "PyTorch inference took 0.24894189834594727 sec\n",
      "PyTorch postprocessing took 0.0015459060668945312 sec\n",
      "Frame 29 processing time: 0.2881 seconds\n",
      "PyTorch inference took 0.2611961364746094 sec\n",
      "PyTorch postprocessing took 0.0022068023681640625 sec\n",
      "Frame 30 processing time: 0.3000 seconds\n",
      "PyTorch inference took 0.2688441276550293 sec\n",
      "PyTorch postprocessing took 0.0018930435180664062 sec\n",
      "Frame 31 processing time: 0.3062 seconds\n",
      "PyTorch inference took 0.266589879989624 sec\n",
      "PyTorch postprocessing took 0.001699209213256836 sec\n",
      "Frame 32 processing time: 0.3053 seconds\n",
      "PyTorch inference took 0.2687671184539795 sec\n",
      "PyTorch postprocessing took 0.0015399456024169922 sec\n",
      "Frame 33 processing time: 0.3063 seconds\n",
      "PyTorch inference took 0.274310827255249 sec\n",
      "PyTorch postprocessing took 0.001318216323852539 sec\n",
      "Frame 34 processing time: 0.3112 seconds\n",
      "PyTorch inference took 0.2651069164276123 sec\n",
      "PyTorch postprocessing took 0.0013880729675292969 sec\n",
      "Frame 35 processing time: 0.3026 seconds\n",
      "PyTorch inference took 0.2673318386077881 sec\n",
      "PyTorch postprocessing took 0.0019409656524658203 sec\n",
      "Frame 36 processing time: 0.3053 seconds\n",
      "PyTorch inference took 0.27668213844299316 sec\n",
      "PyTorch postprocessing took 0.0012850761413574219 sec\n",
      "Frame 37 processing time: 0.3143 seconds\n",
      "PyTorch inference took 0.2723112106323242 sec\n",
      "PyTorch postprocessing took 0.0018448829650878906 sec\n",
      "Frame 38 processing time: 0.3115 seconds\n",
      "PyTorch inference took 0.25311779975891113 sec\n",
      "PyTorch postprocessing took 0.0011382102966308594 sec\n",
      "Frame 39 processing time: 0.2930 seconds\n",
      "PyTorch inference took 0.25148606300354004 sec\n",
      "PyTorch postprocessing took 0.0021839141845703125 sec\n",
      "Frame 40 processing time: 0.2915 seconds\n",
      "PyTorch inference took 0.2693972587585449 sec\n",
      "PyTorch postprocessing took 0.0023369789123535156 sec\n",
      "Frame 41 processing time: 0.3089 seconds\n",
      "PyTorch inference took 0.266693115234375 sec\n",
      "PyTorch postprocessing took 0.002228975296020508 sec\n",
      "Frame 42 processing time: 0.3039 seconds\n",
      "PyTorch inference took 0.2651679515838623 sec\n",
      "PyTorch postprocessing took 0.0018508434295654297 sec\n",
      "Frame 43 processing time: 0.3041 seconds\n",
      "PyTorch inference took 0.2586228847503662 sec\n",
      "PyTorch postprocessing took 0.0015330314636230469 sec\n",
      "Frame 44 processing time: 0.2970 seconds\n",
      "PyTorch inference took 0.31780505180358887 sec\n",
      "PyTorch postprocessing took 0.002028942108154297 sec\n",
      "Frame 45 processing time: 0.3555 seconds\n",
      "PyTorch inference took 0.27985119819641113 sec\n",
      "PyTorch postprocessing took 0.0023279190063476562 sec\n",
      "Frame 46 processing time: 0.3189 seconds\n",
      "PyTorch inference took 0.26158785820007324 sec\n",
      "PyTorch postprocessing took 0.0027768611907958984 sec\n",
      "Frame 47 processing time: 0.3012 seconds\n",
      "PyTorch inference took 0.26531195640563965 sec\n",
      "PyTorch postprocessing took 0.0022211074829101562 sec\n",
      "Frame 48 processing time: 0.3038 seconds\n",
      "PyTorch inference took 0.26671385765075684 sec\n",
      "PyTorch postprocessing took 0.0024721622467041016 sec\n",
      "Frame 49 processing time: 0.3068 seconds\n",
      "PyTorch inference took 0.2580747604370117 sec\n",
      "PyTorch postprocessing took 0.002452850341796875 sec\n",
      "Frame 50 processing time: 0.2974 seconds\n",
      "PyTorch inference took 0.2905290126800537 sec\n",
      "PyTorch postprocessing took 0.002093076705932617 sec\n",
      "Frame 51 processing time: 0.3288 seconds\n",
      "PyTorch inference took 0.36228513717651367 sec\n",
      "PyTorch postprocessing took 0.003609895706176758 sec\n",
      "Frame 52 processing time: 0.4051 seconds\n",
      "PyTorch inference took 0.30153393745422363 sec\n",
      "PyTorch postprocessing took 0.0015349388122558594 sec\n",
      "Frame 53 processing time: 0.3398 seconds\n",
      "PyTorch inference took 0.2994680404663086 sec\n",
      "PyTorch postprocessing took 0.0022399425506591797 sec\n",
      "Frame 54 processing time: 0.3414 seconds\n",
      "PyTorch inference took 0.26655101776123047 sec\n",
      "PyTorch postprocessing took 0.001962900161743164 sec\n",
      "Frame 55 processing time: 0.3046 seconds\n",
      "PyTorch inference took 0.3509078025817871 sec\n",
      "PyTorch postprocessing took 0.0025212764739990234 sec\n",
      "Frame 56 processing time: 0.3931 seconds\n",
      "PyTorch inference took 0.30042195320129395 sec\n",
      "PyTorch postprocessing took 0.0021698474884033203 sec\n",
      "Frame 57 processing time: 0.3410 seconds\n",
      "PyTorch inference took 0.2992839813232422 sec\n",
      "PyTorch postprocessing took 0.0026807785034179688 sec\n",
      "Frame 58 processing time: 0.3392 seconds\n",
      "PyTorch inference took 0.2775909900665283 sec\n",
      "PyTorch postprocessing took 0.0015599727630615234 sec\n",
      "Frame 59 processing time: 0.3151 seconds\n",
      "PyTorch inference took 0.2630329132080078 sec\n",
      "PyTorch postprocessing took 0.002557039260864258 sec\n",
      "Frame 60 processing time: 0.3028 seconds\n",
      "PyTorch inference took 0.25136280059814453 sec\n",
      "PyTorch postprocessing took 0.0020678043365478516 sec\n",
      "Frame 61 processing time: 0.2903 seconds\n",
      "PyTorch inference took 0.27570390701293945 sec\n",
      "PyTorch postprocessing took 0.0022449493408203125 sec\n",
      "Frame 62 processing time: 0.3142 seconds\n",
      "PyTorch inference took 0.26409316062927246 sec\n",
      "PyTorch postprocessing took 0.0023622512817382812 sec\n",
      "Frame 63 processing time: 0.3033 seconds\n",
      "PyTorch inference took 0.25783705711364746 sec\n",
      "PyTorch postprocessing took 0.0016491413116455078 sec\n",
      "Frame 64 processing time: 0.2976 seconds\n",
      "PyTorch inference took 0.24167513847351074 sec\n",
      "PyTorch postprocessing took 0.0018169879913330078 sec\n",
      "Frame 65 processing time: 0.2805 seconds\n",
      "PyTorch inference took 0.25586605072021484 sec\n",
      "PyTorch postprocessing took 0.002519845962524414 sec\n",
      "Frame 66 processing time: 0.2959 seconds\n",
      "PyTorch inference took 0.2636079788208008 sec\n",
      "PyTorch postprocessing took 0.0013740062713623047 sec\n",
      "Frame 67 processing time: 0.3007 seconds\n",
      "PyTorch inference took 0.27376294136047363 sec\n",
      "PyTorch postprocessing took 0.0019960403442382812 sec\n",
      "Frame 68 processing time: 0.3109 seconds\n",
      "PyTorch inference took 0.27181315422058105 sec\n",
      "PyTorch postprocessing took 0.001931905746459961 sec\n",
      "Frame 69 processing time: 0.3097 seconds\n",
      "PyTorch inference took 0.26881980895996094 sec\n",
      "PyTorch postprocessing took 0.002401113510131836 sec\n",
      "Frame 70 processing time: 0.3076 seconds\n",
      "PyTorch inference took 0.2688629627227783 sec\n",
      "PyTorch postprocessing took 0.0015718936920166016 sec\n",
      "Frame 71 processing time: 0.3063 seconds\n",
      "PyTorch inference took 0.2635781764984131 sec\n",
      "PyTorch postprocessing took 0.0019350051879882812 sec\n",
      "Frame 72 processing time: 0.3025 seconds\n",
      "PyTorch inference took 0.37778782844543457 sec\n",
      "PyTorch postprocessing took 0.009737014770507812 sec\n",
      "Frame 73 processing time: 0.4247 seconds\n",
      "PyTorch inference took 0.2749350070953369 sec\n",
      "PyTorch postprocessing took 0.002450227737426758 sec\n",
      "Frame 74 processing time: 0.3154 seconds\n",
      "PyTorch inference took 0.2702469825744629 sec\n",
      "PyTorch postprocessing took 0.0024051666259765625 sec\n",
      "Frame 75 processing time: 0.3100 seconds\n",
      "PyTorch inference took 0.2535860538482666 sec\n",
      "PyTorch postprocessing took 0.00197601318359375 sec\n",
      "Frame 76 processing time: 0.2932 seconds\n",
      "PyTorch inference took 0.3013920783996582 sec\n",
      "PyTorch postprocessing took 0.0016360282897949219 sec\n",
      "Frame 77 processing time: 0.3528 seconds\n",
      "PyTorch inference took 0.37648487091064453 sec\n",
      "PyTorch postprocessing took 0.00185394287109375 sec\n",
      "Frame 78 processing time: 0.4310 seconds\n",
      "PyTorch inference took 0.33980321884155273 sec\n",
      "PyTorch postprocessing took 0.0016040802001953125 sec\n",
      "Frame 79 processing time: 0.3828 seconds\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# test download of benchmarking dataset\n",
    "# OBS link it not working, waiting for updated link to benchmarking dataset\n",
    "\n",
    "dlc_live = DLCLive(\n",
    "    path=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\",\n",
    "    device=\"cpu\",\n",
    "    # snapshot=\"snapshot-263.pt\",\n",
    "    model_type=\"onnx\",\n",
    "    display=True,\n",
    "    precision=\"FP16\",\n",
    ")\n",
    "# short video\n",
    "video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first.avi'\n",
    "#video_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/1_20cms_0degUP_first.avi\"\n",
    "\n",
    "poses, times = analyze_video(\n",
    "    video_path=video_path,\n",
    "    model_type=\"pytorch\",\n",
    "    snapshot = \"snapshot-263.pt\",\n",
    "    device=\"cpu\",\n",
    "    #precision=\"FP16\",\n",
    "    model_path=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\",\n",
    "    display=True,\n",
    "    save_poses=False,\n",
    "    save_dir=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/out\",\n",
    "    draw_keypoint_names=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [p[\"pose\"][1] for p in poses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Mean inference time excluding 1st inference \",\n",
    "    np.round(np.mean(times[1:]) * 1000, 2),\n",
    "    \"ms ±\",\n",
    "    np.round(np.std(times[1:]) * 1000, 2),\n",
    ")\n",
    "print(\n",
    "    \"Mean inference time including 1st inference \",\n",
    "    np.round(np.mean(times) * 1000, 2),\n",
    "    \"ms ±\",\n",
    "    np.round(np.std(times) * 1000, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(times[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_fps_stats(inference_times):\n",
    "    \"\"\"\n",
    "    Calculates the average FPS rate and its standard deviation from a list of inference times.\n",
    "\n",
    "    Args:\n",
    "        inference_times (list): A list of inference times in seconds.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the average FPS rate and its standard deviation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate FPS for each frame\n",
    "    fps_values = [1 / time for time in inference_times]\n",
    "\n",
    "    # Calculate average FPS\n",
    "    average_fps = np.mean(fps_values)\n",
    "\n",
    "    # Calculate standard deviation of FPS\n",
    "    std_dev_fps = np.std(fps_values)\n",
    "\n",
    "    return average_fps, std_dev_fps\n",
    "\n",
    "\n",
    "average_fps, std_dev_fps = calculate_fps_stats(times)\n",
    "\n",
    "print(\"Average FPS:\", average_fps)\n",
    "print(\"Standard Deviation of FPS:\", std_dev_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_model_size(model_path):\n",
    "    \"\"\"\n",
    "    Calculates the size of an ONNX model in bytes.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): The path to the ONNX model file.\n",
    "\n",
    "    Returns:\n",
    "        int: The size of the model in bytes.\n",
    "    \"\"\"\n",
    "    if \".onnx\" in model_path:\n",
    "        model = onnx.load(model_path)\n",
    "        size_bytes = len(model.SerializeToString())\n",
    "    elif \".pt\" in model_path:\n",
    "        model = torch.load(model_path)\n",
    "        print(model[\"model\"].keys())\n",
    "        params = list(model.parameters())\n",
    "        size_bytes = sum([p.numel() * p.element_size() for p in params])\n",
    "\n",
    "    # Convert to KB, MB, GB, etc.\n",
    "    if size_bytes < 1024:\n",
    "        size_str = f\"{size_bytes} B\"\n",
    "    elif size_bytes < 1024 * 1024:\n",
    "        size_str = f\"{size_bytes / 1024:.2f} KB\"\n",
    "    elif size_bytes < 1024 * 1024 * 1024:\n",
    "        size_str = f\"{size_bytes / (1024 * 1024):.2f} MB\"\n",
    "    else:\n",
    "        size_str = f\"{size_bytes / (1024 * 1024 * 1024):.2f} GB\"\n",
    "\n",
    "    return size_str\n",
    "\n",
    "\n",
    "get_model_size(\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/resnet_fp16.onnx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def get_video_characteristics(video_path):\n",
    "    \"\"\"\n",
    "    Extracts the FPS, number of frames, length in seconds, and frame size of a video.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): The path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the FPS, number of frames, length in seconds, and frame size.\n",
    "    \"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Calculate video length in seconds\n",
    "    video_length = frame_count / fps\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return fps, frame_count, video_length, (frame_width, frame_height)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "video_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/1_20cms_0degUP_first.avi\"\n",
    "fps, frame_count, video_length, frame_size = get_video_characteristics(video_path)\n",
    "\n",
    "print(\"FPS:\", fps)\n",
    "print(\"Number of frames:\", frame_count)\n",
    "print(\"Video length (seconds):\", video_length)\n",
    "print(\"Frame size:\", frame_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc-live",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
