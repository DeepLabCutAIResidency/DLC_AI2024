{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLC Live PyTorch Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlclive import DLCLive\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from onnxruntime import quantization\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = [\"fly-kevin\", \"hand-track\", \"superbird\", \"ventral-gait\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you do not have a .onnx model exported, use this cell to export your DLC3.0 snapshot\n",
    "\n",
    "from deeplabcut.pose_estimation_pytorch.config import read_config_as_dict\n",
    "from deeplabcut.pose_estimation_pytorch.models import PoseModel\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Dikra\n",
    "root = Path(\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3])\n",
    "model_cfg = read_config_as_dict(root / \"pytorch_config.yaml\")\n",
    "weights_path = root / \"snapshot-263.pt\"\n",
    "\n",
    "# Anna\n",
    "# root = Path(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\")\n",
    "# model_cfg = read_config_as_dict(root / \"pytorch_config.yaml\")\n",
    "# weights_path = root / \"snapshot-263.pt\"\n",
    "\n",
    "model = PoseModel.build(model_cfg[\"model\"])\n",
    "weights = torch.load(weights_path, map_location=device)\n",
    "model.load_state_dict(weights[\"model\"])\n",
    "\n",
    "dummy_input = torch.zeros((1, 3, 224, 224))\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3] + \"/resnet.onnx\",\n",
    "    verbose=False,\n",
    "    input_names=[\"input\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quant ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32 to FP16\n",
    "from onnxconverter_common import float16\n",
    "\n",
    "onnx_fp32_model_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3] + \"/resnet.onnx\"\n",
    ")\n",
    "onnx_fp16_model_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\"\n",
    "    + projects[3]\n",
    "    + \"/resnet_fp16.onnx\"\n",
    ")\n",
    "\n",
    "model_fp32 = onnx.load(onnx_fp32_model_path)\n",
    "model_fp16 = float16.convert_float_to_float16(model_fp32)\n",
    "onnx.save(model_fp16, onnx_fp16_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_fp32_model_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3] + \"/resnet.onnx\"\n",
    ")\n",
    "model_prep_path = (\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\"\n",
    "    + projects[3]\n",
    "    + \"/resnet_quant_prep.onnx\"\n",
    ")\n",
    "\n",
    "# prep for quantisation\n",
    "quantization.shape_inference.quant_pre_process(\n",
    "    onnx_fp32_model_path, model_prep_path, skip_symbolic_shape=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test frame\n",
    "img = cv2.imread(\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3] + \"/img0006.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with ONNX exported DLC 3.0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dikra\n",
    "onnx_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3],\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cuda\",\n",
    "    display=True,\n",
    "    precision=\"FP16\",\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# onnx_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# onnx_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "onnx_pose = onnx_dlc_live.init_inference(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot from 2024-08-20 14-29-53.png](./docs/assets/Screenshot%20from%202024-08-20%2014-36-00.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected = onnx_pose[0][\"poses\"][0][0][:, 2] > 0.9\n",
    "print(torch.any(detected))\n",
    "x = onnx_pose[0][\"poses\"][0][0][detected, 0]\n",
    "y = onnx_pose[0][\"poses\"][0][0][detected, 1]\n",
    "onnx_pose[0][\"poses\"][:, :, :, 1][:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_pose = onnx_dlc_live.get_pose(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with snaptshot of DLC 3.0 model (.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dikra\n",
    "pytorch_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait\",\n",
    "    snapshot=\"snapshot-263.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# pytorch_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# pytorch_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "pytorch_pose = pytorch_dlc_live.init_inference(frame=img)\n",
    "pytorch_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PyTorch model inference](./docs/assets/Screenshot%20from%202024-08-20%2014-29-53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "root = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait\"\n",
    "test_images = glob.glob(os.path.normpath(root + \"/*.png\"))\n",
    "\n",
    "\n",
    "def mean_time_inference(dlc_live, images):\n",
    "    times = []\n",
    "    for i, img_p in enumerate(images):\n",
    "        img = cv2.imread(img_p)\n",
    "\n",
    "        if i == 0:\n",
    "            start = time.time()\n",
    "            dlc_live.init_inference(img)\n",
    "            end = time.time()\n",
    "        else:\n",
    "            start = time.time()\n",
    "            dlc_live.get_pose(img)\n",
    "            end = time.time()\n",
    "        times.append(end - start)\n",
    "    print(times)\n",
    "\n",
    "    return np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"onnx\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images)\n",
    "print(\n",
    "    f\"TOTAL Inference of ONNX model took on average {mean_time} seconds for {len(test_images)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    snapshot=\"snapshot-200.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\",\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images)\n",
    "print(\n",
    "    f\"Inference of PyTorch model took on average {mean_time} seconds for {len(test_images)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/\" + projects[3],\n",
    "    device=\"tensorrt\",\n",
    "    model_type=\"onnx\",\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images)\n",
    "print(\n",
    "    f\"Inference of PyTorch model took on average {mean_time} seconds for {len(test_images)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dlc_live.get_pose(img)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "Currently the benchmark_pytorch.py script serves to provide a function for analyzing a preexisting video to test PyTorch for running video inference in DLC-Live. Code for running video inference on a live video feed is WIP.\n",
    "\n",
    "For true benchmarking purposes, we aim to add feature for recording the time it takes to analyze each frame / how many frames can be analyzed per second. Discuss what measure to use and consult the DLC Live paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import the analyze_video function from the file where it's defined\n",
    "from dlclive.benchmark_pytorch import analyze_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New version with DLCLive object included in the code\n",
    "\n",
    "\n",
    "# Define the paths\n",
    "video_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/1_20cms_0degUP_first_03s.avi\"\n",
    "model_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait\"\n",
    "\n",
    "# import cProfile\n",
    "# import io\n",
    "# import pstats\n",
    "\n",
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "\n",
    "# Call the analyze_video function with the appropriate arguments\n",
    "poses = analyze_video(\n",
    "    video_path=video_path,\n",
    "    model_path=model_path,\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cuda\",\n",
    "    display=True,\n",
    "    save_poses=True,\n",
    "    resize=0.5,\n",
    "    precision = \"FP16\",\n",
    "    # cropping= [50, 250, 100, 450], # manually set the cropping to specific pixels\n",
    "    dynamic=(\n",
    "        True,\n",
    "        0.5,\n",
    "        10,\n",
    "    ),  # True = we want to apply dynamic cropping, 0.5 = the threshold for accepting a KP as detected, 10 = the margin to expand the calculatted cropping window by so it is not too narrow\n",
    "    save_dir=\"output_directory\",\n",
    "    get_sys_info=True,\n",
    "    draw_keypoint_names=True,\n",
    ")\n",
    "\n",
    "# #'poses' will contain the list of poses detected\n",
    "\n",
    "# # Create a stream to capture the profiler's output\n",
    "# s = io.StringIO()\n",
    "# sortby = 'cumulative'\n",
    "# ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "# ps.print_stats()\n",
    "\n",
    "# # Print the profiling output\n",
    "# print(s.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand model and video\n",
    "\n",
    "# Define the paths\n",
    "video_path = \"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Hand-AnnaStuckert-2024-08-21/videos/Hand.avi\"\n",
    "model_path = \"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Hand-AnnaStuckert-2024-08-21/dlc-models-pytorch/iteration-0/HandAug21-trainset95shuffle101/train\"\n",
    "\n",
    "\n",
    "# Call the analyze_video function with the appropriate arguments\n",
    "poses = analyze_video(\n",
    "    video_path=video_path,\n",
    "    model_path=model_path,\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cuda\",\n",
    "    display=True,\n",
    "    save_poses=True,\n",
    "    resize=0.4,\n",
    "    # cropping= [50, 250, 100, 450], # manually set the cropping to specific pixels\n",
    "    dynamic=(\n",
    "        True,\n",
    "        0.5,\n",
    "        10,\n",
    "    ),  # True = we want to apply dynamic cropping, 0.5 = the threshold for accepting a KP as detected, 10 = the margin to expand the calculatted cropping window by so it is not too narrow\n",
    "    save_dir=\"output_directory\",\n",
    "    get_sys_info=True,\n",
    "    draw_keypoint_names=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlclive import DLCLive\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from onnxruntime import quantization\n",
    "import onnx\n",
    "from dlclive.benchmark_pytorch import analyze_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test download of benchmarking dataset\n",
    "# OBS link it not working, waiting for updated link to benchmarking dataset\n",
    "\n",
    "dlc_live = DLCLive(\n",
    "    path=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\",\n",
    "    device=\"cpu\",\n",
    "    # snapshot=\"snapshot-263.pt\",\n",
    "    model_type=\"onnx\",\n",
    "    display=True,\n",
    "    precision=\"FP16\",\n",
    ")\n",
    "# short video\n",
    "video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first.avi'\n",
    "#video_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/1_20cms_0degUP_first.avi\"\n",
    "\n",
    "poses, times = analyze_video(\n",
    "    video_path=video_path,\n",
    "    model_type=\"pytorch\",\n",
    "    snapshot = \"snapshot-263.pt\",\n",
    "    device=\"cpu\",\n",
    "    #precision=\"FP16\",\n",
    "    model_path=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\",\n",
    "    display=True,\n",
    "    save_poses=False,\n",
    "    save_dir=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/out\",\n",
    "    draw_keypoint_names=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [p[\"pose\"][1] for p in poses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Mean inference time excluding 1st inference \",\n",
    "    np.round(np.mean(times[1:]) * 1000, 2),\n",
    "    \"ms ±\",\n",
    "    np.round(np.std(times[1:]) * 1000, 2),\n",
    ")\n",
    "print(\n",
    "    \"Mean inference time including 1st inference \",\n",
    "    np.round(np.mean(times) * 1000, 2),\n",
    "    \"ms ±\",\n",
    "    np.round(np.std(times) * 1000, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(times[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_fps_stats(inference_times):\n",
    "    \"\"\"\n",
    "    Calculates the average FPS rate and its standard deviation from a list of inference times.\n",
    "\n",
    "    Args:\n",
    "        inference_times (list): A list of inference times in seconds.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the average FPS rate and its standard deviation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate FPS for each frame\n",
    "    fps_values = [1 / time for time in inference_times]\n",
    "\n",
    "    # Calculate average FPS\n",
    "    average_fps = np.mean(fps_values)\n",
    "\n",
    "    # Calculate standard deviation of FPS\n",
    "    std_dev_fps = np.std(fps_values)\n",
    "\n",
    "    return average_fps, std_dev_fps\n",
    "\n",
    "\n",
    "average_fps, std_dev_fps = calculate_fps_stats(times)\n",
    "\n",
    "print(\"Average FPS:\", average_fps)\n",
    "print(\"Standard Deviation of FPS:\", std_dev_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_model_size(model_path):\n",
    "    \"\"\"\n",
    "    Calculates the size of an ONNX model in bytes.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): The path to the ONNX model file.\n",
    "\n",
    "    Returns:\n",
    "        int: The size of the model in bytes.\n",
    "    \"\"\"\n",
    "    if \".onnx\" in model_path:\n",
    "        model = onnx.load(model_path)\n",
    "        size_bytes = len(model.SerializeToString())\n",
    "    elif \".pt\" in model_path:\n",
    "        model = torch.load(model_path)\n",
    "        print(model[\"model\"].keys())\n",
    "        params = list(model.parameters())\n",
    "        size_bytes = sum([p.numel() * p.element_size() for p in params])\n",
    "\n",
    "    # Convert to KB, MB, GB, etc.\n",
    "    if size_bytes < 1024:\n",
    "        size_str = f\"{size_bytes} B\"\n",
    "    elif size_bytes < 1024 * 1024:\n",
    "        size_str = f\"{size_bytes / 1024:.2f} KB\"\n",
    "    elif size_bytes < 1024 * 1024 * 1024:\n",
    "        size_str = f\"{size_bytes / (1024 * 1024):.2f} MB\"\n",
    "    else:\n",
    "        size_str = f\"{size_bytes / (1024 * 1024 * 1024):.2f} GB\"\n",
    "\n",
    "    return size_str\n",
    "\n",
    "\n",
    "get_model_size(\n",
    "    \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/resnet_fp16.onnx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def get_video_characteristics(video_path):\n",
    "    \"\"\"\n",
    "    Extracts the FPS, number of frames, length in seconds, and frame size of a video.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): The path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the FPS, number of frames, length in seconds, and frame size.\n",
    "    \"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Calculate video length in seconds\n",
    "    video_length = frame_count / fps\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return fps, frame_count, video_length, (frame_width, frame_height)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "video_path = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/ventral-gait/1_20cms_0degUP_first.avi\"\n",
    "fps, frame_count, video_length, frame_size = get_video_characteristics(video_path)\n",
    "\n",
    "print(\"FPS:\", fps)\n",
    "print(\"Number of frames:\", frame_count)\n",
    "print(\"Video length (seconds):\", video_length)\n",
    "print(\"Frame size:\", frame_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live video analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annastuckert/anaconda3/envs/deeplabcut3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/annastuckert/anaconda3/envs/deeplabcut3/lib/python3.10/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2024-08-27 14:22:30.429 python[44349:600026] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model took 6.620182991027832 sec\n",
      "PyTorch inference took 0.30005407333374023 sec\n",
      "PyTorch postprocessing took 0.012089967727661133 sec\n",
      "Frame 0 processing time: 7.5922 seconds\n",
      "PyTorch inference took 0.29785704612731934 sec\n",
      "PyTorch postprocessing took 0.0017039775848388672 sec\n",
      "Frame 1 processing time: 0.3845 seconds\n",
      "PyTorch inference took 0.40530920028686523 sec\n",
      "PyTorch postprocessing took 0.0023298263549804688 sec\n",
      "Frame 2 processing time: 0.4795 seconds\n",
      "PyTorch inference took 0.5319540500640869 sec\n",
      "PyTorch postprocessing took 0.0025599002838134766 sec\n",
      "Frame 3 processing time: 0.5984 seconds\n",
      "PyTorch inference took 0.49686288833618164 sec\n",
      "PyTorch postprocessing took 0.0032427310943603516 sec\n",
      "Frame 4 processing time: 0.7457 seconds\n",
      "PyTorch inference took 0.37366414070129395 sec\n",
      "PyTorch postprocessing took 0.001766204833984375 sec\n",
      "Frame 5 processing time: 0.4404 seconds\n",
      "PyTorch inference took 0.3685030937194824 sec\n",
      "PyTorch postprocessing took 0.0015978813171386719 sec\n",
      "Frame 6 processing time: 0.4211 seconds\n",
      "PyTorch inference took 0.33455324172973633 sec\n",
      "PyTorch postprocessing took 0.0022788047790527344 sec\n",
      "Frame 7 processing time: 0.3935 seconds\n",
      "PyTorch inference took 0.318587064743042 sec\n",
      "PyTorch postprocessing took 0.0038750171661376953 sec\n",
      "Frame 8 processing time: 0.3813 seconds\n",
      "PyTorch inference took 0.44629979133605957 sec\n",
      "PyTorch postprocessing took 0.0028641223907470703 sec\n",
      "Frame 9 processing time: 0.5246 seconds\n",
      "PyTorch inference took 0.328228235244751 sec\n",
      "PyTorch postprocessing took 0.0019259452819824219 sec\n",
      "Frame 10 processing time: 0.3807 seconds\n",
      "PyTorch inference took 0.6081271171569824 sec\n",
      "PyTorch postprocessing took 0.0022101402282714844 sec\n",
      "Frame 11 processing time: 0.6890 seconds\n",
      "PyTorch inference took 0.4075760841369629 sec\n",
      "PyTorch postprocessing took 0.002907276153564453 sec\n",
      "Frame 12 processing time: 0.4801 seconds\n",
      "PyTorch inference took 0.3871726989746094 sec\n",
      "PyTorch postprocessing took 0.001985788345336914 sec\n",
      "Frame 13 processing time: 0.4394 seconds\n",
      "PyTorch inference took 0.44070887565612793 sec\n",
      "PyTorch postprocessing took 0.0040950775146484375 sec\n",
      "Frame 14 processing time: 0.5151 seconds\n",
      "PyTorch inference took 0.36582398414611816 sec\n",
      "PyTorch postprocessing took 0.004461050033569336 sec\n",
      "Frame 15 processing time: 0.4239 seconds\n",
      "PyTorch inference took 0.4041450023651123 sec\n",
      "PyTorch postprocessing took 0.0026438236236572266 sec\n",
      "Frame 16 processing time: 0.4688 seconds\n",
      "PyTorch inference took 0.41876792907714844 sec\n",
      "PyTorch postprocessing took 0.001728057861328125 sec\n",
      "Frame 17 processing time: 0.4848 seconds\n",
      "PyTorch inference took 0.3799576759338379 sec\n",
      "PyTorch postprocessing took 0.0023391246795654297 sec\n",
      "Frame 18 processing time: 0.4580 seconds\n",
      "PyTorch inference took 0.38121533393859863 sec\n",
      "PyTorch postprocessing took 0.004023075103759766 sec\n",
      "Frame 19 processing time: 0.4339 seconds\n",
      "PyTorch inference took 0.34909629821777344 sec\n",
      "PyTorch postprocessing took 0.0017709732055664062 sec\n",
      "Frame 20 processing time: 0.3998 seconds\n",
      "PyTorch inference took 0.49908995628356934 sec\n",
      "PyTorch postprocessing took 0.0021219253540039062 sec\n",
      "Frame 21 processing time: 0.5467 seconds\n",
      "PyTorch inference took 0.3561272621154785 sec\n",
      "PyTorch postprocessing took 0.0019741058349609375 sec\n",
      "Frame 22 processing time: 0.4214 seconds\n",
      "PyTorch inference took 0.4927942752838135 sec\n",
      "PyTorch postprocessing took 0.0024080276489257812 sec\n",
      "Frame 23 processing time: 0.5864 seconds\n",
      "PyTorch inference took 0.4532139301300049 sec\n",
      "PyTorch postprocessing took 0.002412080764770508 sec\n",
      "Frame 24 processing time: 0.5055 seconds\n",
      "PyTorch inference took 0.3483748435974121 sec\n",
      "PyTorch postprocessing took 0.002228975296020508 sec\n",
      "Frame 25 processing time: 0.4278 seconds\n",
      "PyTorch inference took 0.35515594482421875 sec\n",
      "PyTorch postprocessing took 0.0031218528747558594 sec\n",
      "Frame 26 processing time: 0.4308 seconds\n",
      "PyTorch inference took 0.35129213333129883 sec\n",
      "PyTorch postprocessing took 0.0019550323486328125 sec\n",
      "Frame 27 processing time: 0.4208 seconds\n",
      "PyTorch inference took 0.31095337867736816 sec\n",
      "PyTorch postprocessing took 0.0014111995697021484 sec\n",
      "Frame 28 processing time: 0.3567 seconds\n",
      "{'host_name': 'MCKDQTN4YXDV', 'op_sys': 'macOS-14.5-arm64-arm-64bit', 'python': 'deeplabcut3', 'device_type': 'CPU', 'device': ['Apple M2'], 'freeze': ['absl-py==2.1.0', 'alabaster==0.7.16', 'albumentations==1.4.3', 'app-model==0.2.7', 'appdirs==1.4.4', 'appnope==0.1.4', 'asttokens==2.4.1', 'astunparse==1.6.3', 'attrs==23.2.0', 'Babel==2.15.0', 'blosc2==2.0.0', 'build==1.2.1', 'cachetools==5.4.0', 'cachey==0.2.1', 'certifi==2024.7.4', 'charset-normalizer==3.3.2', 'click==8.1.7', 'cloudpickle==3.0.0', 'colorcet==3.1.0', 'coloredlogs==15.0.1', 'comm==0.2.2', 'contourpy==1.2.1', 'cycler==0.12.1', 'Cython==3.0.10', 'dask==2024.7.0', 'dask-expr==1.1.7', 'dask-image==2024.5.3', 'debugpy==1.8.2', 'decorator==5.1.1', 'deeplabcut @ git+https://github.com/DeepLabCut/DeepLabCut.git@83f1acb9c179caf46353cdb211318b29832ff6ec', 'deeplabcut-live==1.0.2', 'deeplabcut-live-gui @ file:///Users/annastuckert/Documents/DeepLabCut-live-GUI', 'dill==0.3.8', 'dlclibrary==0.0.6', 'docker-pycreds==0.4.0', 'docstring_parser==0.16', 'docutils==0.17.1', 'einops==0.8.0', 'exceptiongroup==1.2.2', 'executing==2.0.1', 'filelock==3.15.4', 'filterpy==1.4.5', 'flatbuffers==24.3.25', 'flexcache==0.3', 'flexparser==0.3.1', 'fonttools==4.53.1', 'freetype-py==2.4.0', 'fsspec==2024.6.1', 'gast==0.4.0', 'gitdb==4.0.11', 'GitPython==3.1.43', 'google-auth==2.32.0', 'google-auth-oauthlib==0.4.6', 'google-pasta==0.2.0', 'grpcio==1.65.4', 'h5py==3.11.0', 'HeapDict==1.0.1', 'hsluv==5.0.4', 'huggingface-hub==0.23.5', 'humanfriendly==10.0', 'idna==3.7', 'imageio==2.34.2', 'imageio-ffmpeg==0.5.1', 'imagesize==1.4.1', 'imgaug==0.4.0', 'importlib_metadata==8.0.0', 'imutils==0.5.4', 'in-n-out==0.2.1', 'ipykernel==6.29.5', 'ipython==8.26.0', 'jedi==0.19.1', 'Jinja2==3.1.4', 'joblib==1.4.2', 'jsonschema==4.23.0', 'jsonschema-specifications==2023.12.1', 'jupyter_client==8.6.2', 'jupyter_core==5.7.2', 'keras==2.11.0', 'kiwisolver==1.4.5', 'lazy_loader==0.4', 'libclang==18.1.1', 'llvmlite==0.43.0', 'locket==1.0.0', 'magicgui==0.8.3', 'Markdown==3.6', 'markdown-it-py==3.0.0', 'MarkupSafe==2.1.5', 'matplotlib==3.8.4', 'matplotlib-inline==0.1.7', 'mdurl==0.1.2', 'memory-profiler==0.61.0', 'ml-dtypes==0.4.0', 'mpmath==1.3.0', 'msgpack==1.0.8', 'msgpack-numpy==0.4.8', 'multiprocess==0.70.16', 'namex==0.0.8', 'napari==0.4.18', 'napari-console==0.0.9', 'napari-deeplabcut==0.2.1.7', 'napari-plugin-engine==0.2.0', 'napari-svg==0.2.0', 'natsort==8.4.0', 'nest-asyncio==1.6.0', 'networkx==3.3', 'npe2==0.7.6', 'numba==0.60.0', 'numexpr @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_45yefq0kt6/croot/numexpr_1696515289183/work', 'numpy==1.26.4', 'numpydoc==1.5.0', 'oauthlib==3.2.2', 'onnx==1.16.2', 'onnxruntime==1.18.1', 'onnxruntime-tools==1.7.0', 'onnxscript==0.1.0.dev20240813', 'opencv-python==4.10.0.84', 'opencv-python-headless==4.10.0.84', 'opt-einsum==3.3.0', 'optree==0.12.1', 'packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1718189413536/work', 'pandas==1.5.3', 'parso==0.8.4', 'partd==1.4.2', 'patsy==0.5.6', 'pexpect==4.9.0', 'pillow==10.4.0', 'PIMS==0.7', 'Pint==0.24.3', 'pip==24.0', 'platformdirs==4.2.2', 'pooch==1.8.2', 'prompt_toolkit==3.0.47', 'protobuf==5.27.3', 'psutil==6.0.0', 'psygnal==0.11.1', 'ptyprocess==0.7.0', 'pure-eval==0.2.2', 'py-cpuinfo @ file:///home/conda/feedstock_root/build_artifacts/py-cpuinfo_1666774466606/work', 'py3nvml==0.2.7', 'pyarrow==17.0.0', 'pyasn1==0.6.0', 'pyasn1_modules==0.4.0', 'pycocotools==2.0.8', 'pyconify==0.1.6', 'pydantic==1.10.17', 'pydantic-compat==0.1.2', 'Pygments==2.18.0', 'PyOpenGL==3.1.7', 'pyparsing==3.1.2', 'pyproject_hooks==1.1.0', 'pyserial==3.5', 'PySide6==6.4.2', 'PySide6-Addons==6.4.2', 'PySide6-Essentials==6.4.2', 'python-dateutil==2.9.0.post0', 'pytz==2024.1', 'PyYAML==6.0.1', 'pyzmq==26.0.3', 'QDarkStyle==3.1', 'qtconsole==5.5.2', 'QtPy==2.4.1', 'referencing==0.35.1', 'requests==2.32.3', 'requests-oauthlib==2.0.0', 'rich==13.7.1', 'rpds-py==0.19.0', 'rsa==4.9', 'ruamel.yaml==0.17.40', 'ruamel.yaml.clib==0.2.8', 'safetensors==0.4.3', 'scikit-image==0.24.0', 'scikit-learn==1.5.1', 'scipy==1.10.1', 'sentry-sdk==2.10.0', 'setproctitle==1.3.3', 'setuptools==69.5.1', 'shapely==2.0.5', 'shellingham==1.5.4', 'shiboken6==6.4.2', 'six==1.16.0', 'slicerator==1.1.0', 'smmap==5.0.1', 'snowballstemmer==2.2.0', 'Sphinx==4.5.0', 'sphinxcontrib-applehelp==1.0.8', 'sphinxcontrib-devhelp==1.0.6', 'sphinxcontrib-htmlhelp==2.0.5', 'sphinxcontrib-jsmath==1.0.1', 'sphinxcontrib-qthelp==1.0.7', 'sphinxcontrib-serializinghtml==1.1.10', 'stack-data==0.6.3', 'statsmodels==0.14.2', 'superqt==0.6.7', 'sympy==1.13.0', 'tables @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_81qnps6056/croot/pytables_1691621471831/work', 'tabulate==0.9.0', 'tensorboard==2.11.2', 'tensorboard-data-server==0.6.1', 'tensorboard-plugin-wit==1.8.1', 'tensorflow-estimator==2.11.0', 'tensorflow-io-gcs-filesystem==0.37.1', 'tensorflow-macos==2.11.0', 'tensorflow-metal==1.1.0', 'tensorpack==0.11', 'termcolor==2.4.0', 'tf-slim==1.1.0', 'threadpoolctl==3.5.0', 'tifffile==2024.7.2', 'timm==1.0.7', 'tomli==2.0.1', 'tomli_w==1.0.0', 'toolz==0.12.1', 'torch==2.3.1', 'torchvision==0.18.1', 'tornado==6.4.1', 'tqdm==4.66.4', 'traitlets==5.14.3', 'typer==0.12.3', 'typing_extensions==4.12.2', 'tzdata==2024.1', 'urllib3==2.2.2', 'vispy==0.12.2', 'wandb==0.17.4', 'wcwidth==0.2.13', 'Werkzeug==3.0.3', 'wheel==0.43.0', 'wrapt==1.16.0', 'xmltodict==0.13.0', 'zipp==3.19.2'], 'python_version': '3.10.14 (main, May  6 2024, 14:42:37) [Clang 14.0.6 ]', 'git_hash': '05190e296737720ff29ebc5262b3dc8373ab6c91', 'dlclive_version': '1.0.4'}\n",
      "tensor([[3.8278e+02, 4.5821e+02, 1.2926e-01],\n",
      "        [4.9473e+02, 6.9102e+02, 1.0929e-01],\n",
      "        [5.3763e+02, 6.7441e+02, 1.1130e-01],\n",
      "        [6.0541e+02, 6.9768e+02, 1.5426e-01],\n",
      "        [1.0867e+03, 6.9704e+02, 1.6548e-01],\n",
      "        [5.0541e+02, 7.0884e+02, 1.3589e-01],\n",
      "        [6.7952e+02, 6.6504e+02, 7.1488e-02],\n",
      "        [5.3585e+02, 6.9204e+02, 1.8463e-01],\n",
      "        [1.0487e+03, 7.0276e+02, 1.9232e-01],\n",
      "        [5.7104e+02, 7.0659e+02, 1.1026e-01],\n",
      "        [2.4804e+02, 5.7457e+02, 8.7458e-02],\n",
      "        [1.1477e+02, 7.2628e+02, 1.7975e-01]])\n",
      "tensor([[4.1563e+02, 4.5680e+02, 9.4179e-02],\n",
      "        [1.1508e+03, 6.0996e+02, 8.9632e-02],\n",
      "        [8.3667e+02, 6.1678e+02, 9.2152e-02],\n",
      "        [6.3278e+02, 6.9662e+02, 1.3038e-01],\n",
      "        [1.0871e+03, 6.9549e+02, 1.6493e-01],\n",
      "        [5.3546e+02, 7.0897e+02, 1.5401e-01],\n",
      "        [1.6774e+02, 6.3194e+02, 7.6994e-02],\n",
      "        [5.5276e+02, 6.6244e+02, 2.0127e-01],\n",
      "        [1.0852e+03, 6.9546e+02, 1.6287e-01],\n",
      "        [5.6987e+02, 7.2113e+02, 8.7622e-02],\n",
      "        [2.4909e+02, 6.0353e+02, 1.0522e-01],\n",
      "        [9.9126e+01, 7.4071e+02, 1.3034e-01]])\n",
      "tensor([[1.9569e+02, 1.9873e+02, 5.9193e-01],\n",
      "        [2.3323e+02, 4.7262e+02, 1.4628e-01],\n",
      "        [2.9079e+02, 1.5565e+02, 8.9015e-02],\n",
      "        [3.2823e+02, 3.7847e+02, 2.6437e-01],\n",
      "        [2.9216e+02, 1.4356e+02, 2.6325e-01],\n",
      "        [3.8583e+02, 3.6653e+02, 2.6468e-01],\n",
      "        [3.9771e+02, 1.9514e+02, 5.6606e-01],\n",
      "        [4.2761e+02, 3.6138e+02, 2.2215e-01],\n",
      "        [7.9446e+01, 3.5438e+02, 2.9282e-01],\n",
      "        [4.9466e+02, 5.8350e+02, 2.6952e-01],\n",
      "        [3.7746e+02, 6.6857e+02, 1.9783e-01],\n",
      "        [2.2434e+02, 5.6586e+02, 1.7626e-01]])\n",
      "tensor([[2.5415e+02, 1.9617e+02, 4.3716e-01],\n",
      "        [2.3179e+02, 4.9628e+02, 1.9462e-01],\n",
      "        [3.6513e+02, 1.6180e+02, 1.0403e-01],\n",
      "        [3.4275e+02, 4.2976e+02, 3.5407e-01],\n",
      "        [3.6753e+02, 1.5353e+02, 1.3697e-01],\n",
      "        [4.4567e+02, 4.1086e+02, 2.6353e-01],\n",
      "        [3.6499e+02, 1.7198e+02, 3.0139e-01],\n",
      "        [4.6486e+02, 4.2180e+02, 2.0830e-01],\n",
      "        [8.1337e+01, 3.6103e+02, 2.6957e-01],\n",
      "        [5.2790e+02, 6.3512e+02, 1.8125e-01],\n",
      "        [3.7690e+02, 6.7706e+02, 2.1939e-01],\n",
      "        [4.3752e+02, 7.2656e+02, 2.4685e-01]])\n",
      "tensor([[2.1158e+02, 2.4005e+02, 3.4567e-01],\n",
      "        [1.8783e+02, 5.4895e+02, 1.7528e-01],\n",
      "        [3.4502e+02, 1.9252e+02, 1.0872e-01],\n",
      "        [2.5744e+02, 4.7685e+02, 4.0866e-01],\n",
      "        [3.3056e+02, 1.8677e+02, 1.1316e-01],\n",
      "        [3.8505e+02, 4.4216e+02, 4.4725e-01],\n",
      "        [3.3505e+02, 1.9977e+02, 2.7314e-01],\n",
      "        [4.6366e+02, 4.5580e+02, 3.1547e-01],\n",
      "        [5.4763e+01, 4.0791e+02, 2.3784e-01],\n",
      "        [5.1617e+02, 6.6465e+02, 1.7391e-01],\n",
      "        [3.7407e+02, 7.0154e+02, 2.0462e-01],\n",
      "        [4.3506e+02, 7.2446e+02, 2.3231e-01]])\n",
      "tensor([[3.6177e+02, 2.1672e+02, 3.7903e-01],\n",
      "        [2.0106e+02, 5.5748e+02, 1.4745e-01],\n",
      "        [4.8154e+02, 2.8510e+02, 9.5278e-02],\n",
      "        [2.9742e+02, 4.9535e+02, 3.2282e-01],\n",
      "        [2.2767e+02, 2.5939e+02, 1.0893e-01],\n",
      "        [4.1797e+02, 4.6696e+02, 5.4640e-01],\n",
      "        [3.5417e+02, 2.1792e+02, 2.5888e-01],\n",
      "        [4.9713e+02, 4.8178e+02, 2.0604e-01],\n",
      "        [6.7103e+02, 6.5950e+02, 2.0880e-01],\n",
      "        [5.3630e+02, 6.5677e+02, 1.8688e-01],\n",
      "        [2.1627e+02, 5.6891e+02, 1.5566e-01],\n",
      "        [5.3602e+01, 6.9951e+02, 1.4678e-01]])\n",
      "tensor([[2.4875e+02, 2.7999e+02, 3.0945e-01],\n",
      "        [2.2531e+02, 5.7643e+02, 1.4107e-01],\n",
      "        [3.7512e+02, 2.3819e+02, 1.0794e-01],\n",
      "        [2.5181e+02, 5.0664e+02, 3.1584e-01],\n",
      "        [3.6790e+02, 2.2282e+02, 1.5029e-01],\n",
      "        [4.2167e+02, 4.7119e+02, 4.8089e-01],\n",
      "        [3.7767e+02, 2.4239e+02, 2.5335e-01],\n",
      "        [5.0190e+02, 4.8183e+02, 2.1527e-01],\n",
      "        [6.6984e+02, 6.6049e+02, 2.3118e-01],\n",
      "        [5.2347e+02, 6.8029e+02, 1.5464e-01],\n",
      "        [2.8239e+02, 6.7188e+02, 1.7686e-01],\n",
      "        [5.4751e+01, 7.0061e+02, 1.4272e-01]])\n",
      "tensor([[3.7503e+02, 2.5450e+02, 2.9755e-01],\n",
      "        [2.2559e+02, 5.8500e+02, 1.3257e-01],\n",
      "        [3.8385e+02, 2.6418e+02, 1.2294e-01],\n",
      "        [2.8099e+02, 5.2286e+02, 3.7542e-01],\n",
      "        [3.7678e+02, 2.4507e+02, 1.4914e-01],\n",
      "        [4.4926e+02, 4.8719e+02, 4.6267e-01],\n",
      "        [3.8185e+02, 2.5869e+02, 2.6147e-01],\n",
      "        [5.0255e+02, 4.8778e+02, 2.2535e-01],\n",
      "        [6.7007e+02, 6.6041e+02, 2.4494e-01],\n",
      "        [5.2086e+02, 6.8168e+02, 1.9725e-01],\n",
      "        [2.5081e+02, 6.6714e+02, 1.7659e-01],\n",
      "        [5.6291e+01, 6.9945e+02, 1.9215e-01]])\n",
      "tensor([[3.4878e+02, 4.5592e+02, 1.3682e-01],\n",
      "        [9.4221e+01, 6.6244e+02, 1.2922e-01],\n",
      "        [3.1672e+02, 5.5756e+02, 1.0464e-01],\n",
      "        [4.0386e+02, 4.4943e+02, 9.8808e-02],\n",
      "        [5.1365e+02, 4.8724e+02, 1.5851e-01],\n",
      "        [4.0780e+02, 4.2914e+02, 9.4873e-02],\n",
      "        [6.0576e+02, 5.5037e+02, 1.0408e-01],\n",
      "        [4.2467e+02, 4.1384e+02, 1.2692e-01],\n",
      "        [9.0895e+02, 5.5960e+02, 1.7276e-01],\n",
      "        [1.1103e+03, 7.0171e+02, 7.4714e-02],\n",
      "        [8.7607e+01, 7.4838e+02, 1.0177e-01],\n",
      "        [5.4842e+01, 7.0450e+02, 2.8579e-01]])\n",
      "tensor([[3.5112e+02, 4.5597e+02, 8.4211e-02],\n",
      "        [6.1733e+01, 7.1746e+02, 1.4281e-01],\n",
      "        [8.3665e+02, 6.8309e+02, 1.0706e-01],\n",
      "        [6.0263e+02, 6.9892e+02, 9.4089e-02],\n",
      "        [1.0868e+03, 6.9641e+02, 1.7158e-01],\n",
      "        [5.0359e+02, 7.0946e+02, 1.8580e-01],\n",
      "        [6.7871e+02, 6.6394e+02, 6.9834e-02],\n",
      "        [5.0551e+02, 6.6210e+02, 1.5859e-01],\n",
      "        [1.0840e+03, 6.9636e+02, 1.7515e-01],\n",
      "        [1.0809e+03, 7.3265e+02, 9.2565e-02],\n",
      "        [2.4730e+02, 5.3714e+02, 9.7372e-02],\n",
      "        [5.7542e+01, 7.0433e+02, 2.2445e-01]])\n",
      "tensor([[3.8319e+02, 4.5665e+02, 1.0239e-01],\n",
      "        [5.8226e+02, 7.1502e+02, 8.9145e-02],\n",
      "        [5.4057e+02, 6.7316e+02, 1.0605e-01],\n",
      "        [6.0425e+02, 6.9839e+02, 1.0869e-01],\n",
      "        [1.0861e+03, 6.9744e+02, 1.3758e-01],\n",
      "        [5.3073e+02, 6.7896e+02, 1.8881e-01],\n",
      "        [1.3615e+02, 6.3145e+02, 6.8405e-02],\n",
      "        [5.3866e+02, 6.6128e+02, 1.9504e-01],\n",
      "        [1.0818e+03, 6.9781e+02, 1.4209e-01],\n",
      "        [6.0028e+02, 6.8558e+02, 8.8170e-02],\n",
      "        [2.4855e+02, 5.6825e+02, 1.0978e-01],\n",
      "        [8.1842e+02, 6.7269e+02, 1.5651e-01]])\n",
      "tensor([[3.8432e+02, 4.5799e+02, 1.1088e-01],\n",
      "        [1.2533e+02, 6.6832e+02, 8.9987e-02],\n",
      "        [5.4223e+02, 6.7094e+02, 1.1727e-01],\n",
      "        [6.0180e+02, 6.6535e+02, 9.3823e-02],\n",
      "        [1.0859e+03, 6.9728e+02, 1.4662e-01],\n",
      "        [5.0497e+02, 6.9237e+02, 2.2022e-01],\n",
      "        [6.7830e+02, 6.6449e+02, 7.3310e-02],\n",
      "        [5.4170e+02, 6.6040e+02, 2.0563e-01],\n",
      "        [1.0809e+03, 6.9756e+02, 1.3124e-01],\n",
      "        [5.7054e+02, 7.2099e+02, 1.0379e-01],\n",
      "        [2.8209e+02, 5.7136e+02, 1.0485e-01],\n",
      "        [1.1485e+03, 6.2681e+02, 1.6397e-01]])\n",
      "tensor([[3.8356e+02, 4.5858e+02, 8.9879e-02],\n",
      "        [1.2515e+02, 6.6954e+02, 7.5828e-02],\n",
      "        [5.4292e+02, 6.7125e+02, 1.1280e-01],\n",
      "        [6.3135e+02, 6.9799e+02, 9.9730e-02],\n",
      "        [1.0868e+03, 6.9678e+02, 1.2848e-01],\n",
      "        [5.3304e+02, 6.8173e+02, 2.6151e-01],\n",
      "        [6.4753e+02, 6.9979e+02, 7.8232e-02],\n",
      "        [5.5874e+02, 6.6464e+02, 1.9881e-01],\n",
      "        [1.0818e+03, 6.9786e+02, 1.4270e-01],\n",
      "        [5.7031e+02, 7.2105e+02, 1.0338e-01],\n",
      "        [2.8268e+02, 5.7110e+02, 1.2613e-01],\n",
      "        [1.1491e+03, 6.2796e+02, 1.7890e-01]])\n",
      "tensor([[3.8291e+02, 4.5804e+02, 1.0896e-01],\n",
      "        [1.1254e+02, 6.6983e+02, 8.8494e-02],\n",
      "        [8.0890e+02, 6.7965e+02, 8.4653e-02],\n",
      "        [6.0307e+02, 7.0406e+02, 1.0699e-01],\n",
      "        [1.0869e+03, 6.9460e+02, 1.6680e-01],\n",
      "        [5.3699e+02, 7.0936e+02, 2.0329e-01],\n",
      "        [6.4860e+02, 6.9868e+02, 7.7161e-02],\n",
      "        [5.4416e+02, 6.6039e+02, 1.7672e-01],\n",
      "        [1.0850e+03, 6.9491e+02, 1.8429e-01],\n",
      "        [1.0432e+03, 7.2942e+02, 7.0332e-02],\n",
      "        [2.8172e+02, 5.7056e+02, 1.2321e-01],\n",
      "        [9.8072e+01, 7.4215e+02, 1.5261e-01]])\n",
      "tensor([[3.8273e+02, 4.5848e+02, 9.4429e-02],\n",
      "        [1.2395e+03, 6.3170e+02, 8.6272e-02],\n",
      "        [5.4286e+02, 6.7322e+02, 1.1423e-01],\n",
      "        [5.9606e+02, 6.3251e+02, 9.4219e-02],\n",
      "        [1.0870e+03, 6.9428e+02, 1.3713e-01],\n",
      "        [5.3607e+02, 6.7939e+02, 2.3612e-01],\n",
      "        [6.7917e+02, 6.6338e+02, 6.5936e-02],\n",
      "        [5.5963e+02, 6.6266e+02, 1.7405e-01],\n",
      "        [1.0847e+03, 6.9524e+02, 1.6402e-01],\n",
      "        [5.7354e+02, 7.2068e+02, 6.4011e-02],\n",
      "        [2.8115e+02, 5.6959e+02, 1.1109e-01],\n",
      "        [9.8346e+01, 7.4181e+02, 1.6122e-01]])\n",
      "tensor([[3.8306e+02, 4.5738e+02, 9.7753e-02],\n",
      "        [3.9921e+02, 4.7818e+02, 7.8892e-02],\n",
      "        [5.4051e+02, 6.9391e+02, 9.9539e-02],\n",
      "        [6.3209e+02, 6.9785e+02, 8.5869e-02],\n",
      "        [1.0875e+03, 6.9394e+02, 1.3278e-01],\n",
      "        [5.3652e+02, 6.9392e+02, 2.3307e-01],\n",
      "        [6.7942e+02, 6.6385e+02, 6.6287e-02],\n",
      "        [5.6081e+02, 6.6201e+02, 1.5728e-01],\n",
      "        [1.0985e+03, 6.9805e+02, 1.4327e-01],\n",
      "        [6.0366e+02, 6.8759e+02, 6.1230e-02],\n",
      "        [2.7847e+02, 5.3891e+02, 1.1246e-01],\n",
      "        [8.2190e+02, 7.0404e+02, 1.5423e-01]])\n",
      "tensor([[3.8288e+02, 4.5762e+02, 9.8681e-02],\n",
      "        [4.0023e+02, 4.7753e+02, 7.5437e-02],\n",
      "        [5.4288e+02, 6.7316e+02, 1.1081e-01],\n",
      "        [6.3177e+02, 6.9735e+02, 8.7974e-02],\n",
      "        [1.0871e+03, 6.9427e+02, 9.8410e-02],\n",
      "        [5.3613e+02, 7.0996e+02, 2.4426e-01],\n",
      "        [6.7974e+02, 6.6444e+02, 7.2401e-02],\n",
      "        [5.6022e+02, 6.6336e+02, 1.7474e-01],\n",
      "        [1.0500e+03, 7.1262e+02, 1.5053e-01],\n",
      "        [6.0314e+02, 6.8665e+02, 6.7520e-02],\n",
      "        [2.7829e+02, 5.3868e+02, 1.1327e-01],\n",
      "        [5.4448e+02, 7.0611e+02, 1.5428e-01]])\n",
      "tensor([[4.1366e+02, 4.5690e+02, 1.2803e-01],\n",
      "        [3.2990e+02, 4.1139e+02, 8.5434e-02],\n",
      "        [5.3982e+02, 6.9446e+02, 1.0917e-01],\n",
      "        [5.9698e+02, 6.3139e+02, 9.7970e-02],\n",
      "        [1.0867e+03, 6.9459e+02, 1.3151e-01],\n",
      "        [5.3610e+02, 6.9417e+02, 2.3441e-01],\n",
      "        [6.7920e+02, 6.6419e+02, 6.7006e-02],\n",
      "        [5.6012e+02, 6.6306e+02, 1.5010e-01],\n",
      "        [1.0977e+03, 6.9778e+02, 1.5147e-01],\n",
      "        [6.1920e+02, 6.8715e+02, 7.4373e-02],\n",
      "        [2.8179e+02, 5.7053e+02, 1.2002e-01],\n",
      "        [2.7374e+02, 5.0768e+02, 1.7434e-01]])\n",
      "tensor([[4.1276e+02, 4.5765e+02, 9.7683e-02],\n",
      "        [5.7979e+02, 7.1255e+02, 7.4678e-02],\n",
      "        [5.4229e+02, 6.7398e+02, 8.5597e-02],\n",
      "        [6.3318e+02, 6.9849e+02, 9.5866e-02],\n",
      "        [1.0869e+03, 6.9523e+02, 1.3509e-01],\n",
      "        [5.3790e+02, 7.0955e+02, 2.3152e-01],\n",
      "        [6.7865e+02, 6.6386e+02, 7.1262e-02],\n",
      "        [5.5813e+02, 6.6160e+02, 1.4274e-01],\n",
      "        [1.0854e+03, 6.9605e+02, 1.7629e-01],\n",
      "        [6.1891e+02, 6.8634e+02, 6.4240e-02],\n",
      "        [2.8241e+02, 5.7067e+02, 1.2774e-01],\n",
      "        [5.2305e+02, 6.8832e+02, 1.5330e-01]])\n",
      "tensor([[3.8197e+02, 4.5804e+02, 9.8714e-02],\n",
      "        [3.3520e+02, 5.8131e+02, 8.0831e-02],\n",
      "        [5.4283e+02, 6.7316e+02, 1.1118e-01],\n",
      "        [6.3306e+02, 6.6294e+02, 1.0185e-01],\n",
      "        [1.0868e+03, 6.9554e+02, 1.3453e-01],\n",
      "        [5.3599e+02, 6.9438e+02, 2.4980e-01],\n",
      "        [6.7864e+02, 6.6365e+02, 6.6552e-02],\n",
      "        [5.5996e+02, 6.6284e+02, 1.4626e-01],\n",
      "        [1.0844e+03, 6.9655e+02, 1.4143e-01],\n",
      "        [6.1965e+02, 6.8723e+02, 6.6891e-02],\n",
      "        [2.8162e+02, 5.7034e+02, 1.2949e-01],\n",
      "        [2.7448e+02, 5.0761e+02, 1.6455e-01]])\n",
      "tensor([[4.1333e+02, 4.5721e+02, 1.3531e-01],\n",
      "        [3.3600e+02, 5.8244e+02, 7.5854e-02],\n",
      "        [5.4268e+02, 6.7355e+02, 1.0838e-01],\n",
      "        [6.3267e+02, 6.6275e+02, 9.8661e-02],\n",
      "        [1.0850e+03, 6.9660e+02, 1.3974e-01],\n",
      "        [5.3667e+02, 7.1047e+02, 2.3747e-01],\n",
      "        [6.7923e+02, 6.6389e+02, 7.1605e-02],\n",
      "        [5.5360e+02, 7.1267e+02, 1.4248e-01],\n",
      "        [1.0817e+03, 6.9724e+02, 1.3701e-01],\n",
      "        [1.0430e+03, 7.2991e+02, 6.8046e-02],\n",
      "        [2.8114e+02, 5.6939e+02, 1.2727e-01],\n",
      "        [5.4545e+02, 7.0481e+02, 1.6007e-01]])\n",
      "tensor([[4.1281e+02, 4.5664e+02, 1.2661e-01],\n",
      "        [7.0808e+02, 6.9580e+02, 8.4050e-02],\n",
      "        [5.4125e+02, 6.7434e+02, 9.2238e-02],\n",
      "        [6.3271e+02, 6.6247e+02, 9.5397e-02],\n",
      "        [1.0860e+03, 6.9618e+02, 1.5468e-01],\n",
      "        [5.3715e+02, 7.1026e+02, 2.3124e-01],\n",
      "        [6.7904e+02, 6.6369e+02, 6.2697e-02],\n",
      "        [5.5762e+02, 6.6174e+02, 1.3715e-01],\n",
      "        [1.0844e+03, 6.9733e+02, 1.8040e-01],\n",
      "        [1.1096e+03, 6.9957e+02, 5.8881e-02],\n",
      "        [2.8124e+02, 5.7008e+02, 1.1707e-01],\n",
      "        [8.4956e+02, 7.0621e+02, 1.7124e-01]])\n",
      "tensor([[3.8470e+02, 4.8715e+02, 7.7367e-02],\n",
      "        [3.9966e+02, 4.7768e+02, 7.7197e-02],\n",
      "        [5.4339e+02, 6.7206e+02, 8.5188e-02],\n",
      "        [6.0413e+02, 7.0255e+02, 1.0589e-01],\n",
      "        [1.0866e+03, 6.9612e+02, 1.5739e-01],\n",
      "        [5.3638e+02, 6.7969e+02, 2.3322e-01],\n",
      "        [6.8011e+02, 6.6460e+02, 7.0016e-02],\n",
      "        [5.5857e+02, 6.6291e+02, 2.0232e-01],\n",
      "        [1.0849e+03, 6.9747e+02, 1.7089e-01],\n",
      "        [6.0144e+02, 6.8922e+02, 7.7632e-02],\n",
      "        [2.4802e+02, 5.6844e+02, 1.0785e-01],\n",
      "        [5.4618e+02, 7.0603e+02, 1.5164e-01]])\n",
      "tensor([[3.8302e+02, 4.5565e+02, 9.7233e-02],\n",
      "        [1.1002e+02, 6.6793e+02, 1.1660e-01],\n",
      "        [5.0951e+02, 6.7220e+02, 1.1903e-01],\n",
      "        [6.0439e+02, 6.9980e+02, 1.1614e-01],\n",
      "        [1.0857e+03, 6.9803e+02, 1.3153e-01],\n",
      "        [5.0259e+02, 6.7788e+02, 2.3246e-01],\n",
      "        [4.9820e+02, 6.6921e+02, 5.6736e-02],\n",
      "        [5.2322e+02, 6.6269e+02, 1.5871e-01],\n",
      "        [1.0820e+03, 6.9909e+02, 1.1910e-01],\n",
      "        [5.4408e+02, 6.7263e+02, 8.3914e-02],\n",
      "        [2.5045e+02, 5.7123e+02, 1.0835e-01],\n",
      "        [2.1156e+02, 5.0674e+02, 1.4841e-01]])\n",
      "tensor([[1.2779e+02, 5.5786e+02, 1.2424e-01],\n",
      "        [1.5202e+02, 6.8861e+02, 9.6200e-02],\n",
      "        [5.1136e+02, 6.7065e+02, 1.1875e-01],\n",
      "        [5.7084e+02, 7.0042e+02, 8.2954e-02],\n",
      "        [8.3182e+02, 6.7072e+02, 9.9785e-02],\n",
      "        [4.9905e+02, 6.7981e+02, 2.2405e-01],\n",
      "        [3.9846e+02, 5.2727e+02, 1.8588e-01],\n",
      "        [5.1992e+02, 6.9811e+02, 2.4711e-01],\n",
      "        [9.5548e+02, 6.9641e+02, 2.0376e-01],\n",
      "        [5.4042e+02, 7.0395e+02, 8.7055e-02],\n",
      "        [4.1432e+02, 4.7919e+02, 1.0627e-01],\n",
      "        [8.3728e+01, 7.2887e+02, 2.3003e-01]])\n",
      "tensor([[4.3048e+02, 4.2198e+02, 1.3161e-01],\n",
      "        [4.3220e+02, 6.9325e+02, 9.2748e-02],\n",
      "        [2.4938e+02, 4.1349e+02, 1.7926e-01],\n",
      "        [2.0156e+02, 4.5457e+02, 1.1216e-01],\n",
      "        [4.4432e+02, 4.1541e+02, 1.5072e-01],\n",
      "        [3.5388e+02, 6.2551e+02, 4.3971e-01],\n",
      "        [4.5430e+02, 4.3107e+02, 1.9710e-01],\n",
      "        [5.0835e+02, 6.6758e+02, 1.6660e-01],\n",
      "        [4.4656e+02, 4.1120e+02, 1.8708e-01],\n",
      "        [5.1085e+02, 7.0481e+02, 9.5331e-02],\n",
      "        [2.0541e+02, 7.0446e+02, 1.1179e-01],\n",
      "        [9.2949e+01, 6.6215e+02, 2.3056e-01]])\n",
      "tensor([[1.9491e+02, 4.4154e+02, 2.0036e-01],\n",
      "        [4.2536e+02, 6.4273e+02, 1.1864e-01],\n",
      "        [4.0948e+02, 4.2823e+02, 1.5872e-01],\n",
      "        [3.9721e+02, 6.1943e+02, 1.9359e-01],\n",
      "        [4.6658e+02, 4.0982e+02, 1.1763e-01],\n",
      "        [3.5384e+02, 5.9814e+02, 3.8899e-01],\n",
      "        [4.7092e+02, 4.2485e+02, 1.7141e-01],\n",
      "        [5.3883e+02, 6.6513e+02, 1.9997e-01],\n",
      "        [3.7079e+01, 6.3614e+02, 1.8349e-01],\n",
      "        [5.3417e+02, 6.4081e+02, 9.4203e-02],\n",
      "        [1.3982e+02, 6.9662e+02, 1.4029e-01],\n",
      "        [1.1988e+02, 5.9944e+02, 2.0587e-01]])\n",
      "tensor([[2.0102e+02, 4.3111e+02, 2.5455e-01],\n",
      "        [1.9903e+02, 4.8705e+02, 1.0621e-01],\n",
      "        [2.8926e+02, 3.8458e+02, 2.0058e-01],\n",
      "        [2.5539e+02, 6.5579e+02, 1.8740e-01],\n",
      "        [3.2658e+02, 3.6707e+02, 1.0307e-01],\n",
      "        [3.8640e+02, 5.9467e+02, 4.1818e-01],\n",
      "        [6.5103e+02, 6.6695e+02, 1.0178e-01],\n",
      "        [5.2230e+02, 6.6584e+02, 1.9476e-01],\n",
      "        [6.4684e+01, 6.0993e+02, 3.3264e-01],\n",
      "        [2.4308e+02, 7.1268e+02, 1.1496e-01],\n",
      "        [1.5335e+02, 6.9322e+02, 9.1976e-02],\n",
      "        [1.9107e+02, 5.6997e+02, 1.4629e-01]])\n",
      "tensor([[3.2119e+02, 3.7504e+02, 2.3587e-01],\n",
      "        [2.0115e+02, 4.8448e+02, 1.2080e-01],\n",
      "        [2.8668e+02, 3.8616e+02, 2.4575e-01],\n",
      "        [2.5925e+02, 6.5343e+02, 2.0149e-01],\n",
      "        [3.2763e+02, 3.6426e+02, 1.1430e-01],\n",
      "        [3.8866e+02, 5.9199e+02, 3.8375e-01],\n",
      "        [6.5020e+02, 6.6751e+02, 1.0117e-01],\n",
      "        [4.6585e+02, 5.8734e+02, 1.7880e-01],\n",
      "        [6.1451e+01, 5.9888e+02, 4.2656e-01],\n",
      "        [3.3001e+02, 6.5360e+02, 1.2287e-01],\n",
      "        [8.8188e+02, 6.7206e+02, 1.1070e-01],\n",
      "        [1.6999e+02, 5.8651e+02, 1.4195e-01]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Running the analyze_live_video function in a Jupyter notebook\n",
    "\n",
    "from dlclive.LiveVideoInference import analyze_live_video\n",
    "\n",
    "# Define the paths\n",
    "model_path = \"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Hand-AnnaStuckert-2024-08-21/dlc-models-pytorch/iteration-0/HandAug21-trainset95shuffle101/train\"\n",
    "\n",
    "# Call the analyze_live_video function with the appropriate arguments\n",
    "poses = analyze_live_video(\n",
    "    camera=0,\n",
    "    model_path=model_path,\n",
    "    model_type=\"pytorch\",\n",
    "    snapshot=\"snapshot-200.pt\",\n",
    "    device=\"cpu\",\n",
    "    display=True,\n",
    "    save_poses=True,\n",
    "    resize=0.5,\n",
    "    #precision=\"FP16\",\n",
    "    #cropping=[50, 250, 100, 450],  # manually set the cropping to specific pixels\n",
    "    #dynamic=(\n",
    "    #    True,\n",
    "    #    0.5,\n",
    "    #    10,\n",
    "    #),  # True = apply dynamic cropping, 0.5 = threshold for KP detection, 10 = margin for cropping\n",
    "    save_dir=\"output_directory\",\n",
    "    get_sys_info=True,\n",
    "    draw_keypoint_names=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc-live",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
