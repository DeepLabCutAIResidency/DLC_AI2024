{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLC Live PyTorch Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlclive import DLCLive\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshot to ONNX model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you do not have a .onnx model exported, use this cell to export your DLC3.0 snapshot\n",
    "\n",
    "from deeplabcut.pose_estimation_pytorch.config import read_config_as_dict\n",
    "from deeplabcut.pose_estimation_pytorch.models import PoseModel\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#Dikra\n",
    "# root = Path(\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\")\n",
    "# model_cfg = read_config_as_dict(root / \"pytorch_config.yaml\")\n",
    "# weights_path = root / \"snapshot-200.pt\"\n",
    "\n",
    "#Anna\n",
    "root = Path(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\")\n",
    "model_cfg = read_config_as_dict(root / \"pytorch_config.yaml\")\n",
    "weights_path = root / \"snapshot-263.pt\"\n",
    "\n",
    "model = PoseModel.build(model_cfg[\"model\"])\n",
    "weights = torch.load(weights_path, map_location=device)\n",
    "model.load_state_dict(weights[\"model\"])\n",
    "\n",
    "dummy_input = torch.zeros((1, 3, 224, 224))\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/resnet.onnx\",\n",
    "    verbose=False,\n",
    "    input_names=[\"input\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test frame\n",
    "img = cv2.imread(\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/img008.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with ONNX exported DLC 3.0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dikra\n",
    "onnx_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cuda\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# onnx_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# onnx_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "onnx_pose = onnx_dlc_live.init_inference(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot from 2024-08-20 14-29-53.png](./docs/assets/Screenshot%20from%202024-08-20%2014-36-00.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_pose = onnx_dlc_live.get_pose(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with snaptshot of DLC 3.0 model (.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dikra\n",
    "pytorch_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    snapshot=\"snapshot-200.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# pytorch_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# pytorch_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "pytorch_pose = pytorch_dlc_live.init_inference(frame=img)\n",
    "pytorch_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PyTorch model inference](./docs/assets/Screenshot%20from%202024-08-20%2014-29-53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "root = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\"\n",
    "test_images = glob.glob(os.path.normpath(root + \"/*.png\"))\n",
    "\n",
    "\n",
    "def mean_time_inference(dlc_live, images):\n",
    "    times = []\n",
    "    for i, img_p in enumerate(images):\n",
    "        img = cv2.imread(img_p)\n",
    "\n",
    "        if i == 0:\n",
    "            start = time.time()\n",
    "            dlc_live.init_inference(img)\n",
    "            end = time.time()\n",
    "        else:\n",
    "            start = time.time()\n",
    "            dlc_live.get_pose(img)\n",
    "            end = time.time()\n",
    "        times.append(end - start)\n",
    "    print(times)\n",
    "\n",
    "    return np.mean(times), times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"onnx\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images)\n",
    "print(\n",
    "    f\"TOTAL Inference of ONNX model took on average {mean_time} seconds for {len(test_images)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    snapshot=\"snapshot-200.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\"\n",
    ")\n",
    "root = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\"\n",
    "test_images = glob.glob(os.path.normpath(root + \"/*.png\"))\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images) \n",
    "print(f\"Inference of PyTorch model took on average {mean_time} seconds for {len(test_images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "Currently the benchmark_pytorch.py script serves to provide a function for analyzing a preexisting video to test PyTorch for running video inference in DLC-Live. Code for running video inference on a live video feed is WIP.\n",
    "\n",
    "For true benchmarking purposes, we aim to add feature for recording the time it takes to analyze each frame / how many frames can be analyzed per second. Discuss what measure to use and consult the DLC Live paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annastuckert/anaconda3/envs/deeplabcut3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/annastuckert/anaconda3/envs/deeplabcut3/lib/python3.10/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import the analyze_video function from the file where it's defined\n",
    "from dlclive.benchmark_pytorch import analyze_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored When destroying _lsprof profiler:\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/3y/jmv30wb16ss901m1yrdysjh80000gq/T/ipykernel_50549/776358094.py\", line 14, in <module>\n",
      "RuntimeError: Cannot install a profile function while another profile function is being installed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX inference took 0.08013701438903809 sec\n",
      "ONNX inference took 0.023272991180419922 sec\n",
      "Frame 0 processing time: 0.2047 seconds\n",
      "ONNX inference took 0.08839082717895508 sec\n",
      "ONNX inference took 0.021580934524536133 sec\n",
      "Frame 1 processing time: 0.1312 seconds\n",
      "ONNX inference took 0.10285806655883789 sec\n",
      "ONNX inference took 0.03156018257141113 sec\n",
      "Frame 2 processing time: 0.1646 seconds\n",
      "ONNX inference took 0.21061491966247559 sec\n",
      "ONNX inference took 0.03587079048156738 sec\n",
      "Frame 3 processing time: 0.2813 seconds\n",
      "ONNX inference took 0.15520811080932617 sec\n",
      "ONNX inference took 0.033479928970336914 sec\n",
      "Frame 4 processing time: 0.2313 seconds\n",
      "ONNX inference took 0.14543604850769043 sec\n",
      "ONNX inference took 0.024671077728271484 sec\n",
      "Frame 5 processing time: 0.2139 seconds\n",
      "ONNX inference took 0.1022939682006836 sec\n",
      "ONNX inference took 0.02432107925415039 sec\n",
      "Frame 6 processing time: 0.1651 seconds\n",
      "ONNX inference took 0.10728287696838379 sec\n",
      "ONNX inference took 0.028424978256225586 sec\n",
      "Frame 7 processing time: 0.1817 seconds\n",
      "ONNX inference took 0.12629485130310059 sec\n",
      "ONNX inference took 0.02992391586303711 sec\n",
      "Frame 8 processing time: 0.1981 seconds\n",
      "ONNX inference took 0.18933320045471191 sec\n",
      "ONNX inference took 0.03226304054260254 sec\n",
      "Frame 9 processing time: 0.2646 seconds\n",
      "ONNX inference took 0.13659429550170898 sec\n",
      "ONNX inference took 0.03926396369934082 sec\n",
      "Frame 10 processing time: 0.2147 seconds\n",
      "ONNX inference took 0.2056572437286377 sec\n",
      "ONNX inference took 0.027560949325561523 sec\n",
      "Frame 11 processing time: 0.2809 seconds\n",
      "ONNX inference took 0.17596006393432617 sec\n",
      "ONNX inference took 0.030338048934936523 sec\n",
      "Frame 12 processing time: 0.2485 seconds\n",
      "ONNX inference took 0.12009406089782715 sec\n",
      "ONNX inference took 0.024102210998535156 sec\n",
      "Frame 13 processing time: 0.1797 seconds\n",
      "ONNX inference took 0.10401511192321777 sec\n",
      "ONNX inference took 0.02724599838256836 sec\n",
      "Frame 14 processing time: 0.1647 seconds\n",
      "ONNX inference took 0.10861372947692871 sec\n",
      "ONNX inference took 0.026845932006835938 sec\n",
      "Frame 15 processing time: 0.1817 seconds\n",
      "ONNX inference took 0.10307693481445312 sec\n",
      "ONNX inference took 0.029392004013061523 sec\n",
      "Frame 16 processing time: 0.1651 seconds\n",
      "ONNX inference took 0.12484312057495117 sec\n",
      "ONNX inference took 0.030752182006835938 sec\n",
      "Frame 17 processing time: 0.1978 seconds\n",
      "ONNX inference took 0.12430834770202637 sec\n",
      "ONNX inference took 0.02473592758178711 sec\n",
      "Frame 18 processing time: 0.1971 seconds\n",
      "ONNX inference took 0.11144399642944336 sec\n",
      "ONNX inference took 0.0355830192565918 sec\n",
      "Frame 19 processing time: 0.1817 seconds\n",
      "ONNX inference took 0.17222976684570312 sec\n",
      "ONNX inference took 0.04862809181213379 sec\n",
      "Frame 20 processing time: 0.2647 seconds\n",
      "ONNX inference took 0.19389581680297852 sec\n",
      "ONNX inference took 0.04020881652832031 sec\n",
      "Frame 21 processing time: 0.2809 seconds\n",
      "ONNX inference took 0.23169302940368652 sec\n",
      "ONNX inference took 0.06094217300415039 sec\n",
      "Frame 22 processing time: 0.3311 seconds\n",
      "ONNX inference took 0.3182518482208252 sec\n",
      "ONNX inference took 0.030983924865722656 sec\n",
      "Frame 23 processing time: 0.3817 seconds\n",
      "ONNX inference took 0.33234596252441406 sec\n",
      "ONNX inference took 0.05702781677246094 sec\n",
      "Frame 24 processing time: 0.4471 seconds\n",
      "ONNX inference took 0.29474830627441406 sec\n",
      "ONNX inference took 0.0648190975189209 sec\n",
      "Frame 25 processing time: 0.3966 seconds\n",
      "ONNX inference took 0.23867487907409668 sec\n",
      "ONNX inference took 0.04513883590698242 sec\n",
      "Frame 26 processing time: 0.3311 seconds\n",
      "ONNX inference took 0.29758620262145996 sec\n",
      "ONNX inference took 0.03837704658508301 sec\n",
      "Frame 27 processing time: 0.3650 seconds\n",
      "ONNX inference took 0.23262596130371094 sec\n",
      "ONNX inference took 0.035137176513671875 sec\n",
      "Frame 28 processing time: 0.2972 seconds\n",
      "ONNX inference took 0.3369569778442383 sec\n",
      "ONNX inference took 0.043843984603881836 sec\n",
      "Frame 29 processing time: 0.4312 seconds\n",
      "ONNX inference took 0.3432300090789795 sec\n",
      "ONNX inference took 0.06183671951293945 sec\n",
      "Frame 30 processing time: 0.4479 seconds\n",
      "ONNX inference took 0.3097050189971924 sec\n",
      "ONNX inference took 0.030303001403808594 sec\n",
      "Frame 31 processing time: 0.3811 seconds\n",
      "ONNX inference took 0.3211636543273926 sec\n",
      "ONNX inference took 0.06262993812561035 sec\n",
      "Frame 32 processing time: 0.4313 seconds\n",
      "ONNX inference took 0.22645187377929688 sec\n",
      "ONNX inference took 0.035481929779052734 sec\n",
      "Frame 33 processing time: 0.2980 seconds\n",
      "ONNX inference took 0.25403904914855957 sec\n",
      "ONNX inference took 0.04009532928466797 sec\n",
      "Frame 34 processing time: 0.3475 seconds\n",
      "ONNX inference took 0.15462803840637207 sec\n",
      "ONNX inference took 0.04916882514953613 sec\n",
      "Frame 35 processing time: 0.2483 seconds\n",
      "ONNX inference took 0.2108001708984375 sec\n",
      "ONNX inference took 0.03422188758850098 sec\n",
      "Frame 36 processing time: 0.2806 seconds\n",
      "ONNX inference took 0.14618539810180664 sec\n",
      "ONNX inference took 0.02939319610595703 sec\n",
      "Frame 37 processing time: 0.2131 seconds\n",
      "ONNX inference took 0.10592198371887207 sec\n",
      "ONNX inference took 0.024242877960205078 sec\n",
      "Frame 38 processing time: 0.1810 seconds\n",
      "ONNX inference took 0.09595704078674316 sec\n",
      "ONNX inference took 0.019037723541259766 sec\n",
      "Frame 39 processing time: 0.1482 seconds\n",
      "ONNX inference took 0.11117887496948242 sec\n",
      "ONNX inference took 0.024348974227905273 sec\n",
      "Frame 40 processing time: 0.1652 seconds\n",
      "ONNX inference took 0.1102900505065918 sec\n",
      "ONNX inference took 0.019037961959838867 sec\n",
      "Frame 41 processing time: 0.1646 seconds\n",
      "ONNX inference took 0.10240578651428223 sec\n",
      "ONNX inference took 0.021250009536743164 sec\n",
      "Frame 42 processing time: 0.1485 seconds\n",
      "ONNX inference took 0.11937689781188965 sec\n",
      "ONNX inference took 0.019635915756225586 sec\n",
      "Frame 43 processing time: 0.1642 seconds\n",
      "ONNX inference took 0.11210799217224121 sec\n",
      "ONNX inference took 0.019539833068847656 sec\n",
      "Frame 44 processing time: 0.1651 seconds\n"
     ]
    }
   ],
   "source": [
    "# New version with DLCLive object included in the code\n",
    "\n",
    "\n",
    "\n",
    "# Define the paths\n",
    "video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_03s.avi'\n",
    "model_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train'\n",
    "\n",
    "import cProfile\n",
    "import io\n",
    "import pstats\n",
    "\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "# Call the analyze_video function with the appropriate arguments\n",
    "poses = analyze_video(\n",
    "    video_path=video_path,\n",
    "    model_path=model_path,\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cuda\",      \n",
    "    display=True,      \n",
    "    save_poses=True,\n",
    "    resize= 0.5,\n",
    "    #cropping= [50, 250, 100, 450], manually set the cropping to specific pixels\n",
    "    dynamic=(True, 0.5, 10), #True = we want to apply dynamic cropping, 0.5 = the threshold for accepting a KP as detected, 10 = the margin to expand the calculatted cropping window by so it is not too narrow\n",
    "    save_dir='output_directory',  \n",
    "    draw_keypoint_names=True      \n",
    ")\n",
    "\n",
    "# #'poses' will contain the list of poses detected\n",
    "\n",
    "# # Create a stream to capture the profiler's output\n",
    "# s = io.StringIO()\n",
    "# sortby = 'cumulative'\n",
    "# ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "# ps.print_stats()\n",
    "\n",
    "# # Print the profiling output\n",
    "# print(s.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test download of benchmarking dataset \n",
    "#OBS link it not working, waiting for updated link to benchmarking dataset\n",
    "\n",
    "# from dlclive.benchmark_pytorch import download_benchmarking_data\n",
    "\n",
    "# download_benchmarking_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlclive.benchmark_pytorch import get_system_info\n",
    "\n",
    "\n",
    "sys_info = get_system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc-live",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
