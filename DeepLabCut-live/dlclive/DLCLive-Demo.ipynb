{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLC Live PyTorch Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdlclive\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DLCLive, Processor\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdlclive\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Display\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from dlclive import DLCLive, Processor\n",
    "from dlclive.display import Display\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshot to ONNX model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34268/968774635.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(weights_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# In case you do not have a .onnx model exported, use this cell to export your DLC3.0 snapshot\n",
    "\n",
    "from deeplabcut.pose_estimation_pytorch.config import read_config_as_dict\n",
    "from deeplabcut.pose_estimation_pytorch.models import PoseModel\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#Dikra\n",
    "# root = Path(\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\")\n",
    "# model_cfg = read_config_as_dict(root / \"pytorch_config.yaml\")\n",
    "# weights_path = root / \"snapshot-200.pt\"\n",
    "\n",
    "#Anna\n",
    "root = Path(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\")\n",
    "model_cfg = read_config_as_dict(root / \"pytorch_config.yaml\")\n",
    "weights_path = root / \"snapshot-263.pt\"\n",
    "\n",
    "model = PoseModel.build(model_cfg[\"model\"])\n",
    "weights = torch.load(weights_path, map_location=device)\n",
    "model.load_state_dict(weights[\"model\"])\n",
    "\n",
    "dummy_input = torch.zeros((1, 3, 224, 224))\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/resnet.onnx\",\n",
    "    verbose=False,\n",
    "    input_names=[\"input\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test frame\n",
    "img = cv2.imread(\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy/img008.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with ONNX exported DLC 3.0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poses': tensor([[[[7.3832e+01, 2.0015e+02, 8.8095e-02],\n",
       "           [6.9048e+01, 2.9947e+02, 2.2019e-01],\n",
       "           [1.1621e+02, 3.4326e+02, 3.1128e-01],\n",
       "           [1.1751e+02, 3.3694e+02, 5.3471e-01],\n",
       "           [1.1515e+02, 3.2886e+02, 6.5886e-01],\n",
       "           [1.3284e+02, 3.1905e+02, 3.0138e-01],\n",
       "           [7.9400e+01, 2.1545e+02, 6.3844e-01],\n",
       "           [5.0567e+01, 3.0373e+02, 2.5542e-01],\n",
       "           [1.2382e+02, 2.5162e+02, 7.6581e-01],\n",
       "           [1.3796e+02, 2.2743e+02, 6.9886e-01],\n",
       "           [1.4197e+02, 2.0919e+02, 6.3363e-01],\n",
       "           [1.6022e+02, 2.7524e+02, 6.3262e-01],\n",
       "           [9.4027e+01, 2.5252e+02, 1.9836e-01],\n",
       "           [1.4154e+02, 3.7739e+02, 8.7275e-01],\n",
       "           [1.4946e+02, 3.6513e+02, 6.4267e-01],\n",
       "           [2.7488e+01, 2.0230e+02, 8.3611e-01],\n",
       "           [2.6656e+01, 3.4699e+02, 8.0565e-01],\n",
       "           [3.1491e+01, 3.9521e+02, 8.7074e-01],\n",
       "           [2.5922e+01, 1.5689e+02, 7.5119e-01],\n",
       "           [3.0520e+01, 1.7030e+02, 8.1288e-01],\n",
       "           [3.6773e+01, 2.3040e+02, 6.7201e-01],\n",
       "           [9.8596e+01, 2.6144e+02, 5.8880e-01],\n",
       "           [1.2013e+02, 3.0822e+02, 6.6743e-01],\n",
       "           [1.0080e+02, 1.9202e+02, 6.6554e-01],\n",
       "           [1.3621e+02, 3.0187e+02, 2.4326e-01]]]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dikra\n",
    "onnx_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cuda\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# onnx_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# onnx_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "onnx_pose = onnx_dlc_live.init_inference(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot from 2024-08-20 14-29-53.png](./docs/assets/Screenshot%20from%202024-08-20%2014-36-00.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poses': tensor([[[[3.6219e+02, 4.1237e+02, 5.0502e-01],\n",
       "           [3.6271e+02, 3.9584e+02, 5.8567e-01],\n",
       "           [3.5676e+02, 3.7446e+02, 7.0505e-01],\n",
       "           [3.4043e+02, 3.5935e+02, 7.1149e-01],\n",
       "           [3.3437e+02, 4.0497e+02, 6.5122e-01],\n",
       "           [3.3772e+02, 4.0689e+02, 4.4919e-01],\n",
       "           [2.0013e+02, 4.3547e+02, 5.2071e-01],\n",
       "           [3.4130e+02, 4.3089e+02, 3.4862e-01],\n",
       "           [2.1540e+02, 3.9427e+02, 7.8372e-01],\n",
       "           [1.9974e+02, 3.7728e+02, 5.2794e-01],\n",
       "           [1.8255e+02, 3.6741e+02, 6.4688e-01],\n",
       "           [2.7519e+02, 3.3175e+02, 5.7645e-01],\n",
       "           [3.5014e+02, 4.3089e+02, 4.1499e-01],\n",
       "           [4.3459e+02, 3.1566e+02, 8.1675e-01],\n",
       "           [4.2561e+02, 2.9912e+02, 8.8807e-01],\n",
       "           [2.4359e+02, 5.0565e+02, 5.7659e-01],\n",
       "           [3.5792e+02, 4.8275e+02, 3.0052e-01],\n",
       "           [4.0234e+02, 4.1449e+02, 5.4220e-01],\n",
       "           [1.7142e+02, 4.8666e+02, 4.5839e-01],\n",
       "           [3.2110e+02, 5.1144e+02, 2.5084e-01],\n",
       "           [3.9914e+02, 4.1933e+02, 6.2746e-01],\n",
       "           [2.3576e+02, 4.3456e+02, 6.5916e-01],\n",
       "           [3.1890e+02, 4.2855e+02, 6.0924e-01],\n",
       "           [1.7677e+02, 4.1291e+02, 3.5336e-01],\n",
       "           [3.1286e+02, 4.2730e+02, 6.1907e-01]]]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_pose = onnx_dlc_live.get_pose(frame=img)\n",
    "onnx_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLC Live with snaptshot of DLC 3.0 model (.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dikra\n",
    "pytorch_dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    snapshot=\"snapshot-200.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "# Anna\n",
    "# pytorch_dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# pytorch_dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "pytorch_pose = pytorch_dlc_live.init_inference(frame=img)\n",
    "pytorch_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_dlc_live.display.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PyTorch model inference](./docs/assets/Screenshot%20from%202024-08-20%2014-29-53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "root = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\"\n",
    "test_images = glob.glob(os.path.normpath(root + \"/*.png\"))\n",
    "\n",
    "\n",
    "def mean_time_inference(dlc_live, images):\n",
    "    times = []\n",
    "    for i, img_p in enumerate(images):\n",
    "        img = cv2.imread(img_p)\n",
    "\n",
    "        if i == 0:\n",
    "            start = time.time()\n",
    "            dlc_live.init_inference(img)\n",
    "            end = time.time()\n",
    "        else:\n",
    "            start = time.time()\n",
    "            dlc_live.get_pose(img)\n",
    "            end = time.time()\n",
    "        times.append(end - start)\n",
    "    print(times)\n",
    "\n",
    "    return np.mean(times), times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model took 0.10965681076049805 sec\n",
      "ONNX inference took 1.3574702739715576 sec\n",
      "ONNX inference took 0.0489506721496582 sec\n",
      "ONNX inference took 0.05127286911010742 sec\n",
      "ONNX inference took 0.04564523696899414 sec\n",
      "ONNX inference took 0.045925140380859375 sec\n",
      "ONNX inference took 0.04640650749206543 sec\n",
      "ONNX inference took 0.04673171043395996 sec\n",
      "ONNX inference took 0.04611396789550781 sec\n",
      "ONNX inference took 0.04607415199279785 sec\n",
      "ONNX inference took 0.04634737968444824 sec\n",
      "ONNX inference took 0.04708504676818848 sec\n",
      "ONNX inference took 0.04560351371765137 sec\n",
      "ONNX inference took 0.04625558853149414 sec\n",
      "ONNX inference took 0.0464634895324707 sec\n",
      "ONNX inference took 0.04563474655151367 sec\n",
      "ONNX inference took 0.045355796813964844 sec\n",
      "ONNX inference took 0.04635453224182129 sec\n",
      "ONNX inference took 0.04609203338623047 sec\n",
      "ONNX inference took 0.04608511924743652 sec\n",
      "ONNX inference took 0.04626035690307617 sec\n",
      "ONNX inference took 0.046393632888793945 sec\n",
      "ONNX inference took 0.0465397834777832 sec\n",
      "ONNX inference took 0.046779632568359375 sec\n",
      "ONNX inference took 0.04633593559265137 sec\n",
      "ONNX inference took 0.0462336540222168 sec\n",
      "ONNX inference took 0.04585909843444824 sec\n",
      "ONNX inference took 0.046262502670288086 sec\n",
      "ONNX inference took 0.045493125915527344 sec\n",
      "ONNX inference took 0.045987844467163086 sec\n",
      "ONNX inference took 0.04673480987548828 sec\n",
      "ONNX inference took 0.04610133171081543 sec\n",
      "ONNX inference took 0.045844316482543945 sec\n",
      "ONNX inference took 0.0463109016418457 sec\n",
      "ONNX inference took 0.04712677001953125 sec\n",
      "ONNX inference took 0.04647088050842285 sec\n",
      "ONNX inference took 0.046700477600097656 sec\n",
      "ONNX inference took 0.045346975326538086 sec\n",
      "ONNX inference took 0.046086788177490234 sec\n",
      "[1.5576214790344238, 0.1221768856048584, 0.10003113746643066, 0.05940842628479004, 0.0643613338470459, 0.11397576332092285, 0.07359075546264648, 0.0666961669921875, 0.058905601501464844, 0.06929302215576172, 0.06383943557739258, 0.06470751762390137, 0.07389712333679199, 0.060437679290771484, 0.11040067672729492, 0.06292057037353516, 0.06296396255493164, 0.057489633560180664, 0.06966829299926758, 0.06379413604736328, 0.06551814079284668, 0.08559513092041016, 0.07795429229736328, 0.061656951904296875, 0.06424546241760254, 0.06567192077636719, 0.06733465194702148, 0.06701922416687012, 0.06349897384643555, 0.060565948486328125, 0.06183934211730957, 0.06692099571228027, 0.08755826950073242, 0.09217476844787598, 0.08644914627075195, 0.0647575855255127, 0.06128668785095215, 0.09169793128967285]\n",
      "TOTAL Inference of ONNX model took on average (0.11231381642191034, [1.5576214790344238, 0.1221768856048584, 0.10003113746643066, 0.05940842628479004, 0.0643613338470459, 0.11397576332092285, 0.07359075546264648, 0.0666961669921875, 0.058905601501464844, 0.06929302215576172, 0.06383943557739258, 0.06470751762390137, 0.07389712333679199, 0.060437679290771484, 0.11040067672729492, 0.06292057037353516, 0.06296396255493164, 0.057489633560180664, 0.06966829299926758, 0.06379413604736328, 0.06551814079284668, 0.08559513092041016, 0.07795429229736328, 0.061656951904296875, 0.06424546241760254, 0.06567192077636719, 0.06733465194702148, 0.06701922416687012, 0.06349897384643555, 0.060565948486328125, 0.06183934211730957, 0.06692099571228027, 0.08755826950073242, 0.09217476844787598, 0.08644914627075195, 0.0647575855255127, 0.06128668785095215, 0.09169793128967285]) seconds for 38 images\n"
     ]
    }
   ],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"onnx\",\n",
    "    display=True,\n",
    ")\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images)\n",
    "print(\n",
    "    f\"TOTAL Inference of ONNX model took on average {mean_time} seconds for {len(test_images)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_live = DLCLive(\n",
    "    path=\"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\",\n",
    "    snapshot=\"snapshot-200.pt\",\n",
    "    device=\"cuda\",\n",
    "    model_type=\"pytorch\"\n",
    ")\n",
    "root = \"/media/dikra/PhD/DATA/DLC24_Data/dlc-live-dummy\"\n",
    "test_images = glob.glob(os.path.normpath(root + \"/*.png\"))\n",
    "\n",
    "mean_time = mean_time_inference(dlc_live, test_images) \n",
    "print(f\"Inference of PyTorch model took on average {mean_time} seconds for {len(test_images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "Currently the benchmark_pytorch.py script serves to provide a function for analyzing a preexisting video to test PyTorch for running video inference in DLC-Live. Code for running video inference on a live video feed is WIP.\n",
    "\n",
    "For true benchmarking purposes, we aim to add feature for recording the time it takes to analyze each frame / how many frames can be analyzed per second. Discuss what measure to use and consult the DLC Live paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlclive.benchmark_pytorch import analyze_video\n",
    "from dlclive import DLCLive\n",
    "\n",
    "dlc_live = DLCLive(\n",
    "    path=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\",\n",
    "    model_type=\"onnx\",\n",
    "    device=\"cpu\",\n",
    "    display=True,\n",
    ")\n",
    "#short video\n",
    "video_path = '/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/1_20cms_0degUP_first_1s.avi'\n",
    "\n",
    "#providing output directory\n",
    "poses = analyze_video(video_path=video_path, dlc_live=dlc_live, save_poses=True, save_dir='output_directory', draw_keypoint_names=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc-live",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
