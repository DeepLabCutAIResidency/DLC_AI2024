{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PyTorch raw model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlclive import DLCLive, Processor\n",
    "import dlclive\n",
    "from dlclive.display import Display\n",
    "import cv2\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(540, 682, 3)\n"
     ]
    }
   ],
   "source": [
    "#Loading model\n",
    "\n",
    "dlc_proc = Processor()\n",
    "#Dikra\n",
    "dlc_live = DLCLive(pytorch_cfg=\"/media1/data/dikra/fly-kevin\", processor=dlc_proc, snapshot='/media1/data/dikra/fly-kevin/snapshot-100.pt', model_type='onnx')\n",
    "img = cv2.imread(\"/media1/data/dikra/fly-kevin/img001.png\")\n",
    "print(img.shape)\n",
    "img = img / 255.0\n",
    "img = np.array(img, dtype=np.float32)\n",
    "img = img.reshape(1, 3, 540, 682)\n",
    "\n",
    "# Anna\n",
    "# dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt')\n",
    "# # dlc_live = DLCLive(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/DLC_dev-single-animal_resnet_50_iteration-1_shuffle-1\", processor=dlc_proc)\n",
    "# img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "torch.Size([1, 25, 69, 87])\n",
      "torch.Size([1, 69, 87, 25])\n",
      "torch.Size([1, 50, 69, 87])\n",
      "torch.Size([1, 69, 87, 25, 2])\n",
      "dz torch.Size([1, 1, 25, 3])\n",
      "tensor([[[10, 22, 24, 11, 18, 11, 20, 12, 13, 11, 11, 18, 13, 60, 10, 17,  5,\n",
      "           5, 27, 57, 18, 19, 19, 21, 18]]]) tensor([[[54, 61, 60, 52, 20, 55, 62, 53, 53, 52, 52, 62, 53, 12, 53, 17, 38,\n",
      "          38, 32, 44, 62, 60, 62, 62, 60]]])\n",
      "tensor([[[164.1184, 358.2253, 392.0227, 183.4705, 295.2524, 182.2032, 322.3113,\n",
      "          189.5103, 209.1273, 180.8862, 180.5940, 305.8344, 212.8524, 966.0091,\n",
      "          165.3865, 282.4215,  84.4847,  85.8932, 438.0833, 920.9091, 299.0209,\n",
      "          304.5890, 312.2062, 334.7838, 297.6540]]]) tensor([[[ 869.9263,  989.9418,  969.7960,  837.6666,  330.6241,  879.7804,\n",
      "          1003.5058,  862.9377,  863.4908,  846.4874,  846.7697,  997.7080,\n",
      "           862.4837,  200.4533,  857.4780,  291.5507,  616.6808,  616.0269,\n",
      "           518.3417,  712.0759,  997.5941,  970.5095,  998.2376,  999.5897,\n",
      "           970.5465]]])\n",
      "tensor([[[[ -2.0736,  -3.8816,   0.5471],\n",
      "          [  5.9418,  -1.7747,   0.5615],\n",
      "          [  1.7959,   0.0227,   0.5309],\n",
      "          [ -2.3334,  -0.5295,   0.5323],\n",
      "          [  2.6241,  -0.7476,   0.5319],\n",
      "          [ -8.2196,  -1.7968,   0.5496],\n",
      "          [  3.5058,  -5.6887,   0.5208],\n",
      "          [  6.9377, -10.4897,   0.5479],\n",
      "          [  7.4909,  -6.8727,   0.5298],\n",
      "          [  6.4875,  -3.1138,   0.5186],\n",
      "          [  6.7697,  -3.4060,   0.5225],\n",
      "          [ -2.2920,   9.8344,   0.5448],\n",
      "          [  6.4837,  -3.1476,   0.5390],\n",
      "          [  0.4533,  -1.9909,   0.5719],\n",
      "          [  1.4780,  -2.6135,   0.5586],\n",
      "          [ 11.5507,   2.4215,   0.5418],\n",
      "          [  0.6808,  -3.5153,   0.5521],\n",
      "          [  0.0269,  -2.1068,   0.5389],\n",
      "          [ -1.6583,  -1.9167,   0.5349],\n",
      "          [  0.0759,   0.9091,   0.5425],\n",
      "          [ -2.4060,   3.0209,   0.5445],\n",
      "          [  2.5095,  -7.4110,   0.5246],\n",
      "          [ -1.7624,   0.2062,   0.5344],\n",
      "          [ -0.4104,  -9.2163,   0.5317],\n",
      "          [  2.5465,   1.6540,   0.5427]]]])\n",
      "Inference of Pytorch model used 0.642005443572998 seconds\n",
      "{'poses': tensor([[[[8.6993e+02, 1.6412e+02, 5.4712e-01],\n",
      "          [9.8994e+02, 3.5823e+02, 5.6155e-01],\n",
      "          [9.6980e+02, 3.9202e+02, 5.3089e-01],\n",
      "          [8.3767e+02, 1.8347e+02, 5.3233e-01],\n",
      "          [3.3062e+02, 2.9525e+02, 5.3193e-01],\n",
      "          [8.7978e+02, 1.8220e+02, 5.4962e-01],\n",
      "          [1.0035e+03, 3.2231e+02, 5.2076e-01],\n",
      "          [8.6294e+02, 1.8951e+02, 5.4794e-01],\n",
      "          [8.6349e+02, 2.0913e+02, 5.2978e-01],\n",
      "          [8.4649e+02, 1.8089e+02, 5.1860e-01],\n",
      "          [8.4677e+02, 1.8059e+02, 5.2254e-01],\n",
      "          [9.9771e+02, 3.0583e+02, 5.4478e-01],\n",
      "          [8.6248e+02, 2.1285e+02, 5.3896e-01],\n",
      "          [2.0045e+02, 9.6601e+02, 5.7191e-01],\n",
      "          [8.5748e+02, 1.6539e+02, 5.5855e-01],\n",
      "          [2.9155e+02, 2.8242e+02, 5.4177e-01],\n",
      "          [6.1668e+02, 8.4485e+01, 5.5206e-01],\n",
      "          [6.1603e+02, 8.5893e+01, 5.3894e-01],\n",
      "          [5.1834e+02, 4.3808e+02, 5.3493e-01],\n",
      "          [7.1208e+02, 9.2091e+02, 5.4254e-01],\n",
      "          [9.9759e+02, 2.9902e+02, 5.4449e-01],\n",
      "          [9.7051e+02, 3.0459e+02, 5.2457e-01],\n",
      "          [9.9824e+02, 3.1221e+02, 5.3436e-01],\n",
      "          [9.9959e+02, 3.3478e+02, 5.3175e-01],\n",
      "          [9.7055e+02, 2.9765e+02, 5.4273e-01]]]])}\n"
     ]
    }
   ],
   "source": [
    "#timing inference\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "pose = dlc_live.init_inference(frame=img)\n",
    "end = time.time()\n",
    "print(f\"Inference of Pytorch model used {end - start} seconds\")\n",
    "\n",
    "print(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input must be a list of dictionaries or a single numpy array for input 'input.1'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m dlc_live \u001b[38;5;241m=\u001b[39m DLCLive(pytorch_cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, processor\u001b[38;5;241m=\u001b[39mdlc_proc, snapshot\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m pose \u001b[38;5;241m=\u001b[39m \u001b[43mdlc_live\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/dlclive/dlclive.py:443\u001b[0m, in \u001b[0;36mDLCLive.init_inference\u001b[0;34m(self, frame, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# load model\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# if self.model_type == \"base\":\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# get pose of first frame (first inference is often very slow)\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     pose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     pose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/dlclive/dlclive.py:567\u001b[0m, in \u001b[0;36mDLCLive.get_pose\u001b[0;34m(self, frame, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m model_path \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpytorch_cfg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/*.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    565\u001b[0m ort_session \u001b[38;5;241m=\u001b[39m ort\u001b[38;5;241m.\u001b[39mInferenceSession(model_path)\n\u001b[0;32m--> 567\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mort_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m outputs_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheatmap\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor(outputs[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocref\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor(outputs[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m    574\u001b[0m }\n\u001b[1;32m    575\u001b[0m predictor \u001b[38;5;241m=\u001b[39m HeatmapPredictor()\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplabcut3/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input must be a list of dictionaries or a single numpy array for input 'input.1'."
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from pathlib import Path\n",
    "\n",
    "dlc_live = DLCLive(pytorch_cfg=\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train\", processor=dlc_proc, snapshot='/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/Ventral_gait_model/train/snapshot-263.pt', model_type='onnx')\n",
    "\n",
    "img = cv2.imread(\"/Users/annastuckert/Documents/DLC_AI_Residency/DLC_AI2024/DeepLabCut-live/exported DLC model for dlc-live/img049.png\")\n",
    "\n",
    "pose = dlc_live.init_inference(frame=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "pose = dlc_live.init_inference(frame=img)\n",
    "end = time.time()\n",
    "print(f\"Inference of Pytorch model used {end - start} seconds\")\n",
    "\n",
    "print(pose)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplabcut3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
